{
  "research_topic": "Improving fine-tuning performance of language models.",
  "queries": [
    "parameter-efficient fine-tuning",
    "LoRA fine-tuning",
    "prefix-tuning",
    "adapter tuning",
    "learning rate scheduling"
  ],
  "research_study_list": [
    {
      "title": "Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning ",
      "abstract": "Fine-tuning large pre-trained language models on downstream tasks has become\nan important paradigm in NLP. However, common practice fine-tunes all of the\nparameters in a pre-trained model, which becomes prohibitive when a large\nnumber of downstream tasks are present. Therefore, many fine-tuning methods are\nproposed to learn incremental updates of pre-trained weights in a parameter\nefficient way, e.g., low-rank increments. These methods often evenly distribute\nthe budget of incremental updates across all pre-trained weight matrices, and\noverlook the varying importance of different weight parameters. As a\nconsequence, the fine-tuning performance is suboptimal. To bridge this gap, we\npropose AdaLoRA, which adaptively allocates the parameter budget among weight\nmatrices according to their importance score. In particular, AdaLoRA\nparameterizes the incremental updates in the form of singular value\ndecomposition. Such a novel approach allows us to effectively prune the\nsingular values of unimportant updates, which is essentially to reduce their\nparameter budget but circumvent intensive exact SVD computations. We conduct\nextensive experiments with several pre-trained models on natural language\nprocessing, question answering, and natural language generation to validate the\neffectiveness of AdaLoRA. Results demonstrate that AdaLoRA manifests notable\nimprovement over baselines, especially in the low budget settings. Our code is\npublicly available at https://github.com/QingruZhang/AdaLoRA .",
      "full_text": "Published as a conference paper at ICLR 2023 ADALORA: A DAPTIVE BUDGET ALLOCATION FOR PARAMETER -EFFICIENT FINE -TUNING Qingru Zhang†∗, Minshuo Chen‡, Alexander Bukharin†, Nikos Karampatziakis⋄, Pengcheng He⋄, Yu Cheng⋄, Weizhu Chen⋄ and Tuo Zhao† †Georgia Institute of Technology ‡Princeton University ⋄Microsoft Azure AI {qingru.zhang,abukharin3,tourzhao}@gatech.edu mc0750@princeton.edu {nikosk,penhe,yu.cheng,wzchen}@microsoft.com ABSTRACT Fine-tuning large pre-trained language models on downstream tasks has become an important paradigm in NLP. However, common practice fine-tunes all of the parameters in a pre-trained model, which becomes prohibitive when a large number of downstream tasks are present. Therefore, many fine-tuning methods are proposed to learn incremental updates of pre-trained weights in a parameter efficient way, e.g., low-rank increments. These methods often evenly distribute the budget of incremental updates across all pre-trained weight matrices, and overlook the varying importance of different weight parameters. As a consequence, the fine- tuning performance is suboptimal. To bridge this gap, we propose AdaLoRA, which adaptively allocates the parameter budget among weight matrices according to their importance score. In particular, AdaLoRA parameterizes the incremental updates in the form of singular value decomposition. Such a novel approach allows us to effectively prune the singular values of unimportant updates, which is essentially to reduce their parameter budget but circumvent intensive exact SVD computations. We conduct extensive experiments with several pre-trained models on natural language processing, question answering, and natural language generation to validate the effectiveness of AdaLoRA. Results demonstrate that AdaLoRA manifests notable improvement over baselines, especially in the low budget settings. Our code is publicly available at https://github.com/ QingruZhang/AdaLoRA. 1 I NTRODUCTION Pre-trained language models (PLMs) have manifested superior performance in various natural language processing tasks (Devlin et al., 2019; Liu et al., 2019; He et al., 2021b; Radford et al., 2019; Brown et al., 2020). The most common way to adapt pre-trained models to down-stream tasks is to fine-tune all the parameters (full fine-tuning, Qiu et al. (2020); Raffel et al. (2020)). However, pre-trained models typically incurs large memory footprint. For example, BERT model (Devlin et al., 2019) consists up to 300 million parameters; T5 (Raffel et al., 2020) comprises up to 11 billion parameters and GPT-3 (Brown et al., 2020) contains up to 175 billion parameters. When building a NLP system upon these pre-trained models, we usually handle multiple tasks that arrive simultaneously (Radford et al., 2019). Given a large number of down-stream tasks, full fine-tuning requires that each task maintains a separated copy of large models. The resulting memory consumption is prohibitively expensive. To address this issue, researchers have proposed two main lines of research to reduce the fine-tuning parameters, while maintaining or even improving the performance of PLMs. Specifically, one line of research focuses on adding small neural modules to PLMs and fine-tune only these modules for each task – the base model is kept frozen and shared across tasks. In this way, only a small number of task-specific parameters are introduced and updated, greatly enhancing the practicality of large models. For example, adapter tuning (Houlsby et al., 2019; Rebuffi et al., 2017; Pfeiffer et al., 2020; ∗Work was done during Qingru Zhang’s internship at Microsoft Azure AI. 1 arXiv:2303.10512v2  [cs.CL]  20 Dec 2023Published as a conference paper at ICLR 2023 Wq Wk Wv Wo Wf1 Wf2 88.50 88.75 89.00 89.25 89.50 89.75 90.00 MNLI Matched Acc 88.58 88.98 89.36 89.28 89.91 89.99 (a) Selected weight matrix 1,2,3 4,5,6 7,8,9 10,11,12 78 80 82 84 86 88MNLI Matched Acc 77.87 85.82 88.15 88.6 (b) Selected layers Figure 1: Given the total trainable parameters as 0.28M, we apply LoRA only to selected weight matrices (left) or selected layers (right) of DeBERTaV3-base and compare the fine-tuning performance on MNLI-m. Figure 1a: we only fine-tune a selected type of weight matrix of every transformer layer, including query/key/value projection (Wq, Wk, Wv), output projection (Wo) in the self-attention, and two weight matrices (Wf1 , Wf2 ) in two-layer FFNs. In Figure 1b, we apply LoRA to every weight matrix of the selected layers. He et al., 2022) inserts small neural modules called adapters between the layers of the base model. Prefix tuning (Li & Liang, 2021) and prompt tuning (Lester et al., 2021) attach additional trainable prefix tokens to the input or hidden layers of the base model. These methods have shown to achieve comparable performance to full fine-tuning, while only updating less than 1% of the original model parameters, significantly releasing the memory consumption. Another line of research proposes to model the incremental update of the pre-trained weights in a parameter-efficient way, without modifying the model architecture (Zaken et al., 2021; Guo et al., 2020; Hu et al., 2022). Given a pre-trained weight matrix1 W(0), for example, diff pruning (Guo et al., 2020) models its incremental update ∆ as a sparse matrix. Diff pruning initializes ∆ as the same dimension as W(0) and then prunes ∆ element-wise based on the magnitude of the entries. As such, diff pruning can increase the parameter efficiency substantially by adaptively retaining important updates and pruning unimportant ones. Nonetheless, diff pruning has several limitations. First, it relies on low-level implementation to speed up the computation of unstructured sparse matrices, which is not well supported by existing deep learning frameworks. Therefore, we have to store ∆ as a dense matrix during training. Second, it needs to update every entry of ∆ with their gradients and then prune them. This results in similar computational cost as full fine-tuning (Guo et al., 2020). To overcome these drawbacks, Hu et al. (2022) propose a method named LoRA, which parameterizes ∆ as a low-rank matrix by the product of two much smaller matrices: W = W(0) + ∆ = W(0) + BA, (1) where W(0), ∆ ∈ Rd1×d2 , A ∈ Rr×d2 and B ∈ Rd1×r with r ≪ {d1, d2}. During fine-tuning, only A and B are updated. The rank r is chosen to be much smaller than the dimension of W (e.g., r = 8 when d1 = d2 = 1024). With less than 0.5% additional trainable parameters, the training overhead can be reduced up to 70%, compared to full fine-tuning. However, LoRA achieves comparable or even better performance than full fine-tuning (Hu et al., 2022). Meanwhile, the product of two samll matrices is more friendly to implement and deploy than unstructured sparse matrices in diff pruning. LoRA still has limitations as it prespecifies the rank r of each incremental matrix ∆ identical. This ignores the fact that the importance of weight matrices varies significantly across modules and layers when fine-tuning pre-trained models. To illustrate this point, we present an concrete example in Figure 1. We compare the performance of LoRA when fine-tuning specific modules or layers with the same number of trainable parameters. Figure 1a shows that fine-tuning feed-forward networks (FFN) achieves better performance than self-attention modules. In addition, Figure 1b demonstrates that weight matrices in top layers are more important than those in bottom layers. Adding more trainable parameters to the critical weight matrices can lead to better model performance. In contrast, adding more parameters to those less important weight matrices yields very marginal gains or even hurt model performance. Given the parameter budget, i.e., the number of total trainable parameters, we always prefer to allocate more parameters to those important modules. Distributing the budget evenly to all weight matrices/layers, like LoRA and other methods (e.g., adapter and prefix tuning), often gives suboptimal performance. To this end, a natural question is: How can we allocate the parameter budget adaptively according to importance of modules to improve the performance of parameter-efficient fine-tuning? 1Unless specified otherwise, we use W(0) to denote any pre-trained weight matrix. 2Published as a conference paper at ICLR 2023 To answer this question, we propose a new method –AdaLoRA (Adaptive Low-Rank Adaptation), which dynamically allocates the parameter budget among weight matrices during LoRA-alike fine- tuning. Specifically, AdaLoRA adjusts the rank of incremental matrices to control their budget. Critical incremental matrices are assigned with high rank such that they can capture more fine-grained and task-specific information. Less importance ones are pruned to have lower rank to prevent overfitting and save the computational budget. There are some methods to control the rank of matrices in the existing literature of matrix approximation (Cai et al., 2010; Koltchinskii et al., 2011; Toh & Yun, 2010). Most of them directly compute singular value decomposition (SVD) of a matrix and then truncate the smallest singular values. Such an operation can manipulate the rank explicitly and, more importantly, minimize the difference between the resulting matrix and the original matrix. However, for fine-tuning large models, it becomes prohibitively expensive to iteratively apply SVD for a large number of high-dimensional weight matrices. Therefore, instead of computing SVD exactly, we parameterize ∆ as ∆ = PΛQ to mimic SVD. The diagonal matrix Λ contains singular values while the orthogonal matrices P and Q represent left/right singular vectors of ∆. To regularize the orthogonality of P and Q, an additional penalty is added to training loss. Such a parameterization avoids the intensive computations of SVD. Besides, another advantage is that we only need to drop the unimportant singular values while the singular vectors are maintained. This preserves the possibility of future recovery and stabilizes the training. See a detailed comparison to LoRA in Section 3. Based on our SVD parameterization, AdaLoRA dynamically adjusts the rank of ∆ = PΛQ by importance scoring. Specifically, we divide the incremental matrix PΛQ into triplets, where each triplet Gi contains the i-th singular value and the corresponding singular vectors. To quantify the importance of triplets, we propose a novel importance metric, which takes account of the contribution of every entry in Gi to the model performance (Sanh et al., 2020; Liang et al., 2021; Zhang et al., 2022). Triplets with low importance scores are granted low priority and hence the singular values are zeroed out. Triplets with high importance are retained for fine-tuning. Moreover, we also propose a global budget scheduler to facilitate the training. In particular, we start from an initial parameter budget, which is slightly higher than the final budget, and then gradually reduce it until matching the target. Such a scheduler can improve the training stability and model performance. Please see Section 3 for a detailed description of our importance metric and budget scheduler. We conduct extensive experiments on a wide range of tasks and models to demonstrate the effec- tiveness of AdaLoRA. Specifically, we evaluate the performance using DeBERTaV3-base (He et al., 2021a) on natural language understanding (GLUE, Wang et al. (2019)) and question answering (SQuADv1, Rajpurkar et al. (2016) and SQuADv2, Rajpurkar et al. (2018)) datasets. We also apply our methods to BART-large (Lewis et al., 2019) and evaluate the performance on natural language generation (XSum, Narayan et al. (2018) and CNN/DailyMail, Hermann et al. (2015)) tasks. We show AdaLoRA consistently outperforms the baseline, especially under low budget settings. For example, with less than 0.1% trainable parameters of full fine-tuning, AdaLoRA achieves a 1.2% F1 improvement on the SQuAD2.0 dataset compared with state-of-the-art approaches. 2 B ACKGROUND Transformer-based Models. A typical transformer model consists of L stacked blocks, where each block contains two submodules: a multi-head attention (MHA) and a fully connected FFN. Given the input sequence X ∈ Rn×d, MHA performs the attention function in parallel h heads: MHA (X) = Concat(head1, ...,headh)Wo, headi = Softmax \u0010 XWqi (XWki )⊤/ p dh \u0011 XWvi , where Wo ∈ Rd×d is an output projection and Wqi , Wki , Wvi ∈ Rd×dh are query, key and value projections of head i. dh is typically set to d/h. The other important module is a FFN which consists of two linear transformations with a ReLU activation in between: FFN(X) = ReLU(XWf1 + b1)Wf2 + b2, where Wf1 ∈ Rd×dm and Wf2 ∈ Rdm×d. Finally, a residual connection is used followed by a layer normalization (Ba et al., 2016). Low Rank Adaptation.LoRA (Hu et al., 2022) models the incremental update of the pre-trained weights by the product of two small matrices. For h = W(0)x, the modified forward pass is: h = W(0)x + ∆x = W(0)x + BAx, (2) where W(0), ∆ ∈ Rd1×d2 , A ∈ Rr×d2 and B ∈ Rd1×r with r ≪ {d1, d2}. A typically adopts a random Gaussion initialization while B is initialized with zero to have ∆ = 0 at the beginning of 3Published as a conference paper at ICLR 2023 training. We further denoteAi∗ as the i-th row ofA, B∗i as the i-th column ofB, and Gi = {Ai∗, B∗i} as the i-th doublet. Hu et al. (2022) only apply LoRA to query and value projections (i.e, Wq and Wv) in the MHAs. He et al. (2022) extend it to weight matrices of FFNs (i.e, Wf1 and Wf2 ), leading to the performance improvement . Meanwhile, they propose a unified view of various efficient tuning methods including adapter tuning, prefix tuning and LoRA. 3 A DALORA M ETHOD Our method contains two important components: (i) SVD-based adaptation, which formulates the incremental matrices in the form of singular value decomposition; (ii) Importance-aware rank allocation, which prunes redundant singular values based on our newly-designed importance metric. 3.1 SVD-B ASED ADAPTATION As mentioned in Section 1, we propose to parameterize the incremental updates of the pre-trained weight matrices in the form of singular value decomposition: W = W(0) + ∆ = W(0) + PΛQ, (3) where P ∈ Rd1×r and Q ∈ Rr×d2 represent the left/right singular vectors of ∆ and the diagonal matrix Λ ∈ Rr×r contains the singular values {λi}1≤i≤r with r ≪ min(d1, d2). We further denote Gi = {P∗i, λi, Qi∗} as the triplet containing the i-th singular value and vectors. In practice, since Λ is diagonal, we only need to save it as a vector in Rr. Λ is initialized with zero while P and Q adopt a random Gaussian initialization to ensure ∆ = 0 at the beginning of training. To enforce the orthogonality of P and Q, i.e., P⊤P = QQ⊤ = I, we utilize the following regularizer2: R(P, Q) = ∥P⊤P − I∥2 F + ∥QQ⊤ − I∥2 F. (4) In our method, Λ is iteratively pruned to adjust the rank after each gradient decent step. As mentioned in Section 1, one can directly compute SVD for every ∆ to manipulate singular values. The computational complexity, however, is O(min(d1, d2)d1d2). It becomes extremely expensive to iteratively apply SVD for a large number of high-dimensional incremental matrices. In contrast, our parameterization avoids intensive SVD computation, greatly releasing the computational overhead. We remark that one can also apply structured pruning to LoRA to control the rank (i.e., pruneBA doublet-wise in (1)), whereas it has the following disadvantages. First, when a doublet is measured as unimportant, we have to prune all of its elements. It makes scarcely possible to reactivate the pruned doublets as their entries are all zeroed out and not trained. In contrast, AdaLoRA only masks out the singular values based on (3) while the singular vectors are always maintained. It preserves the potential of future recovery for the triplets dropped by mistake. Second, A and B of LoRA are not orthogonal, meaning the doublets can be dependent with each other. Discarding the doublets can incur larger variation from the original matrix than truncating the smallest singular values. Therefore, the incremental matrices are often altered dramatically after each step of rank allocation, which causes training instability and even hurts generalization. To demonstrate this point, we present an ablation study in Section 4.4, which compares AdaLoRA with structured pruning for LoRA. 3.2 I MPORTANCE -AWARE RANK ALLOCATION We apply the SVD-based adaptation (3) to every weight matrix including Wq, Wk, Wv, Wf1 and Wf2 of each transformer layer. In order to control the budget, we iteratively prune singular values in correspondence to their importance score during the training. For clear reference, we use k to index the incremental matrix, i.e., ∆k = PkΛkQk for k = 1 , . . . , n, where n is the number of adapted weight matrices. We denote the i-th triplet of ∆k as Gk,i = {Pk,∗i, λk,i, Qk,i∗} and its importance score as Sk,i. We further denote the parameter sets P = {Pk}n k=1, E = {Λk}n k=1, Q = {Qk}n k=1 and training cost as C(P, E, Q). With the regularization (4), the training objective is given by L(P, E, Q) = C(P, E, Q) + γ Pn k=1 R(Pk, Qk), where γ > 0 is the regularization coefficient. At the t-th step, we first take a stochastic gradient step to update P(t) k , Λ(t) k and Q(t) k for k = 1, . . . , n. Specifically, for Λ(t) k ˜Λ(t) k = Λ(t) k − η∇Λk L(P(t), E(t), Q(t)), (5) 2We present the experiments in Appendix G to verify the effectiveness of the regularization. 4Published as a conference paper at ICLR 2023 where η > 0 is learning rate. Then, given importance score S(t) k , the singular values are pruned following Λ(t+1) k = T (˜Λ(t) k , S(t) k ), with T (˜Λ(t) k , S(t) k )ii = \u001a ˜Λ(t) k,ii S(t) k,i is in the top-b(t) of S(t), 0 otherwise, (6) where S(t) = {S(t) k,i}1≤k≤n,1≤i≤r contains the importance score of all triplets. Here b(t) is the budget of remaining singular values at the t-th step, which we explain more in Section 3.3. In this way, we leave more budget to the incremental matrices of higher priority by pruning the singular values of less important ones. In the sequel, we introduce several options to design the importance score. Magnitude of singular valuesis the most straightforward way to quantify the importance of every triplet, i.e., Sk,i = |λk,i|. In this way, only the least significant singular values are discarded. It minimizes the deviation from the original matrix and further stabilizes the training. Many existing methods use this criterion to control the rank of matrix (Cai et al., 2010; Koltchinskii et al., 2011; Toh & Yun, 2010). However, we remark that such a simple metric cannot properly quantify the contribution of parameters to model performance. Sensitivity-based importanceis another option for importance scoring, which quantifies the sensi- tivity of parameters to the training loss (Molchanov et al., 2019; Sanh et al., 2020; Liang et al., 2021; Zhang et al., 2022). The prior work, however, leverages the sensitivity to quantify the importance of single entries and applies it for unstructured pruning that prunes weights element-wise. When it turns to our case, we have to design a new metric as the triplets are discarded group-wise. Every entry’s sensitivity ought to be considered and properly combined to quantify the overall contribution of the triplet to model performance. Therefore, we propose a newly-designed importance metric in account of both the singular value and vectors in triplet Gk,i: Sk,i = s(λk,i) + 1 d1 d1X j=1 s(Pk,ji) + 1 d2 d2X j=1 s(Qk,ij), (7) where we calculate the mean importance of Pk,∗i and Qk,i∗ such that Sk,i does not scale with the number of parameters in Gk,i. Here s(·) is a specific importance function for single entries. We can adopt the sensitivity for s(·), which is defined as the magnitude of the gradient-weight product: I(wij) = |wij∇wij L|, (8) where wij is any trainable parameter. (8) essentially approximates the change in loss when a parameter is zeroed out. If the removal of a parameter has a large influence, then the model is sensitive to it and we should retain it (Molchanov et al., 2019; Liang et al., 2021; Zhang et al., 2022). However, Zhang et al. (2022) point out that the sensitivity in (8) is not yet a reliable importance indi- cator. Such a score is estimated on the sampled mini batch. The stochastic sampling and complicated training dynamics incur high variability and large uncertainty for estimating the sensitivity with (8). Therefore, Zhang et al. (2022) propose to resolve this issue by sensitivity smoothing and uncertainty quantification: I (t) (wij) =β1I (t−1) (wij) + (1− β1)I(t)(wij) (9) U (t) (wij) =β2U (t−1) (wij) + (1− β2) \f\f\fI(t)(wij) − I (t) (wij) \f\f\f, (10) where 0 < β1, β2 < 1. I (t) is the smoothed sensitivity by exponential moving average and U (t) is the uncertainty term quantified by the local variation between I(t) and I (t) . Then they define the importance as the product between I (t) and U (t) , which can be another option for s(·): s(t)(wij) = I (t) (wij) · U (t) (wij). (11) We present a detailed ablation study in Section 4.4 to compare the performance of different importance metrics. We find the proposed metric (7) based on the sensitivity variant (11) generally performs best. We summarize the detailed algorithm in Algorithm 1. 5Published as a conference paper at ICLR 2023 Algorithm 1AdaLoRA 1: Input: Dataset D; total iterations T; budget schedule {b(t)}T t=0; hyperparameters η, γ, β1, β2. 2: for t = 1, . . . , Tdo 3: Sample a mini-batch from D and compute the gradient ∇L(P, E, Q); 4: Compute the sensitivity I(t) in (8) for every parameter in {P, E, Q}; 5: Update I (t) as (9) and U (t) as (10) for every parameter in {P, E, Q}; 6: Compute S(t) k,i by (7), for k = 1, . . . , nand i = 1, . . . , r; 7: Update P(t+1) k = P(t) k − η∇Pk L(P, E, Q) and Q(t+1) k = Q(t) k − η∇Qk L(P, E, Q); 8: Update Λ(t+1) k = T (Λ(t) k − η∇Λk L(P, E, Q), S(t) k ) given the budget b(t). 9: end for 10: Output: The fine-tuned parameters {P(T), E(T), Q(T)}. 3.3 G LOBAL BUDGET SCHEDULER As mentioned in Section 1, adjusting the rank is naturally to control the parameter budget in the context of low-rank adaptation. Hence we define the budget b(t) as the total rank of all incremental matrices, i.e., the number of total singular values. Recall that the budget allocation is iteratively conducted during the fine-tuning. To facilitate the training, we propose a global budget scheduler. Specifically, we start from an initial budgetb(0) that is slightly higher than the target budgetb(T) (e.g., 1.5 times of b(T)). We set the initial rank of each incremental matrix as r = b(0)/n. We warm up the training for ti steps, and then follow a cubic schedule to decrease the budget b(t) until it reaches b(T). Finally, we fix the resulting budget distribution and fine-tune the model for tf steps. The exact equation for the budget schedule is presented in Appendix A. This allows AdaLoRA to explore the parameter space first and then focus on the most important weights later. 4 E XPERIMENTS We implement AdaLoRA for fine-tuning DeBERTaV3-base (He et al., 2021a) and BART-large (Lewis et al., 2019). We evaluate the effectiveness of the proposed algorithm on natural language understanding (GLUE, Wang et al. (2019)), question answering (SQuADv1, Rajpurkar et al. (2016) and SQuADv2, Rajpurkar et al. (2018)), and natural language generation (XSum, Narayan et al. (2018) and CNN/DailyMail Hermann et al. (2015)). All the gains have passed significant tests with p <0.05. Implementation Details. We use PyTorch(Paszke et al., 2019) to implement all the algorithms. Our implementation is based on the publicly available Huggingface Transformers3 (Wolf et al., 2019) code-base. All the experiments are conducted on NVIDIA V100 GPUs. LoRA scales ∆x by α/r where α is a constant in r. As a result, the magnitude of output can be consistent given different r. It reduces the efforts of retuning learning rate when varying r. Typically α is set as 16 or 32 and never tuned (Hu et al., 2022; Yang & Hu, 2020). Following LoRA, we add the same scaling for (3) and fix α as LoRA. Besides, in Algorithm 1, we prune singular values every ∆T steps (e.g., ∆T = 100) such that the pruned triplets can still get updated within these intervals and possibly reactivated in future iterations. Baselines. We compare AdaLoRA with the following methods: • Full fine-tuning is the most common approach for adaptation. During fine-tuning, the model is initialized with pre-trained weights and biases, and all model parameters undergo gradient updates. • Bitfit (Zaken et al., 2021) is an effective parameter-efficient fine-tuning method. The method only fine-tunes bias vectors in the pre-trained model. • Adapter tuning (Houlsby et al., 2019; Pfeiffer et al., 2020) inserts two-layer adapters between transformer blocks. We compare with two types of adapter. Houlsby adapter as proposed in Houlsby et al. (2019) is inserted between the self-attention module and the FFN module followed by a subsequent residual connection. Recently, Pfeiffer et al. (2020) propose a more efficient design with adapters only applied after FFN modules and LayerNorm modules (Ba et al., 2016), which we call 3https://github.com/huggingface/transformers 6Published as a conference paper at ICLR 2023 Table 1: Results with DeBERTaV3-base on GLUE development set. The best results on each dataset are shown in bold. We report the average correlation for STS-B.Full FT, HAdapter and PAdapter represent full fine-tuning, Houlsby adapter, and Pfeiffer adapter respectively. We report mean of5 runs using different random seeds. Method # Params MNLI SST-2 CoLA QQP QNLI RTE MRPC STS-B All m/mm Acc Mcc Acc/F1 Acc Acc Acc Corr Ave. Full FT 184M 89.90/90.12 95.63 69.19 92.40/89.80 94.03 83.75 89.46 91.60 88.09 BitFit 0.1M 89.37/89.91 94.84 66.96 88.41/84.95 92.24 78.70 87.75 91.35 86.02 HAdapter 1.22M 90.13/90.17 95.53 68.64 91.91/89.27 94.11 84.48 89.95 91.48 88.12 PAdapter 1.18M 90.33/90.39 95.61 68.77 92.04/89.40 94.29 85.20 89.46 91.54 88.24 LoRAr=8 1.33M 90.65/90.69 94.95 69.82 91.99/89.38 93.87 85.20 89.95 91.60 88.34 AdaLoRA 1.27M 90.76/90.79 96.10 71.45 92.23/89.74 94.55 88.09 90.69 91.84 89.31 HAdapter 0.61M 90.12/90.23 95.30 67.87 91.65/88.95 93.76 85.56 89.22 91.30 87.93 PAdapter 0.60M 90.15/90.28 95.53 69.48 91.62/88.86 93.98 84.12 89.22 91.52 88.04 HAdapter 0.31M 90.10/90.02 95.41 67.65 91.54/88.81 93.52 83.39 89.25 91.31 87.60 PAdapter 0.30M 89.89/90.06 94.72 69.06 91.40/88.62 93.87 84.48 89.71 91.38 87.90 LoRAr=2 0.33M 90.30/90.38 94.95 68.71 91.61/88.91 94.03 85.56 89.71 91.68 88.15 AdaLoRA 0.32M 90.66/90.70 95.80 70.04 91.78/89.16 94.49 87.36 90.44 91.63 88.86 Pfeiffer adapter. The number of trainable parameters is determined by the number of layers, the hidden dimension of adapters and the dimension of their inputs. • LoRA (Hu et al., 2022) is a state-of-the-art method for parameter-efficient fine-tuning. The method parameterizes incremental updates by two small matrices and only fine-tune them. The number of trainable parameter is controlled by the rank r and the number of adapted weight matrices n. Hu et al. (2022) apply LoRA to query and value projections only. In empirical, we find that applying LoRA to all weight matrices, i.e., Wq, Wk, Wv, Wf1 and Wf2 , can further improve its performance (Please see Appendix F). Hence, we compare with this generalized LoRA to maximize its performance. We use publicly available implementation 4 to run all the baselines. Please refer to Hu et al. (2022) and reference therein for details. 4.1 N ATURAL LANGUAGE UNDERSTANDING Models and Datasets.We evaluate the fine-tuning performance of DeBERTaV3-base (He et al., 2021a) using the proposed algorithm. We conduct experiments on the General Language Understand- ing Evaluation (GLUE, Wang et al. 2019) benchmark. The benchmark includes two single-sentence classification tasks, three similarity and paraphrase tasks and four natural language inference tasks. Dataset details are summarized in Appendix B. Implementation Details. DeBERTaV3-base consists of 183 millions parameters. We compare AdaLoRA with the baselines under different budget levels, for example, given the total trainable parameters as 0.3/0.6/1.2 million. In order to match the parameter budget, we select the hidden dimensions of adapters from {8, 16, 32, 64}, set the rank r of LoRA as {2, 4, 8}, and choose the final budget b(T) of AdaLoRA from {144, 288, 576}. Then we set b(0) as 1.5 times of b(T) for AdaLoRA and select the regularization coefficient γ from {0.1, 0.3, 0.5}. We set the exponential moving average parameters β1 and β2 as their default value 0.85. We select the learning rate from {5 × 10−5, 8 × 10−5, 1 × 10−4, 2 × 10−4}. More details are presented in Appendix C. Main results.We compare AdaLoRA with the baseline methods under different budget settings. Table 1 shows experimental results on the GLUE development set. We see that AdaLoRA achieves better or on par performance compared with existing approaches on all datasets under all budget levels. For example, when the parameter budget is 0.3M, AdaLoRA achieves 87.36% accuracy on RTE, which is 1.8% higher than the best-performing baseline. Besides, AdaLoRA with extreme low budget can often perform better than the baselines with higher budget. For example, AdaLoRA achieve 70.04% Mcc. score on CoLA with 0.3M fine-tuning parameters, which is higher than all baseline methods with lager budget (e.g., 0.6M and 1.2M). 4.2 Q UESTION ANSWERING Models and Datasets.We evaluate performance of the proposed algorithm on two question answering (QA) datasets: SQuAD v1.1 (Rajpurkar et al., 2016) and SQuADv2.0 (Rajpurkar et al., 2018), where 4https://github.com/microsoft/LoRA 7Published as a conference paper at ICLR 2023 Table 2: Results with DeBERTaV3-base on SQuAD v1.1 and SQuADv2.0. Here # Params is the number of trainable parameters relative to that in full fine-tuning. We report EM/F1. The best results in each setting are shown in bold. SQuADv1.1 SQuADv2.0 Full FT 86.0 / 92.7 85.4 / 88.4 # Params 0.08% 0.16% 0.32% 0.65% 0.08% 0.16% 0.32% 0.65% HAdapter 84.4/91.5 85.3/92.1 86.1/92.7 86.7/92.9 83.4/86.6 84.3/87.3 84.9/87.9 85.4/88.3 PAdapter 84.4/91.7 85.9/92.5 86.2/92.8 86.6/93.0 84.2/87.2 84.5/87.6 84.9/87.8 84.5/87.5 LoRA 86.4/92.8 86.6/92.9 86.7/93.1 86.7/93.1 84.7/87.5 83.6/86.7 84.5/87.4 85.0/88.0 AdaLoRA 87.2/93.4 87.5/93.6 87.5/93.7 87.6/93.7 85.6/88.7 85.7/88.8 85.5/88.6 86.0/88.9 we use AdaLoRA to fine-tune DeBERTaV3-base. These tasks are treated as a sequence labeling problem, where we predict the probability of each token being the start and end of the answer span. Dataset details can be found in Appendix D. Implementation Details.We compare AdaLoRA with the baseline methods under different parameter budgets. That is we have the number of trainable parameters as 0.08%/0.16%/0.32%/0.65% of total pre-trained parameters. To match the budget requirements, we select the hidden dimensions of adapters from {4, 8, 16, 32, 64}, set the rank r of LoRA as {1, 2, 4, 8} and choose the final total rank b(T) of AdaLoRA from {72, 144, 288, 576}. We set the batch size as 16. We use AdamW (Loshchilov & Hutter, 2019) as the optimizer and we set the learning rate as 1 × 10−3 for AdaLoRA. Please refer to Appendix D for more details. Main Results. Table 2 summarizes experimental results when we fine-tune DeBERTaV3-base under 4 different budget settings: 0.08%, 0.16%, 0.32% and 0.65% of total pre-trained parameters. From the result, we see that AdaLoRA consistently outperforms existing approaches under all the budget levels in term of two evaluation metrics: exact match (EM) and F1. Notice that the performance of Houlsby adapter and Pfeiffer adapter are notably decreased when we reduce the parameter budget. In contrast, our method shows the consistent performance under different budget levels. For example, AdaLoRA achieves 88.7% F1 on SQuADv2.0 with the smallest budget 0.08%. It is close to its performance under the high budget and it is also 1.2% higher than the best-performing baseline. 4.3 N ATURAL LANGUAGE GENERATION Table 3: Results with BART-large on XSum and CNN/DailyMail. Here# Params is the number of trainable parameters relative to that in full fine-tuning. We report R-1/2/L. The best results are shown in bold. # Params Method XSum CNN/DailyMail 100% Full FT 45.49 / 22.33 / 37.26 44.16 / 21.28 / 40.90 2.20% LoRA 43.95 / 20.72 / 35.68 45.03 / 21.84 / 42.15 AdaLoRA 44.72 / 21.46 / 36.4645.00 / 21.89 / 42.16 1.10% LoRA 43.40 / 20.20 / 35.20 44.72 / 21.58 / 41.84 AdaLoRA 44.35 / 21.13 / 36.1344.96 / 21.77 / 42.09 0.26% LoRA 43.18 / 19.89 / 34.92 43.95 / 20.91 / 40.98 AdaLoRA 43.55 / 20.17 / 35.2044.39 / 21.28 / 41.50 0.13% LoRA 42.81 / 19.68 / 34.73 43.68 / 20.63 / 40.71 AdaLoRA 43.29 / 19.95 / 35.0443.94 / 20.83 / 40.96 Models and Datasets.To provide a comparison with the state-of-the-art in natural language gener- ation (NLG) tasks, we apply AdaLoRA to fine-tune a BART-large model (Lewis et al., 2019). We evaluate model performance on two datasets: XSum (Narayan et al., 2018) and CNN/DailyMail (Hermann et al., 2015). Implementation Details.Similarly as DeBERTav3-base, we apply low-rank/SVD-based adaptation to every weight matrix of both encoder and decoder layers. We report ROUGE 1/2/L scores (R-1/2/L, Lin (2004)). We set the training epochs as 15. For XSum, we set the beam length as 8 and batch size 8Published as a conference paper at ICLR 2023 as 64. For CNN/DailyMail, we set the beam length as 4 and batch size as 32. Please see Appendix E for the detailed configuration. Main Results.Experimental results are summarized in Table 3, where we compare the fine-tuning performance under four budget levels: the number of trainable parameters is 0.13%, 0.26%, 1.10% and 2.20% of total pre-trained parameters. We see that AdaLoRA achieves better or on par performance compared with the baseline on both datasets (XSum and CNN/DailyMail) under all the budget levels. For example, AdaLoRA achieves 21.13 R-2 score when budget level is 1.10%, compared with 19.89 for LoRA. 4.4 A NALYSIS Different budget levels.Figure 2 illustrates experimental results of fine-tuning DeBERTaV3-base under different budget levels. We see that on all the three datasets (MNLI-m, SQuADv2.0 and XSum), AdaLoRA achieves consistent performance improvement under all the budget levels compared with the baseline. The performance gain is more significant when increasing the budget for the XSum task, suggesting a high budget can help NLG tasks. Note that on the MNLI and SQuADv2.0 datasets, the performance of AdaLoRA under low budget levels (≤ 1%) can match the results of high budget settings. For example, AdaLoRA achieves 88.78% F1 on SQuADv2.0 when the budget is 0.16%. It is close to the performance (88.89% F1) of the highest budget (4.65%) with a more significant gain over the baseline. 0.08 0.16 0.32 0.65 0.96 1.30 1.95 2.88 # Params (%) 90.2 90.3 90.4 90.5 90.6 90.7Acc (MNLI-m) LoRA  AdaLoRA (a) MNLI 0.08 0.16 0.32 0.65 1.30 2.70 4.65 # Params (%) 87.0 87.5 88.0 88.5 89.0 F1  (b) SQuADv2.0 0.13 0.26 1.1 2.2 4.5 7.9 12.5 # Params (%) 20.0 20.5 21.0 21.5 ROUGE-2  (c) XSum Figure 2: Fine-tuning performance under different budget levels. We compare AdaLoRA with the generalized LoRA that applies to every weight matrix. Comparison to low-rank parameterization.As mentioned in Section 3.1, one can alternatively prune LoRA doublet-wise to conduct the rank allocation. In this case, the doublets are zeroed out entirely, raising the barrier to reactivate them. It can cause training instability and hurt the generalization when some crucial doublets are pruned by mistake. In Table 4, we compare AdaLoRA with pruning LoRA on three datasets (SST-2, RTE, and CoLA) to illustrate this point. We apply the same importance score, budget scheduler and training setups as Section 4.1 for pruning LoRA. We can see that AdaLoRA outperforms pruning LoRA on all the datasets under all the budget levels. Table 4: We present two ablation studies in this table: (i) Comparison between AdaLoRA and structured pruning on LoRA. (ii) Comparison of different importance metrics for AdaLoRA. SST-2 RTE CoLA # Params 0.08% 0.16% 0.65% 0.08% 0.16% 0.65% 0.08% 0.16% 0.65% Prune LoRA 94.84 94.50 94.95 86.28 86.15 87.00 66.71 69.29 69.57 AdaLoRA 95.52 95.80 96.10 87.36 87.73 88.09 70.21 70.04 71.45 s(·) = I(·) 94.61 95.30 95.64 87.36 87.71 88.10 66.71 68.83 70.19 Si = |λi| 95.41 95.41 95.87 87.00 86.28 88.00 67.67 68.44 70.38 Variants of the importance score.Recall that in AdaLoRA, the importance score is defined by the sensitivity and uncertainty of every entry in the triplet (7). In Table 4, we examine two variants of the importance score: (i) changing s(·) in (7) to sensitivity-only; (ii) directly defining Si as |λi|. From the results, we can see that the proposed importance score generally performs best. The other two variants can degenerate the model performance up to 0.9%. The role of two components.We remark that both two components of our method - SVD adaptation and adaptive budget allocation, play vital roles for the performance gain. To demonstrate it, we 9Published as a conference paper at ICLR 2023 compare AdaLoRA with the following variants: (i) SVD-LoRA: fine-tuning only with the proposed SVD-based adaptation in (3) and (4); (ii) LoRA regu: LoRA with orthogonal regularization (4) on A and B; (iii) AdaLoRAγ = 0: AdaLoRA without orthogonal regularization (4). Table 5 present the results when fine-tuning DeBERTaVe-base on SST-2 and MNLI. We can see that fine-tuning only with SVD adaptation shows an improvement over LoRA but cannot match the performance of AdaLoRA. Meanwhile, without SVD orthogonal regularization, the performance of AdaLoRA can degenerate. These results validate that both components contribute to the model performance. Table 5: We present ablation studies about SVD-based adaptation, orthogonal regularization, and budget allocation in this table. For MNLI, we report the average score of m/mm acc. SST-2 MNLI # Params 0.08% 0.16% 0.32% 0.65% 0.08% 0.16% 0.32% 0.65% LoRA 94.38 94.95 - 94.95 90.19 90.34 - 90.57 LoRAregu - 94.61 94.72 94.61 - 90.30 90.40 90.66 SVD-LoRA 95.33 95.18 95.07 95.53 90.28 90.25 90.52 90.62 AdaLoRAγ = 0 95.41 95.10 95.30 95.10 90.37 90.34 90.56 90.43 AdaLoRA 95.64 95.80 96.10 96.10 90.65 90.68 90.66 90.77 The resulting budget distribution.Figure 3 shows the resulting rank of each incremental matrix of DeBERTaV3-base fine-tuned with AdaLoRA. We find that AdaLoRA always prefers to allocating more budget to FFNs and top layers. Such behavior aligns with our empirical conclusions presented in Figure 1 that weight matrices of FFN moduels and top layers are more important for model performance. Hence, it validates that our proposed importance metric can guide AdaLoRA to focus on crucial modules. Meanwhile, the rank distribution generated by AdaLoRA is consistent across different budget levels, tasks and models. It means the number of remaining parameters is linearly scaled with b(T) and hence we can tune b(T) to control the remaining parameters. 1 2 3 4 5 6 7 8 9 10 11 12 Layer Wf2 Wf1 Wo Wv Wk Wq 4 1 5 2 3 5 5 6 10 5 5 0 6 9 9 9 12 11 12 12 12 12 12 2 7 3 5 8 8 10 12 12 12 12 12 5 6 6 10 6 10 11 11 11 12 12 11 9 5 4 5 5 10 9 9 11 12 12 12 12 3 2 5 4 7 7 7 10 11 11 10 3 0 2 4 6 8 10 12 The ﬁnal rank Figure 3: The resulting rank of each incremental matrix when fine-tuning DeBERTaV3-base on MNLI with AdaLoRA. Here the x-axis is the layer index and the y-axis represents different types of adapted weight matrices. 5 C ONCLUSION We propose a parameter-efficient fine-tuning method – AdaLoRA that adaptively allocates the parameter budget according to importance scoring. In AdaLoRA, we parameterize the incremental updates of weight matrices in the form of singular value decomposition. Then, we dynamically allocate the parameter budget among incremental matrices by manipulating the singular values based on a new importance metric. Such an a pproach effectively improves the model performance and parameter efficiency. We conduct extensive experiments on natural language processing, question answering and natural language generation tasks. Results show that AdaLoRA outperforms existing approaches. 10Published as a conference paper at ICLR 2023 REFERENCES Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Process- ing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. Jian-Feng Cai, Emmanuel J Cand`es, and Zuowei Shen. A singular value thresholding algorithm for matrix completion. SIAM Journal on optimization, 20(4):1956–1982, 2010. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 4171–4186, Minneapolis, Minnesota, 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. Demi Guo, Alexander M Rush, and Yoon Kim. Parameter-efficient transfer learning with diff pruning. arXiv preprint arXiv:2012.07463, 2020. Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, and Graham Neubig. Towards a unified view of parameter-efficient transfer learning. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=0RDcd5Axok. Pengcheng He, Jianfeng Gao, and Weizhu Chen. Debertav3: Improving deberta using electra-style pre-training with gradient-disentangled embedding sharing. arXiv preprint arXiv:2111.09543, 2021a. Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. Deberta: Decoding-enhanced bert with disentangled attention. In International Conference on Learning Representations, 2021b. Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. Teaching machines to read and comprehend. Advances in neural information processing systems, 28, 2015. Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for nlp. In International Conference on Machine Learning, pp. 2790–2799. PMLR, 2019. Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum? id=nZeVKeeFYf9. Vladimir Koltchinskii, Karim Lounici, and Alexandre B Tsybakov. Nuclear-norm penalization and optimal rates for noisy low-rank matrix completion. The Annals of Statistics, 39(5):2302–2329, 2011. Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 3045–3059, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.243. URL https: //aclanthology.org/2021.emnlp-main.243. 11Published as a conference paper at ICLR 2023 Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv preprint arXiv:1910.13461, 2019. Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (eds.), Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021 , pp. 4582–4597. Association for Computational Linguistics, 2021. doi: 10.18653/v1/2021.acl-long.353. URL https://doi.org/10.18653/v1/2021. acl-long.353. Chen Liang, Simiao Zuo, Minshuo Chen, Haoming Jiang, Xiaodong Liu, Pengcheng He, Tuo Zhao, and Weizhu Chen. Super tickets in pre-trained language models: From model compression to improving generalization. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 6524–6538, Online, 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.510. Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out, pp. 74–81, 2004. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019 . OpenReview.net, 2019. Pavlo Molchanov, Arun Mallya, Stephen Tyree, Iuri Frosio, and Jan Kautz. Importance estimation for neural network pruning. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019, pp. 11264–11272. Computer Vision Foundation / IEEE, 2019. doi: 10.1109/CVPR.2019.01152. Shashi Narayan, Shay B Cohen, and Mirella Lapata. Don’t give me the details, just the sum- mary! topic-aware convolutional neural networks for extreme summarization. arXiv preprint arXiv:1808.08745, 2018. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas K ¨opf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d’Alch´e- Buc, Emily B. Fox, and Roman Garnett (eds.),Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pp. 8024–8035, 2019. Jonas Pfeiffer, Aishwarya Kamath, Andreas R¨uckl´e, Kyunghyun Cho, and Iryna Gurevych. Adapter- fusion: Non-destructive task composition for transfer learning. arXiv preprint arXiv:2005.00247, 2020. Xipeng Qiu, Tianxiang Sun, Yige Xu, Yunfan Shao, Ning Dai, and Xuanjing Huang. Pre-trained models for natural language processing: A survey. Science China Technological Sciences, 63(10): 1872–1897, 2020. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, et al. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21(140):1–67, 2020. 12Published as a conference paper at ICLR 2023 Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pp. 2383–2392, Austin, Texas, 2016. Association for Computational Linguistics. doi: 10.18653/v1/D16-1264. Pranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don’t know: Unanswerable questions for SQuAD. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pp. 784–789, Melbourne, Australia, 2018. Association for Computational Linguistics. doi: 10.18653/v1/P18-2124. Sylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea Vedaldi. Learning multiple visual domains with residual adapters. Advances in neural information processing systems, 30, 2017. Victor Sanh, Thomas Wolf, and Alexander M. Rush. Movement pruning: Adaptive sparsity by fine-tuning. 2020. Kim-Chuan Toh and Sangwoon Yun. An accelerated proximal gradient algorithm for nuclear norm regularized linear least squares problems. Pacific Journal of optimization, 6(615-640):15, 2010. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R ´emi Louf, Morgan Funtowicz, et al. Huggingface’s transformers: State-of-the-art natural language processing. ArXiv preprint, abs/1910.03771, 2019. Greg Yang and Edward J Hu. Feature learning in infinite-width neural networks. arXiv preprint arXiv:2011.14522, 2020. Elad Ben Zaken, Shauli Ravfogel, and Yoav Goldberg. Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models. arXiv preprint arXiv:2106.10199, 2021. Qingru Zhang, Simiao Zuo, Chen Liang, Alexander Bukharin, Pengcheng He, Weizhu Chen, and Tuo Zhao. Platon: Pruning large transformer models with upper confidence bound of weight importance. In International Conference on Machine Learning, pp. 26809–26823. PMLR, 2022. 13Published as a conference paper at ICLR 2023 A G LOBAL BUDGET SCHEDULE As mentioned in Section 3.3, we propose a global budget scheduler to gradually decrease the budget b(t) following a cubic schedule. The detailed equation is given as follows: b(t) =    b(0) 0 ≤ t < ti b(T) + \u0000 b(0) − b(T)\u0001\u0010 1 − t−ti−tf T−ti−tf \u00113 ti ≤ t < T− tf b(T) o.w. . (12) B GLUE D ATASET STATISTICS We present the dataset statistics of GLUE (Wang et al., 2019) in the following table. Table 6: Summary of the GLUE benchmark. Corpus Task #Train #Dev #Test #Label Metrics Single-Sentence Classification (GLUE) CoLA Acceptability 8.5k 1k 1k 2 Matthews corr SST Sentiment 67k 872 1.8k 2 Accuracy Pairwise Text Classification (GLUE) MNLI NLI 393k 20k 20k 3 Accuracy RTE NLI 2.5k 276 3k 2 Accuracy QQP Paraphrase 364k 40k 391k 2 Accuracy/F1 MRPC Paraphrase 3.7k 408 1.7k 2 Accuracy/F1 QNLI QA/NLI 108k 5.7k 5.7k 2 Accuracy Text Similarity (GLUE) STS-B Similarity 7k 1.5k 1.4k 1 Pearson/Spearman corr C N ATURAL LANGUAGE UNDERSTANDING C.1 B UDGET CONFIGURATION For each budget level, we tune the final budget b(T) for AdaLoRA, the rank r for LoRA, the hidden dimension d for two adapters to match the budget requirements. Table 7: Detailed budget setup for GLUE benchmark. # Params Houlsby Adapter (d) Pfeiffer Adapter ( d) LoRA ( r) AdaLoRA ( b(T)) 1.2M 32 64 8 576 0.6M 16 32 4 288 0.3M 8 16 2 144 Alternatively, we can also set the final average rank ¯r(T) = b(T)/n for AdaLoRA to control the budget, which is set as 2, 4, and 8 given the final budget as 144, 288, and 576 respectively. Then we select the initial rank r from {4, 6, 12} for the final average rank {2, 4, 8} respectively. C.2 T RAINING DETAILS We tune the learning rate from {8 × 10−5, 5 × 10−5, 3 × 10−5, 1 × 10−4, 3 × 10−4, 5 × 10−4, 8 × 10−4, 1 × 10−3} and pick the best learning rate for every method. For each dataset, the batch size is set as identical for every method. 14Published as a conference paper at ICLR 2023 Table 8: Hyper-parameter setup of AdaLoRA for GLUE benchmark. Dataset learning rate batch size # epochs γ t i ∆T tf MNLI 5 × 10−4 32 7 0.1 8000 100 50000 RTE 1.2 × 10−3 32 50 0.3 600 1 1800 QNLI 1.2 × 10−3 32 5 0.1 2000 100 8000 MRPC 1 × 10−3 32 30 0.1 600 1 1800 QQP 5 × 10−4 32 5 0.1 8000 100 25000 SST-2 8 × 10−4 32 24 0.1 6000 100 22000 CoLA 5 × 10−4 32 25 0.5 800 10 3500 STS-B 2.2 × 10−3 32 25 0.1 800 10 2000 D Q UESTION ANSWERING D.1 B UDGET CONFIGURATION Given the budget, we control the trainable parameters for each method as the following table. Table 9: Detailed budget setup for question answering. # Params Houlsby Adapter Pfeiffer Adapter LoRA AdaLoRA d d r b (T)/¯r(T)/r 0.65% 32 64 8 576 / 8 / 12 0.32% 16 32 4 288 / 4 / 6 0.16% 8 16 2 144 / 2 / 4 0.08% 4 8 1 72 / 1 / 2 D.2 T RAINING DETAILS We set the batch size as 16. We select the learning rate from {8 × 10−5, 5 × 10−5, 3 × 10−5, 1 × 10−4, 3 × 10−4, 5 × 10−4, 8 × 10−4, 1 × 10−3} and pick the best-performing learning rate for every method. The configuration of AdaLoRA is listed in the following table. Table 10: Hyper-parameter setup of AdaLoRA for question answering tasks. Dataset learning rate batch size # epochs γ t i ∆T tf SQuADv1.1 1 × 10−3 16 10 0.1 5000 100 25000 SQuADv2.0 1 × 10−3 16 12 0.1 5000 100 50000 D.3 D ATASET The statistics of question answering datasets are summarized in Table 11. Table 11: Statistics of the SQuAD dataset. # Train # Validation SQuAD v1.1 87,599 10,570 SQuAD v2.0 130,319 11,873 E N ATURAL LANGUAGE GENERATION E.1 B UDGET CONFIGURATION Given the budget, we control the trainable parameters for each method as the following table. 15Published as a conference paper at ICLR 2023 Table 12: Detailed budget setup for summarization tasks. # Params Houlsby Adapter Pfeiffer Adapter LoRA AdaLoRA d d r b (T)/¯r(T)/r 0.65% 32 64 8 576 / 8 / 12 0.32% 16 32 4 288 / 4 / 6 0.16% 8 16 2 144 / 2 / 4 0.08% 4 8 1 72 / 1 / 2 E.2 T RAINING DETAILS We set the batch size as 16. We select the learning rate from {8 × 10−5, 5 × 10−5, 3 × 10−5, 1 × 10−4, 3 × 10−4, 5 × 10−4, 8 × 10−4, 1 × 10−3} and pick the best-performing learning rate for every method. The configuration of AdaLoRA is listed in the following table. Table 13: Hyper-parameter setup of AdaLoRA for summarization tasks. Dataset learning rate batch size # epochs γ t i ∆T tf XSum 5 × 10−4 64 25 0.1 6000 100 50000 CNN/DailyMail 5 × 10−4 32 15 0.1 5000 100 85000 F A BLATION STUDY FOR LORA As mentioned in Section 4, we find that the performance of LoRA can be further improved when applying it to every weight matrix, compared to fine-tuning Wq and Wv only (Hu et al., 2022). This observation aligns with the empirical results of He et al. (2022). In Table 14, we follow the same training configuration as Section 4.1 and present an ablation study to illustrate this point. Table 14: We compare the fine-tuning performance when apply LoRA to every weight matrix or Wq, Wv only. The parameter budget is fixed as 0.3M. We report accuracy for QQP and MRPC, accuracy(m) for MNLI, and average correlation for STS-B. MNLI QQP CoLA RTE QNLI SST-2 MRPC STS-B LoRA (Wq, Wk) 89.80 90.48 67.04 83.75 93.69 94.84 90.20 91.05 LoRA (all) 90.30 91.61 68.71 85.56 94.31 94.95 90.44 91.68 G O RTHOGONAL REGULARIZATION To verify the effectiveness of (4), we plot ∥P⊤P − I∥2 F and ∥QQ⊤ − I∥2 F to show whether P and Q are regularized to be orthogonal. We fine-tune a DeBERTaV3-base model on SST-2 with AdaLoRA and follow the same training configuration as Section 4.1. We set γ as 0.1 and plot the two terms along the training horizon. From Figure 4, we can see that two regularization terms can be optimized to a very small value (e.g., 0.001) at the beginning of training. Therefore, both P and Q can be enforced to be orthogonal quickly during the initial warm-up of AdaLoRA. It ensures that the triplets are not dependent with each other. H C OMPARISON OF TRAINING COST We compare the training cost between AdaLoRA and LoRA in the following table. We use two methods to fine-tune DeBERTaV3-base on a single NVIDIA V100 GPU. We do training only and set hyperparameters, e.g., batch size and training epochs, the same as in Section 4. Table 15 shows that AdaLoRA incurs 11% additional training time on MNLI and 16% on SQuADv2 under different budgets. The memory footprint of two methods are quite close. Such results demonstrate that AdaLoRA does not incur significant training overheads. The reason behind is that 16Published as a conference paper at ICLR 2023 0 20000 40000 Iterations 10−4 10−3 10−2 10−1 100 ||P⊤P −I||2 F (a) P of Wo at the first layer. 0 20000 40000 Iterations 10−4 10−3 10−2 10−1 100 ||QQ⊤ −I||2 F (b) Q of Wo at the first layer. 0 20000 40000 Iterations 10−4 10−3 10−2 10−1 100 ||P⊤P −I||2 F (c) P of Wf2 at the first layer. 0 20000 40000 Iterations 10−4 10−3 10−2 10−1 ||QQ⊤ −I||2 F (d) Q of Wf2 at the first layer Figure 4: We plot the ∥P⊤P − I∥2 F and ∥QQ⊤ − I∥2 F when fine-tuning DeBERTaV3-base on SST-2. Table 15: Comparison of practical training cost between AdaLoRA and LoRA. Dataset # Param Method GPU Mem Time/epoch MNLI 0.08% LoRA 11.094 GB 105 min AdaLoRA 11.104 GB 116 min 0.16% LoRA 11.098 GB 105 min AdaLoRA 11.110 GB 117 min 0.65% LoRA 11.128 GB 105 min AdaLoRA 11.188 GB 117 min SST-2 0.08% LoRA 13.138 GB 60 min AdaLoRA 13.148 GB 71 min 0.16% LoRA 13.142 GB 61 min AdaLoRA 13.164 GB 71 min 0.65% LoRA 13.170 GB 61 min AdaLoRA 13.226 GB 71 min we only evaluate the importance score for small incremental matrices PΛQ. Their total number of parameters is usually less than 1% of pre-trained weights. Therefore, it does not lead to significant computational cost to update the importance scores of these well-structured small matrices, compared to forward-backward pass of full model. 17",
      "meta_data": {
        "arxiv_id": "2303.10512v2",
        "authors": [
          "Qingru Zhang",
          "Minshuo Chen",
          "Alexander Bukharin",
          "Nikos Karampatziakis",
          "Pengcheng He",
          "Yu Cheng",
          "Weizhu Chen",
          "Tuo Zhao"
        ],
        "published_date": "2023-03-18T22:36:25Z",
        "pdf_url": "https://arxiv.org/pdf/2303.10512v2.pdf",
        "github_url": "https://github.com/microsoft/LoRA"
      },
      "llm_extracted_info": {
        "main_contributions": "The main research problem addressed is the suboptimal performance of parameter-efficient fine-tuning methods that allocate incremental update budgets evenly across pre-trained weight matrices, neglecting their varying importance. AdaLoRA proposes to adaptively allocate this budget based on importance. Key contributions include parameterizing incremental updates using singular value decomposition (SVD) to enable efficient pruning of singular values for less important updates, circumventing intensive exact SVD computations. It introduces a novel importance metric that considers contributions of singular values and vectors to model performance, and a global budget scheduler for stable training. AdaLoRA consistently outperforms baselines, particularly in low-budget scenarios, across NLU, QA, and NLG tasks, achieving significant performance improvements with minimal trainable parameters.",
        "methodology": "AdaLoRA's methodology involves SVD-based adaptation where incremental updates (∆) to pre-trained weights are parameterized as ∆ = PΛQ, mimicking SVD with P and Q as singular vectors and Λ as a diagonal matrix of singular values. Orthogonal regularization (R(P,Q) = ||P^TP - I||^2_F + ||QQ^T - I||^2_F) is applied to ensure orthogonality of P and Q. An importance-aware rank allocation mechanism dynamically prunes singular values. It uses a novel importance metric (Sk,i) for each singular triplet (singular value and corresponding vectors), which combines smoothed sensitivity and uncertainty of each entry's contribution to the training loss. Singular values of triplets with low importance scores are zeroed out to reduce rank. A global budget scheduler facilitates training by starting with a slightly higher initial budget and gradually reducing it via a cubic schedule to the target budget, stabilizing training and allowing exploration before focusing on important weights.",
        "experimental_setup": "Experiments were conducted using DeBERTaV3-base for Natural Language Understanding (NLU) on the GLUE benchmark (including MNLI, SST-2, CoLA, QQP, QNLI, RTE, MRPC, STS-B) and Question Answering (QA) on SQuAD v1.1 and SQuAD v2.0 datasets. For Natural Language Generation (NLG), BART-large was fine-tuned on XSum and CNN/DailyMail datasets. AdaLoRA's performance was compared against several baselines: Full fine-tuning, Bitfit, Houlsby Adapter, Pfeiffer Adapter, and a generalized LoRA applied to all weight matrices (Wq, Wk, Wv, Wf1, Wf2). Evaluation metrics included accuracy, Matthews correlation, F1 score, Exact Match (EM), and ROUGE 1/2/L scores. Experiments were performed under various low parameter budget settings (e.g., 0.08% to 2.2% of total pre-trained parameters). Ablation studies investigated the impact of SVD-based adaptation, orthogonal regularization, the proposed importance metric (comparing it to magnitude-based and sensitivity-only metrics), and structural pruning for LoRA.",
        "limitations": "While AdaLoRA demonstrates strong performance, it incurs an additional training time overhead compared to LoRA, specifically noted as 11% on MNLI and 16% on SQuADv2, despite the authors stating it does not incur \"significant\" overhead. The method also introduces several hyperparameters related to the orthogonal regularization (γ), sensitivity smoothing (β1, β2), and the global budget scheduler (initial budget, warm-up steps, decay steps) that require careful tuning for optimal performance across different tasks and budget levels.",
        "future_research_directions": "Not mentioned",
        "experimental_code": "class GPT2Config(object):    def __init__(        self,        vocab_size_or_config_json_file=50257,        n_positions=1024,        n_ctx=1024,        n_embd=768,        n_layer=12,        n_head=12,        layer_norm_epsilon=1e-5,        initializer_range=0.02,        lora_attn_dim=0,        lora_attn_alpha=128,        lora_dropout=0.0,        lora_r_dropout=0.0,        fix_dropout=0.0,    ):        self.vocab_size = vocab_size_or_config_json_file        self.n_ctx = n_ctx        self.n_positions = n_positions        self.n_embd = n_embd        self.n_layer = n_layer        self.n_head = n_head        self.layer_norm_epsilon = layer_norm_epsilon        self.initializer_range = initializer_range        self.lora_attn_dim = lora_attn_dim        self.lora_attn_alpha = lora_attn_alpha        self.lora_dropout = lora_dropout        self.lora_r_dropout = lora_r_dropout        self.fix_dropout = fix_dropoutclass Attention(nn.Module):    def __init__(self, nx, n_ctx, config, scale=False):        super(Attention, self).__init__()        n_state = nx        assert n_state % config.n_head == 0        self.register_buffer(\"bias\", torch.tril(torch.ones(n_ctx, n_ctx)).view(1, 1, n_ctx, n_ctx))        self.n_head = config.n_head        self.split_size = n_state        self.scale = scale        self.c_attn = lora.MergedLinear(            nx, n_state * 3,             r=config.lora_attn_dim,             lora_alpha=config.lora_attn_alpha,             lora_dropout=config.lora_dropout,             enable_lora=[True, False, True],             fan_in_fan_out=True,            merge_weights=False        )        self.c_proj = Conv1D(n_state, nx)        self.config = config\nif args.lora_dim > 0:\n    lora.mark_only_lora_as_trainable(lm_net)",
        "experimental_info": "LoRA parameters (lora_attn_dim, lora_attn_alpha, lora_dropout) are configurable via command-line arguments. The default values for fine-tuning are:lora_attn_dim: 0 (default, meaning LoRA is disabled unless changed)lora_attn_alpha: 128 (default)lora_dropout: 0.0 (default)The `lora_attn_dim` and `lora_attn_alpha` are also configurable during beam decoding with default values 0 and 128 respectively. If `lora_attn_dim` is greater than 0, LoRA layers are marked as trainable in the model."
      }
    },
    {
      "title": "Parameter-Efficient Fine-Tuning Design Spaces",
      "abstract": "Parameter-efficient fine-tuning aims to achieve performance comparable to\nfine-tuning, using fewer trainable parameters. Several strategies (e.g.,\nAdapters, prefix tuning, BitFit, and LoRA) have been proposed. However, their\ndesigns are hand-crafted separately, and it remains unclear whether certain\ndesign patterns exist for parameter-efficient fine-tuning. Thus, we present a\nparameter-efficient fine-tuning design paradigm and discover design patterns\nthat are applicable to different experimental settings. Instead of focusing on\ndesigning another individual tuning strategy, we introduce parameter-efficient\nfine-tuning design spaces that parameterize tuning structures and tuning\nstrategies. Specifically, any design space is characterized by four components:\nlayer grouping, trainable parameter allocation, tunable groups, and strategy\nassignment. Starting from an initial design space, we progressively refine the\nspace based on the model quality of each design choice and make greedy\nselection at each stage over these four components. We discover the following\ndesign patterns: (i) group layers in a spindle pattern; (ii) allocate the\nnumber of trainable parameters to layers uniformly; (iii) tune all the groups;\n(iv) assign proper tuning strategies to different groups. These design patterns\nresult in new parameter-efficient fine-tuning methods. We show experimentally\nthat these methods consistently and significantly outperform investigated\nparameter-efficient fine-tuning strategies across different backbone models and\ndifferent tasks in natural language processing.",
      "full_text": "PARAMETER -EFFICIENT FINE -TUNING DESIGN SPACES Jiaao Chen†∗, Aston Zhang‡, Xingjian Shi‡, Mu Li‡, Alex Smola‡, Diyi Yang⋄ †Georgia Institute of Technology,‡Amazon Web Services, ⋄Stanford University ABSTRACT Parameter-efﬁcient ﬁne-tuning aims to achieve performance comparable to ﬁne-tuning, using fewer trainable parameters. Several strategies (e.g., Adapters, preﬁx tuning, BitFit, and LoRA) have been proposed. However, their designs are hand-crafted separately, and it remains unclear whether cer- tain design patterns exist for parameter-efﬁcient ﬁne-tuning. Thus, we present a parameter-efﬁcient ﬁne-tuning design paradigm and discover design patterns that are applicable to different experi- mental settings. Instead of focusing on designing another individual tuning strategy, we introduce parameter-efﬁcient ﬁne-tuning design spaces that parameterize tuning structures and tuning strate- gies. Speciﬁcally, any design space is characterized by four components: layer grouping, trainable parameter allocation, tunable groups, and strategy assignment. Starting from an initial design space, we progressively reﬁne the space based on the model quality of each design choice and make greedy selection at each stage over these four components. We discover the following design patterns: (i) group layers in a spindle pattern; (ii) allocate the number of trainable parameters to layers uni- formly; (iii) tune all the groups; (iv) assign proper tuning strategies to different groups. These design patterns result in new parameter-efﬁcient ﬁne-tuning methods. We show experimentally that these methods consistently and signiﬁcantly outperform investigated parameter-efﬁcient ﬁne-tuning strategies across different backbone models and different tasks in natural language processing1. 1 Introduction Large pretrained models have achieved the state-of-the-art performances across a wide variety of downstream natural language processing tasks through ﬁne-tuning on task-speciﬁc labeled data [Devlin et al., 2019, Liu et al., 2019, Yang et al., 2019, Joshi et al., 2019, Sun et al., 2019, Clark et al., 2019, Lewis et al., 2020a, Bao et al., 2020, He et al., 2020, Raffel et al., 2020, Ziems et al., 2022]. However, ﬁne-tuning all the parameters and storing them separately for different tasks is expensive in terms of computation and storage overhead (e.g., 355M parameters for RoBERTa [Liu et al., 2019] and 175B parameters for GPT- 3 [Brown et al., 2020]). This makes it difﬁcult to deploy in real-world natural language processing (NLP) systems composed of multiple tasks. To adapt general knowledge in pretrained models to speciﬁc down-stream tasks in a more parameter-efﬁcient way, various strategies have been proposed where only a small number of (extra) parameters are learned while the remaining pretrained parameters are frozen [Houlsby et al., 2019a, Pfeiffer et al., 2021, Li and Liang, 2021, Brown et al., 2020, Lester et al., 2021a, Schick and Sch ¨utze, 2021, Ziems et al., 2022]. Adapter tuning [Houlsby et al., 2019a] is among the earliest strategies to steer pretrained models with a limited number of parameters. It inserts adapters (small neural modules) to each layer of the pretrained network and only the adapters are trained at the ﬁne-tuning time. Inspired by the success of prompting methods that control pretrained language models through textual prompts [Brown et al., 2020], preﬁx tuning [Li and Liang, 2021] and prompt tuning [Lester et al., 2021b] prepend additional tunable tokens to the input or hidden layers and only train these soft prompts when ﬁne-tuning on downstream tasks. BitFit [Zaken et al., 2021] updates the bias terms in pretrained models while freezing the remaining parameters. LoRA [Hu et al., 2021] decomposes attention weight gradients into low-rank matrices to reduce the number of trainable parameters. With promising results from such research, He et al. [2022] proposed a uniﬁed view of these existing strategies and ∗Work done during an internship at Amazon Web Services. Correspondence to Jiaao Chen<jiaaochen@gatech.edu> and Aston Zhang <astonz@amazon.com>. 1Code is available at: https://github.com/amazon-science/peft-design-spaces . arXiv:2301.01821v1  [cs.CL]  4 Jan 2023P P P L P L A B L A B L… Layer Grouping P L Strategy Assignment Trainable Parameter Allocation Tunable Groups p ⇥ p Figure 1: A parameter-efﬁcient ﬁne-tuning design space. It is characterized by (i) layer grouping (how to group consecutive layers), (ii) trainable parameter allocation (how to allocate the number of trainable parameters to layers), (iii) tunable groups (which groups will be ﬁnetuned), and (iv) strategy assignment (how to assign proper strategies, such as among Adapter, Preﬁx, BitFit, and LoRA, to groups). illustrated differences and connections among them. Like its antecedents, the resulting method is stillequally assigned to different pretrained layers. Despite being effective, most parameter-efﬁcient ﬁne-tuning strategies have been developed via manual design pro- cesses, without much consideration of whether design patterns exist across these different strategies and how such patterns might apply to different backbone models and downstream tasks. Moreover, different strategies are usually applied separately; thus, it is unclear which strategy works best when and where [Mao et al., 2022], as well as how these different strategies reinforce or complement each other. In this light, our goal is to understand the parameter- efﬁcient ﬁne-tuning design in a more comprehensive view and discover design patterns that are both interpretable and applicable across different experimental settings. Instead of designing yet another individual strategy that is equally applied to different pretrained layers, we introduce parameter-efﬁcient ﬁne-tuning design spaces that parameterize both tuning structures and strategies. More con- cretely, any of these design spaces is characterized by four major components as shown in Figure 1: layer grouping, trainable parameter allocation, tunable groups, and strategy assignment. Starting from a relatively unconstrained parameter-efﬁcient ﬁne-tuning design space, we progressively reﬁne the space by comparing the overall quality of models randomly sampled from design spaces enforced with different constraints (e.g., each group has the same number of layers). Throughout the experimental process, we discover several design patterns for parameter-efﬁcient ﬁne-tuning, such as group layers in a spindle pattern, allocate the number of trainable parameters to layers uniformly, tune all the groups, and assign proper tuning strategies to different groups. We fur- ther introduce new parameter-efﬁcient ﬁne-tuning methods that adopt all these discovered design patterns. Extensive experiments show that our methods consistently outperform investigated parameter-efﬁcient ﬁne-tuning strategies. Al- though we use T5 [Raffel et al., 2020] and classiﬁcation tasks as the working example, we ﬁnd that our methods with all these discovered design patters are applicable to other backbones (e.g., RoBERTa [Liu et al., 2019], BART [Lewis et al., 2020b], and XLNet [Yang et al., 2019]) and different natural language processing tasks (e.g., summarization, machine translation, and eight SuperGLUE datasets). Our contributions can be summarized as follows: (i) We introduce parameter-efﬁcient ﬁne-tuning design spaces. (ii) Based on these design spaces, we discover several design patterns in parameter-efﬁcient ﬁne-tuning via comprehen- sive experiments. (iii) Our discovered design patterns lead to parameter-efﬁcient ﬁne-tuning methods, consistently outperforming investigated parameter-efﬁcient ﬁne-tuning strategies across different backbone models and different NLP tasks. 22 Related Work Our work is closely related to and built upon the research about the network design spaces and parameter-efﬁcient ﬁne-tuning. We discuss the connections and differences below. Network Design Spaces A lot of works designed neural network models via an ad-hoc discovery of new design choices that improve performances [Radosavovic et al., 2019], such as the use of deeper architectures or residuals. Recently, there have been works [Radosavovic et al., 2020, You et al., 2020, Radosavovic et al., 2019] performing at the design space level to discover new design principles for convolutional neural networks [Radosavovic et al., 2020] and graph neural networks [You et al., 2020]. Inspired by this line of research, we focus on the design space perspective to rethink parameter-efﬁcient ﬁne-tuning, with the goal of discovering design patterns that are applicable to different experimental settings. Parameter-Efﬁcient Fine-Tuning for NLP As pretrained models grow in size, storing ﬁne-tuned models becomes exceedingly expensive, and ﬁne-tuning becomes infeasible for those without extremely high compute resources. A growing body of research has been devoted to ﬁnding parameter-efﬁcient alternatives for adapting large-scale pre- trained models with reduced memory and storage costs. Houlsby et al. [2019b] proposed to adapt large models using bottleneck layers (with skip-connections) between each layer. This idea has been extended in many domains [Stick- land and Murray, 2019, Pfeiffer et al., 2020, Rebufﬁ et al., 2017, Lin et al., 2020]. Other works have aimed to avoid introducing additional parameters by identifying and training only a subset of all model parameters [Zhao et al., 2020, Guo et al., 2020, Mallya et al., 2018, Radiya-Dixit and Wang, 2020, Sung et al., 2021, Zaken et al., 2021]. Recent works also explored the idea of rank decomposition based on parameterized hypercomplex multiplications via the Kro- necker product [Zhang et al., 2021a] and injecting trainable rank decomposition matrices into each layer [Hu et al., 2021, Karimi Mahabadi et al., 2021]. Li and Liang [2021] introduced preﬁx-tuning that prepends a set of preﬁxes to autoregressive language models or prepends preﬁxes for both encoders and decoders. The preﬁx parameters are updated while the pretrained parameters are ﬁxed. Lester et al. [2021a] proposed a similar method, but only added virtual tokens at the embedding layer of large-scale models rather than discrete prompts [Deng et al., 2022, Zhong et al., 2022]. Bari et al. [2022] proposed semi-parametric prompt tuning that converges more easily, where memory prompts are input-adaptive without the need for tuning. Recently, He et al. [2022] and Ding et al. [2022] proposed a uniﬁed view of the existing parameter-efﬁcient ﬁne-tuning strategies and illustrated the difference and connections among them. Mao et al. [2022] also introduced a uniﬁed framework to combine different methods through mixture- of-experts. In contrast to these aforementioned works that assign their individual method equally to different pretrained layers, we focus on more general design spaces of parameter-efﬁcient ﬁne-tuning. This could provide a more comprehensive view of parameter-efﬁcient ﬁne-tuning in terms of both the tuning structures and tuning strategies. Through experiments where we progressively reﬁne design spaces, we discover design patterns for parameter-efﬁcient ﬁne-tuning. 3 Components of Design Spaces When deﬁning design spaces of parameter-efﬁcient ﬁne-tuning, we aim to cover key design components and provide a representative set of choices in each design component. Note that our goal is not to enumerate all possible design spaces, but to demonstrate how the use of design spaces can help inform parameter-efﬁcient ﬁne-tuning research. Concretely, in our work, the parameter-efﬁcient ﬁne-tuning design spaces are formed by a representative set of choices in parameter-efﬁcient ﬁne-tuning, which consists of the following four components: (i) layer grouping, (ii) trainable parameter allocation, (iii) tunable groups, and (iv) strategy assignment. Following the illustrated design space exam- ple in Figure 1, we describe these four design components in detail below and will explore their design choices in Section 4. Layer Grouping Different layers in pretrained models capture different information and behave differently. For example, Jawahar et al. [2019] found that the {3, 4, 5, 6, 7, 9, 12}-th layers have the most representation power in BERT and every layer captures a different type of information ranging from the surface, syntactic, to the semantic level representation of text. For instance, the 9th layer has predictive power for semantic tasks such as checking random swapping of coordinated clausal conjuncts, while the 3rd layer performs best in surface tasks like predicting sentence length. Therefore when adapting these pretrained models to downstream tasks, how to group layers with similar behaviors together is critical to the design and application of proper parameter-efﬁcient ﬁne-tuning strategies. For this design component, we study the patterns of how to group consecutive layers in pretrained models (e.g., transformer layers in T5) during the ﬁne-tuning process. 3Trainable Parameter Allocation In parameter-efﬁcient ﬁne-tuning, the total number of trainable parameters is usually preset, such as a small portion of the total number of parameters in the pretrained models. We will study different design choices for how to allocate a predeﬁned number of trainable parameters to layers. Tunable Groups Zaken et al. [2021] found that not all the parameters need to be tuned during ﬁne-tuning on the downstream tasks. For instance, BitFit [Zaken et al., 2021] only updates the bias parameters in pretrained models while freezing the remaining parameters. Thus, we study which groups need to be learned during parameter-efﬁcient ﬁne-tuning to attain better performances. Strategy Assignment In order to improve the parameter efﬁciency, different sets of strategies [Li and Liang, 2021, Lester et al., 2021a, Houlsby et al., 2019a, Hu et al., 2021] have been proposed where only a small number of (ex- tra) parameters are tuned and the remaining parameters in these pretrained models are frozen to adapt their general knowledge to speciﬁc down-stream tasks. Inspired by effectiveness of offering architectural ﬂexibility [Zhang et al., 2021a,b], we hypothesize that different groups might beneﬁt from different proper strategies (or combinations) for capturing different types of information. More formally, given a set of individual strategies Afor assignment, for any group Gi, assign a subset Ui ⊂A to each layer in Gi. 4 Discovering Design Patterns Building on these four different design components of PEFT design spaces, we will start from a relatively uncon- strained design space and progressively discover the design patterns. 4.1 Design Space Experimental Setup We ﬁrst describe our experimental setup for discovering the design patterns. Note that our process is generic for other tasks and future pretrained backbone models. Datasets Our process for discovering design patterns of PEFT is based on the average performances on the widely- used GLUE benchmark [Wang et al., 2018]. It covers a wide range of natural language understanding tasks. First, single-sentence tasks include (i) Stanford Sentiment Treebank (SST-2) and (ii) Corpus of Linguistic Acceptability (CoLA). Second, similarity and paraphrase tasks include (i) Quora Question Pairs (QQP), (ii) Semantic Textual Sim- ilarity Benchmark (STS-B), and (iii) Microsoft Research Paraphrase Corpus (MRPC). Third, inference tasks include (i) Multi-Genre Natural Language Inference (MNLI), (ii) Question Natural Language Inference (QNLI), and (iii) Rec- ognizing Textual Entailment (RTE). To compare performances, the Matthews correlation is measured for CoLA; the Spearman correlation is used for STS-B, and accuracy is measured for the rest GLUE tasks. Pretrained Backbone Models and Model Settings We use T5-base/3b [Raffel et al., 2020] as the main pretrained backbone models for discovering design patterns via our PEFT design spaces. We use Hugging Face 2 for our imple- mentations and follow the default settings. During the exploration, we set the total number of trainable parameters (in the percentage of that in the backbone model) to 0.5% by following He et al. [2022]. 4.2 Discovering Design Patterns Using T5-base In this subsection, we describe the empirical process for discovering the design patterns using T5-base (pretrained backbone model) as the working example. Each PEFT design space (denoted as Si) consists of a set of models ( Si- models) that satisfy constraints characterizing the space with respect to layer grouping, trainable parameter allocation, tunable groups, and strategy assignment. To discover design patterns, we start from a relatively unconstrained PEFT design space ( S0). Then we progressively reﬁne design spaces (from S0 to S1:4) by comparing overall quality of models in design spaces enforced with different constraints (e.g., each group has the same number of layers). To quantify the overall quality of models in any design space Si with a low-compute, low-epoch regime [Radosavovic et al., 2020], we randomly sample 100 models from Si, ﬁne-tune with 3 epochs 3, and compute the average of the GLUE average performances. 2https://huggingface.co/docs/transformers/index 3We set the low epoch by observing whether it is enough for models to obtain stable performances to draw consistent conclusions (See Table 7 in the Appendix). 4We emphasize that our goal is to demonstrate how the perspective of design spaces can help inform PEFT research, rather than to ﬁnd out the “best” design space or method. For computational efﬁciency, it is beyond the scope of this work to enumerate all possible constraints with respect to the design space components (Section 3). 4.2.1 The Initial S0 Design Space The initial relatively unconstrained design space S0 consists of all models without constraints on the design space components (Section 3). Individual PEFT strategies consist of Adapter, Preﬁx, BitFit, and LoRA. One can think of this S0 design space as a set of random models ( S0-models) with random design patterns. Speciﬁcally, without grouping constraints, each layer of the pretrained layer has a half chance to be tuned: if tuned, random strategies (or combinations) with a random amount of trainable parameters are assigned to that layer. Before comparing more subtle design patterns such as how to properly assign tunable strategies among Adapter, Preﬁx, BitFit, and LoRA, we begin with exploring how to group layers and how to allocate the total number of trainable parameters to layers. 4.2.2 The S1 Design Space with Additional Grouping Constraints Inspired by Radosavovic et al. [2020], we also consider 4 groups (G1, . . . , G4, in the order of forward pass) in the experiments 4. Denote by Ni the number of layers in Gi. As illustrated in Figure 2, we compare the following layer grouping patterns: (i) Increasing (Ni+1 > Ni): the number of layers in groups gradually increases; (ii) Uniform (Ni+1 = Ni): the number of layers in groups is the same; (iii) Decreasing (Ni+1 < Ni): the number of layers in groups gradually decreases; (iv) Spindle (N1 < N2 = N3 > N4): the numbers of layers in groups at both ends are smaller; and (v) Bottleneck (N1 > N2 = N3 < N4): the numbers of layers in groups at both ends are bigger. Figure 2: Layer grouping patterns, where the horizontal and vertical axes represent groups (G1, . . . , G4) and numbers of layers in groups. These layer grouping patterns lead to 5 different design spaces. Any of these 5 design spaces consists of all models in the S0 design space that satisfy one of these grouping pattern constraints. To compare the overall model qualities of different design spaces, we (i) randomly sample 100 models from the S0 design space that satisfy each grouping pattern constraint (Figure 2); (ii) ﬁne-tune with 3 epochs; and (iii) compute the average performances for each design space. We will follow this procedure as we progressively add new constraints later. The averaged performances are shown in Table 1 5. We ﬁnd that models from the design space with the spindle grouping pattern (Figure 2) consistently outperform those from the other design spaces across all the 8 GLUE tasks. This may be due to the complexities of information captured in different layers of large pretrained models, which favor information adaptation in the discovered layer grouping pattern. From now on, we will group layers in a spindle pattern. We refer to S0 with this additional design pattern as the new S1 design space. 4.2.3 The S2 Design Space with Additional Parameter Constraints We continue to explore design patterns in trainable parameter allocation to reﬁne the S1 design space. Denote by ni the number of trainable parameters for the i-th layer of the pretrained backbone model, we compare the following design patterns: (i) Increasing (ni+1 ≥ni): the number of trainable parameters in every layer gradually increases (or remains the same); (ii) Uniform (ni+1 = ni): the number of trainable parameters in every layer is the same; and (iii) Decreasing (ni+1 ≤ni): the number of trainable parameters in every layer gradually decreases (or remains the same). Following the procedure described in Section 4.2.2, we obtain 100 models for each of these 3 new design spaces. Table 2 reports the average performances of these 3 design spaces. The uniform allocation design pattern obtains the highest GLUE average performance, making this relatively simple, interpretable design pattern favorable. 4The experimental results with 8 groups are shown in the Table 16 in the Appendix. 5The training time for the step is shown in the Table 18 in the Appendix. 5Table 1: Average performances (low-compute, low-epoch regime: 100 random models, 3 tuning epochs) on the GLUE datasets using the T5-base pretrained backbone model. We compare adding different layer grouping constraints to the S0 design space. Layer Grouping SST-2 MNLI QNLI QQP RTE STS-B MRPC CoLA Avg S0-models 76.9 70.1 72.5 73.3 63.6 71.7 73.8 24.3 65.7 Increasing 85.3 74.9 77.2 77.5 66.8 76.2 76.0 33.0 70.8 Uniform 84.8 73.7 78.1 78.6 68.5 77.8 79.2 36.1 72.1 Decreasing 81.9 72.1 78.3 76.7 67.3 75.9 78.6 28.7 70.0 Spindle 86.9 75.5 79.8 79.4 69.8 78.3 80.1 37.3 73.3 Bottleneck 84.5 74.6 76.9 78.1 69.2 76.2 78.6 32.1 71.3 Table 2: Average performances (low-compute, low-epoch regime: 100 random models, 3 tuning epochs) on the GLUE datasets using the T5-base pretrained backbone model. We compare adding different parameter allocation constraints to the S1 design space. Param Allocation SST-2 MNLI QNLI QQP RTE STS-B MRPC CoLA Avg Increasing 87.2 77.9 79.4 78.7 71.6 77.6 81.4 32.0 73.2 Uniform 87.8 77.4 80.1 80.5 73.9 78.1 80.4 34.3 74.0 Decreasing 86.4 75.8 78.4 77.0 70.4 77.1 78.7 35.8 72.4 We will allocate the number of trainable parameters to layers uniformly. We refer to S1 with this additional design pattern as the new S2 design space. 4.2.4 The S3 Design Space with Additional Tunable Group Constraints Before digging into the strategy assignment design patterns, it is necessary to examine which groups need to be tuned. After all, it is only meaningful to study assigning strategies to different groups after we ﬁnd out which groups need to be ﬁne-tuned. As shown in Table 3, we explore various design patterns in tunable groups to further constrain the S2 design space. Based on the GLUE average performances, we ﬁnd that all the groups need to be tuned to obtain the best performances. This suggests that all the groups of pretrained layers have captured useful information that should be adapted to the downstream tasks. We will tune all the groups. We refer to S2 with this additional design pattern as the new S3 design space. 4.2.5 The S4 Design Space with Additional Strategy Constraints Finally, we study the subtle design pattern with respect to assigning proper strategies by further constraining the derived S3 design space. Speciﬁcally, each design space consists of models that assign a subset of {Adapter (A), Preﬁx (P), BitFit (B), and LoRA (L) }to all layers of any group Gi (i = 1, . . . ,4). We begin by adding different G1 strategy assignment constraints to the S3 space. Following the same pattern discovery procedure (Section 4.2.2), we discover strategy assignment patterns for G1. Then we progressively add Gi (i >1) strategy assignment constraints together with the discovered strategy assignment patterns for all Gj (j = 1, . . . , i−1) to the S3 space. Due to space limit, we present results of this process in the Appendix ( G1 in Table 8, G2 Table 9, G3 in Table 10, and G4 in Table 11), which suggests strategy assignment ofG1-(A, L) – G2-(A, P) – G3-(A, P, B) –G4-(P, B, L) for the T5-base pretrained backbone model. We will assign the discovered proper tuning strategies to groups.We refer to S3 with this additional design pattern as the new S4 design space, which consists of the ﬁnal S4-model. 4.3 Discovering Design Patterns Using T5-3b We then repeat the above process on T5-3b to examine if the design patterns we discovered using smaller models (T5- base) still apply when we use larger models. The results are shown in Table 12 (layer grouping), Table 13 (trainable parameter allocation), Table 14 (tunable groups) and Table 15 (strategy assignment) in the Appendix. We observe that the design patterns still apply when larger models like T5-3b are used: (i) grouping layers in a spindle pattern (Table 12), (ii) uniformly allocating the number of trainable parameters to layers (Table 13), (iii) tuning all the groups 6Table 3: Average performances (low-compute, low-epoch regime: 100 random models, 3 tuning epochs) on the GLUE datasets using the T5-base pretrained backbone model. We compare adding different tunable group constraints to the S2 design space. Tunable Groups SST-2 MNLI QNLI QQP RTE STS-B MRPC CoLA Avg G1 82.6 72.1 77.6 70.6 65.3 71.9 77.6 27.6 68.2 G2 83.3 72.8 77.5 72.8 63.6 72.8 77.5 27.5 68.4 G3 83.6 73.3 78.2 73.3 66.4 71.3 77.9 22.9 68.4 G4 83.2 73.0 77.9 73.7 63.9 72.0 77.9 27.9 68.7 G1, G2 83.5 73.2 78.0 75.4 67.7 73.2 78.0 28.0 69.6 G3, G4 87.8 74.6 78.3 76.9 68.6 74.3 78.3 28.3 70.7 G1, G2, G3 86.0 75.8 79.0 77.8 71.8 78.8 79.0 33.0 72.6 G2, G3, G4 85.2 76.6 79.1 78.6 70.1 77.6 79.1 31.9 72.2 G1,G2,G3,G4 88.3 77.4 82.1 81.5 74.9 79.4 81.4 34.3 74.9 Table 4: Performances of different tuning methods on the GLUE datasets using the T5-base (upper part) and T5-3b (lower part) pretrained backbone models, respectively. The results are averaged over 20 random runs (with standard deviations as subscripts). The S4-model and the S4-3b-model perform signiﬁcantly better than the second-best PEFT methods in all the eight datasets at the signiﬁcance level p <0.05(∗) or even p <0.01(∗∗). Method SST-2 MNLI QNLI QQP RTE STS-B MRPC CoLA Average full 95.2 87.1 93.7 89.4 80.1 89.4 90.7 51.1 84.5 Adapter 94.6 85.5 89.8 86.7 75.3 86.7 89.1 59.2 83.3 Preﬁx 94.0 81.6 87.8 83.4 64.3 83.1 84.8 34.0 76.6 BitFit 94.4 84.5 90.6 88.3 74.3 86.6 90.1 57.7 83.3 LoRA 94.8 84.7 91.6 88.5 75.8 86.3 88.7 51.5 82.7 S4-model 95.5∗∗ 1.7 87.6∗∗ 1.0 92.7∗∗ 1.1 88.8∗∗ 1.0 80.4∗ 2.3 87.4∗ 2.0 91.2∗∗ 2.4 62.2∗ 3.2 85.7 full 97.4 91.4 96.3 89.7 91.1 90.6 92.5 67.1 89.5 Adapter 96.3 89.9 94.7 87.8 83.4 90 89.7 65.2 87.1 Preﬁx 96.3 82.8 88.9 85.5 78.3 83.5 85.4 42.7 80.4 BitFit 95.8 89.5 93.5 88.5 86.2 90.7 88.6 64.2 87.1 LoRA 96.2 90.6 94.9 89.1 91.2 91.1 91.1 67.4 88.9 S4-3b-model 97.2∗∗ 1.8 91.6∗∗ 1.2 96.6∗∗ 1.0 89.5∗∗ 1.5 91.5∗ 2.8 91.5∗ 2.5 91.9∗ 2.0 69.7∗ 3.4 89.9 (Table 14), and (iv) tuning different groups with proper strategies (Table 15). For T5-3b, the discovered proper strategy assignment is G1-(P, L) –G2-(A, L) – G3-(P, B, L) –G4-(A, P, B). We refer to the ﬁnal design space asS4-3b and the ﬁnal model in this space as S4-3b-model. 5 Evaluation The S4-model (Section 4.2.5) and S4-3b-model (Section 4.3) adopt all the design patterns that have been discovered by using T5-base and T5-3b, respectively. As a result, they are both new methods of PEFT. We will evaluate their effectiveness when applied to different pretrained backbone models and different NLP tasks. 5.1 Experimental Setup Datasets Besides the GLUE datasets [Wang et al., 2018] (Section 4.1), we further evaluate our methods on two generation tasks used by He et al. [2022]: (i) Abstractive Summarization using XSum [Narayan et al., 2018], and (ii) Machine Translation using the WMT 2016 en-ro dataset [Bojar et al., 2016]. We report ROUGE scores [Lin, 2004] on the XSum test set, and BLEU scores [Papineni et al., 2002] on the en-ro test set. Models and Model Settings We mainly compare our methods with the following baselines: (i) Full Fine-tuning (full): it ﬁne-tunes all the model parameters in the pretrained models; (ii) Adapter [Houlsby et al., 2019a]: it adds adapter modules to each transformer layer; (iii) Preﬁx [Li and Liang, 2021]: it optimizes a set of small continuous vectors prepended to transformer layers; (iv) BitFit [Zaken et al., 2021]: it only updates the bias terms in pretrained models; (v) LoRA [Hu et al., 2021]: it decomposes the attention weight into low-rank matrices to reduce the number of trainable parameters. Besides T5 [Raffel et al., 2020], we additionally apply our methods to other backbone models 7Table 5: Performances of different tuning methods on GLUE datasets using the RoBERTa-base (upper part) and RoBERTa-large (lower part) pretrained backbone models. The results are averaged over 20 random runs (with standard deviations as subscripts). Here we also include two baselines: (i) S0-model, where all the designs are randomly selected for RoBERTa as in the S0 design space; (ii) S3-model, where strategies are randomly assigned to different RoBERTa layer groups as in the S3 design space. The S4-model and S4-3b-model perform signiﬁcantly better than the second-best PEFT methods in all the eight datasets at the signiﬁcance level p <0.05(∗) or even p <0.01(∗∗). Method SST-2 MNLI QNLI QQP RTE STS-B MRPC CoLA Average full 94.8 87.6 92.8 91.9 80.8 90.3 90.2 63.6 86.5 Adapter 94.2 87.1 93.1 90.2 71.5 89.7 88.5 60.8 84.4 Preﬁx 94.0 86.8 91.3 90.5 74.5 90.3 88.2 61.5 84.6 BitFit 93.7 84.8 91.3 84.5 77.8 90.8 90.0 61.8 84.3 LoRA 94.9 87.5 93.1 90.8 83.1 90.0 89.6 62.6 86.4 S0-model 94.2 95.3 90.4 90.6 75.6 89.6 88.0 60.9 85.6 S3-model 94.3 87.2 92.8 91.0 81.8 90.3 89.2 63.2 86.2 S4-model 94.81.6 87.8∗∗ 0.8 93.4∗∗ 1.3 91.6∗ 1.2 85.8∗∗ 1.8 90.4∗ 2.0 90.0∗∗ 1.8 63.2∗ 3.5 87.1 full 96.4 90.2 94.7 92.2 86.6 92.4 90.9 68.0 88.9 Adapter 96.6 90.5 94.8 91.7 80.1 92.1 90.9 67.8 88.1 Preﬁx 95.7 87.6 92.1 88.7 82.3 89.6 87.4 62.8 85.7 BitFit 96.1 88.0 93.4 90.2 86.2 90.9 92.7 64.2 87.7 LoRA 96.2 90.6 94.7 91.6 87.4 92.0 89.7 68.2 88.8 S0-model 95.5 86.5 92.3 89.8 84.6 89.2 86.3 61.2 85.6 S3-model 96.3 89.4 93.8 90.2 85.9 90.8 90.9 63.4 87.6 S4-3b-model 96.6∗∗ 1.3 90.8∗ 1.1 95.1∗∗ 0.8 92.0∗∗ 1.2 87.22.8 92.3∗ 2.2 91.8∗∗ 1.8 68.4∗ 3.2 89.3 including RoBERTa-base/large [Liu et al., 2019] and BART-base/large [Lewis et al., 2020a]. We use the default settings. We set the total number of trainable parameters (in the percentage of that in the backbone model) by following He et al. [2022]. Speciﬁcally, this value is set to 0.5% for Adapter, Preﬁx, LoRA, and our methods, and 0.1% for BitFit. For all the experiments, we followed Liu et al. [2019] to set the linear decay scheduler with a warmup ratio of 0.06 for training. The batch size was 128 for base models and 64 for large models. The maximum learning rate was 5e −5 and the maximum number of training epochs was set to be either 5 or 10. All the experiments were performed using 8 A100 GPUs. 5.2 Effectiveness on GLUE with T5 Backbones Table 6: Performances of different tuning methods on generation tasks (XSUM and en-ro) using the BART-base (upper part) and BART-large (lower part) pretrained backbone models. Method XSUM(R-1/2/L) en-ro (BLEU) full 40.5/19.2/34.8 34.5 Adapter 37.7/17.9/33.1 33.3 Preﬁx 38.2/18.4/32.4 33.8 BitFit 37.2/17.5/31.4 33.2 LoRA 38.9/18.6/33.5 33.6 PA 39.3/18.7/33.8 33.8 S4-model 40.2/19.3/34.2 34.1 full 45.1/22.3/37.2 37.9 Adapter 43.8/20.8/35.7 35.3 Preﬁx 43.4/20.4/35.5 35.6 BitFit 42.8/18.7/33.2 35.2 LoRA 42.9/19.4/34.8 35.8 PA 43.9/20.6/35.6 36.4 S4-3b-model 44.3/21.7/36.8 37.2 With our discovered design patterns, we ﬁne-tune T5-base (S4-model) and T5-3b ( S4-3b-model) on GLUE and compare them with all the baseline methods. The results are shown in Table 4, where the key measure is the GLUE average performance (last column). We ﬁnd that our S4-model and S4- 3b-model consistently outperform the investigated methods in the key measure. By tuning only 0.5% parameters, our methods even outperform the full ﬁne-tuning baseline where all the parameters are tuned, indicating the effectiveness of our discov- ered PEFT design patterns. 5.3 General Effectiveness on GLUE with RoBERTa Backbones We directly apply the S4-model and S4-3b-model (adopting design patterns discovered using T5- base and T5-3b) to ﬁne-tune the RoBERTa-base and RoBERTa-large pretrained backbone models (with no extra discovery process), respectively. We keep all the other settings the same and evaluate them on GLUE datasets. We also compare with variant methods randomly sampled from two de- 8sign spaces: (i) S0-model, where all the designs are randomly selected for RoBERTa as in S0; (ii) S3-model, where strategies are randomly assigned to different RoBERTa layer groups as in S3. Table 5 shows that (i) the design pat- terns (adopted by S4-model and S4-3b-model) discovered using T5 models are applicable to the RoBERTa backbone models and outperform the investigated methods in GLUE average performances with no extra discovery process;(ii) improved performances fromS0-models, S3-models, to S4-(3b)-models support adding more constraints in the pattern discovery process (Section 4). 5.4 General Effectiveness on Generation Tasks with BART Backbones Like in Section 5.3, we further directly apply the S4-model and S4-3b-model (adopting design patterns discovered using T5-base and T5-3b) to ﬁne-tune the BART-base and BART-large pretrained backbone models (without additional discovery process.), respectively. We evaluate the models on two generation tasks: summarization (XSUM) and machine translation (en-ro) following He et al. [2022]. We also compare with PA (parallel adapter) using the same number of trainable parameters [He et al., 2022]. Table 6 shows that our methods, although adopting design patterns discovered from classiﬁcation tasks using T5, still outperform investigated PEFT strategies on generation tasks with different BART backbones. 6 Conclusion PEFT adapts knowledge in pretrained models to down-stream tasks in a more parameter-efﬁcient fashion. Instead of focusing on designing another strategy in the ﬁrst place, we introduced PEFT design spaces. We empirically discovered several design patterns in PEFT. These design patterns led to new PEFT methods. Experiments showed that these methods consistently outperform investigated PEFT strategies across different backbone models and different tasks in natural language processing. References Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional trans- formers for language understanding. In NAACL-HLT, 2019. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019. Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le. Xlnet: Generalized autoregressive pretraining for language understanding. InAdvances in neural information processing systems, pages 5754–5764, 2019. Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S. Weld, Luke Zettlemoyer, and Omer Levy. Spanbert: Improving pre-training by representing and predicting spans. Transactions of the Association for Computational Linguistics , 8:64–77, 2019. Yu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Xuyi Chen, Han Zhang, Xin Tian, Danxiang Zhu, Hao Tian, and Hua Wu. Ernie: Enhanced representation through knowledge integration. arXiv preprint arXiv:1904.09223, 2019. Kevin Clark, Minh-Thang Luong, Quoc V Le, and Christopher D Manning. Electra: Pre-training text encoders as discriminators rather than generators. In International Conference on Learning Representations, 2019. Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence pre-training for natural language generation, transla- tion, and comprehension. SCL, 2020a. Hangbo Bao, Li Dong, Furu Wei, Wenhui Wang, Nan Yang, Xiaodong Liu, Yu Wang, Songhao Piao, Jianfeng Gao, Ming Zhou, et al. Unilmv2: Pseudo-masked language models for uniﬁed language model pre-training. arXiv preprint arXiv:2002.12804, 2020. Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. Deberta: Decoding-enhanced bert with disentangled attention. arXiv preprint arXiv:2006.03654, 2020. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a uniﬁed text-to-text transformer, 2020. Caleb Ziems, Jiaao Chen, Camille Harris, Jessica Anderson, and Diyi Yang. V ALUE: Understanding dialect disparity in NLU. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 91: Long Papers) , pages 3701–3720, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.258. URL https://aclanthology.org/2022.acl-long.258. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners, 2020. Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efﬁcient transfer learning for NLP. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning , vol- ume 97 of Proceedings of Machine Learning Research , pages 2790–2799. PMLR, 09–15 Jun 2019a. URL http://proceedings.mlr.press/v97/houlsby19a.html. Jonas Pfeiffer, Aishwarya Kamath, Andreas R ¨uckl´e, Kyunghyun Cho, and Iryna Gurevych. AdapterFusion: Non- destructive task composition for transfer learning. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume , pages 487–503, Online, April 2021. Association for Computational Linguistics. URL https://www.aclweb.org/anthology/2021.eacl-main.39. Xiang Lisa Li and Percy Liang. Preﬁx-tuning: Optimizing continuous prompts for generation, 2021. Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efﬁcient prompt tuning, 2021a. Timo Schick and Hinrich Sch ¨utze. Exploiting cloze-questions for few-shot text classiﬁcation and natural language inference. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume , pages 255–269, Online, April 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.eacl-main.20. URL https://aclanthology.org/2021.eacl-main.20. Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efﬁcient prompt tuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing , pages 3045–3059, Online and Punta Cana, Dominican Republic, November 2021b. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.243. URL https://aclanthology.org/2021.emnlp-main.243. Elad Ben Zaken, Shauli Ravfogel, and Yoav Goldberg. Bitﬁt: Simple parameter-efﬁcient ﬁne-tuning for transformer- based masked language-models, 2021. URL https://arxiv.org/abs/2106.10199. Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models, 2021. URL https://arxiv.org/abs/2106.09685. Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, and Graham Neubig. Towards a uniﬁed view of parameter-efﬁcient transfer learning. In International Conference on Learning Representations, 2022. Yuning Mao, Lambert Mathias, Rui Hou, Amjad Almahairi, Hao Ma, Jiawei Han, Scott Yih, and Madian Khabsa. UniPELT: A uniﬁed framework for parameter-efﬁcient language model tuning. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 6253–6264, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.433. URL https: //aclanthology.org/2022.acl-long.433. Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoy- anov, and Luke Zettlemoyer. BART: Denoising sequence-to-sequence pre-training for natural language genera- tion, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Com- putational Linguistics , pages 7871–7880, Online, July 2020b. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.703. URL https://www.aclweb.org/anthology/2020.acl-main.703. Ilija Radosavovic, Justin Johnson, Saining Xie, Wan-Yen Lo, and Piotr Doll ´ar. On network design spaces for visual recognition, 2019. URL https://arxiv.org/abs/1905.13214. Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He, and Piotr Doll ´ar. Designing network design spaces, 2020. URL https://arxiv.org/abs/2003.13678. Jiaxuan You, Rex Ying, and Jure Leskovec. Design space for graph neural networks, 2020. URL https://arxiv. org/abs/2011.08843. Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efﬁcient transfer learning for nlp. In International Conference on Machine Learning, pages 2790–2799. PMLR, 2019b. Asa Cooper Stickland and Iain Murray. Bert and pals: Projected attention layers for efﬁcient adaptation in multi-task learning. In International Conference on Machine Learning, pages 5986–5995. PMLR, 2019. 10Jonas Pfeiffer, Aishwarya Kamath, Andreas R ¨uckl´e, Kyunghyun Cho, and Iryna Gurevych. Adapterfusion: Non- destructive task composition for transfer learning. arXiv preprint arXiv:2005.00247, 2020. Sylvestre-Alvise Rebufﬁ, Hakan Bilen, and Andrea Vedaldi. Learning multiple visual domains with residual adapters. arXiv preprint arXiv:1705.08045, 2017. Zhaojiang Lin, Andrea Madotto, and Pascale Fung. Exploring versatile generative language model via parameter- efﬁcient transfer learning. arXiv preprint arXiv:2004.03829, 2020. Mengjie Zhao, Tao Lin, Fei Mi, Martin Jaggi, and Hinrich Sch ¨utze. Masking as an efﬁcient alternative to ﬁnetuning for pretrained language models. arXiv preprint arXiv:2004.12406, 2020. Demi Guo, Alexander M Rush, and Yoon Kim. Parameter-efﬁcient transfer learning with diff pruning. arXiv preprint arXiv:2012.07463, 2020. Arun Mallya, Dillon Davis, and Svetlana Lazebnik. Piggyback: Adapting a single network to multiple tasks by learning to mask weights. In Proceedings of the European Conference on Computer Vision (ECCV), pages 67–82, 2018. Evani Radiya-Dixit and Xin Wang. How ﬁne can ﬁne-tuning be? learning efﬁcient language models. In International Conference on Artiﬁcial Intelligence and Statistics, pages 2435–2443. PMLR, 2020. Yi-Lin Sung, Varun Nair, and Colin A Raffel. Training neural networks with ﬁxed sparse masks. In M. Ranzato, A. Beygelzimer, Y . Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Pro- cessing Systems, volume 34, pages 24193–24205. Curran Associates, Inc., 2021. URL https://proceedings. neurips.cc/paper/2021/file/cb2653f548f8709598e8b5156738cc51-Paper.pdf. Aston Zhang, Yi Tay, Shuai Zhang, Alvin Chan, Anh Tuan Luu, Siu Hui, and Jie Fu. Beyond fully-connected lay- ers with quaternions: Parameterization of hypercomplex multiplications with 1/n parameters. In International Conference on Learning Representations, 2021a. Rabeeh Karimi Mahabadi, James Henderson, and Sebastian Ruder. Compacter: Efﬁcient low-rank hypercomplex adapter layers. Advances in Neural Information Processing Systems, 34:1022–1035, 2021. Mingkai Deng, Jianyu Wang, Cheng-Ping Hsieh, Yihan Wang, Han Guo, Tianmin Shu, Meng Song, Eric P. Xing, and Zhiting Hu. Rlprompt: Optimizing discrete text prompts with reinforcement learning, 2022. URL https: //arxiv.org/abs/2205.12548. Wanjun Zhong, Yifan Gao, Ning Ding, Zhiyuan Liu, Ming Zhou, Jiahai Wang, Jian Yin, and Nan Duan. Improving task generalization via uniﬁed schema prompt, 2022. URL https://arxiv.org/abs/2208.03229. M Saiful Bari, Aston Zhang, Shuai Zheng, Xingjian Shi, Yi Zhu, Shaﬁq Joty, and Mu Li. Spt: Semi-parametric prompt tuning for multitask prompted learning. arXiv preprint arXiv:2212.10929, 2022. Ning Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zonghan Yang, Yusheng Su, Shengding Hu, Yulin Chen, Chi-Min Chan, Weize Chen, Jing Yi, Weilin Zhao, Xiaozhi Wang, Zhiyuan Liu, Hai-Tao Zheng, Jianfei Chen, Yang Liu, Jie Tang, Juanzi Li, and Maosong Sun. Delta tuning: A comprehensive study of parameter efﬁcient methods for pre-trained language models, 2022. URL https://arxiv.org/abs/2203.06904. Ganesh Jawahar, Benoˆıt Sagot, and Djam´e Seddah. What does BERT learn about the structure of language? In Pro- ceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3651–3657, Florence, Italy, July 2019. Association for Computational Linguistics. Aston Zhang, Yi Tay, Yikang Shen, Alvin Chan, and Shuai Zhang. Self-instantiated recurrent units with dynamic soft recursion. Advances in Neural Information Processing Systems, 34:6503–6514, 2021b. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. Glue: A multi-task benchmark and analysis platform for natural language understanding. In BlackboxNLP@EMNLP, 2018. Shashi Narayan, Shay B. Cohen, and Mirella Lapata. Don’t give me the details, just the summary! topic-aware convo- lutional neural networks for extreme summarization. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1797–1807, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1206. URL https://aclanthology.org/D18-1206. Ondˇrej Bojar, Rajen Chatterjee, Christian Federmann, Yvette Graham, Barry Haddow, Matthias Huck, Antonio Ji- meno Yepes, Philipp Koehn, Varvara Logacheva, Christof Monz, Matteo Negri, Aur ´elie N ´ev´eol, Mariana Neves, Martin Popel, Matt Post, Raphael Rubino, Carolina Scarton, Lucia Specia, Marco Turchi, Karin Verspoor, and Mar- cos Zampieri. Findings of the 2016 conference on machine translation. In Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers, pages 131–198, Berlin, Germany, August 2016. Association for Computational Linguistics. doi: 10.18653/v1/W16-2301. URL https://aclanthology.org/W16-2301. 11Chin-Yew Lin. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 74–81, Barcelona, Spain, July 2004. Association for Computational Linguistics. URL https: //aclanthology.org/W04-1013. Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In ACL, 2002. 12A More Experimental Results Table 7: Average performances (low-compute, low-epoch regime: 100 random models, tuning epochs = 1, 2, 3, 4, 20 for ﬁve different blocks) on the GLUE datasets using the T5-base pretrained backbone model. We compare adding different grouping constraints to the S0 design space. Grouping Patterns SST-2 MNLI QNLI QQP RTE STS-B MRPC CoLA Avg 1 epochs Increasing 73.2 63.3 67.8 68.8 63.8 67.2 64.1 11.0 59.9 Uniform 72.8 64.1 63.4 63.4 62.5 69.8 65.8 12.1 59.2 Decreasing 72.4 63.2 65.1 69.8 59.3 62.7 63.6 18.7 59.4 Spindle 72.6 64.8 66.8 71.1 62.1 62.3 64.8 12.3 59.6 Bottleneck 72.2 63.7 65.3 68.3 61.2 63.2 66.6 12.1 59.0 2 epochs Increasing 76.2 69.3 73.2 76.5 65.8 72.2 74.0 21.0 66.0 Uniform 74.8 70.9 74.1 75.6 66.5 73.4 71.2 22.1 66.1 Decreasing 71.4 70.1 72.1 76.8 64.3 71.7 73.6 18.7 64.8 Spindle 76.6 71.9 71.8 74.4 67.5 73.5 71.8 22.3 66.2 Bottleneck 74.2 71.1 69.6 73.3 65.2 73.3 73.6 24.1 65.5 3 epochs Increasing 85.3 74.9 77.2 77.5 66.8 76.2 76.0 33.0 70.8 Uniform 84.8 73.7 78.1 78.6 68.5 77.8 79.2 36.1 72.1 Decreasing 81.9 72.1 78.3 76.7 67.3 75.9 78.6 28.7 69.9 Spindle 86.9 75.5 79.8 79.4 69.8 78.3 80.1 47.3 74.6 Bottleneck 84.5 74.6 76.9 78.1 69.2 76.2 78.6 32.1 71.3 4 epochs Increasing 88.3 78.5 80.2 80.5 70.8 80.2 80.0 37.0 74.4 Uniform 88.8 78.9 81.9 81.5 71.5 80.8 81.4 39.1 75.4 Decreasing 87.6 74.1 80.8 81.7 79.3 78.9 79.6 38.7 75.1 Spindle 89.6 79.8 83.6 82.8 71.8 81.3 82.1 39.3 76.3 Bottleneck 86.5 77.6 82.7 81.1 70.2 70.9 81.6 36.1 73.3 20 epochs Increasing 92.3 83.3 86.2 82.5 71.8 82.2 84.0 51.0 79.1 Uniform 92.8 83.9 86.1 83.6 72.5 83.8 84.2 52.1 79.9 Decreasing 91.4 82.1 85.1 83.1 69.3 81.7 83.6 48.7 78.1 Spindle 93.6 84.8 87.8 84.4 73.5 84.3 85.8 52.3 80.8 Bottleneck 92.1 82.6 85.6 83.3 71.2 83.2 84.6 52.1 79.3 B General Effectiveness on SuperGLUE with XLNet Backbones We also directly use the S4-model and S4-3b-model (adopting design patterns discovered using T5-base and T5-3b) to ﬁne-tune the XLNet-base and XLNet-large pretrained backbone models without any extra discovery process. We keep all the other settings the same and evaluate them on SuperGLUE datasets. Table 17 reiterates the fact that our PEFT design patterns discovered from T5 models are generelizable to the XLNet backbone models and outperform the investigated methods in other tasks (SuperGLUE) with no additional discovery process. C On the Discovery Sequence In this work, we follow the discovery sequence of “grouping patterns – trainable parameter allocation – tunable groups – strategy assignment”: 1. To explore and understand the design patterns in all the layers in large pre-trained models in scale, it is necessary and more efﬁcient to study the layers in the unit of groups. So we start with the grouping patterns. 13Table 8: Average performances (low-compute, low-epoch regime: 100 random models, 3 tuning epochs) on the GLUE datasets using the T5-base pretrained backbone model. We compare adding different G1 strategy assignment con- straints to the S3 design space. Strategy Assignment SST-2 MNLI QNLI QQP RTE STS-B MRPC CoLA Avg G1-Adapter (A) 89.8 83.5 84.9 80.8 72.5 80.8 78.5 37.7 76.1 G1-Preﬁx (P) 89.3 83.1 84.4 80.1 70.1 80.0 77.6 33.0 74.7 G1-BitFit (B) 89.0 82.9 84.1 81.4 72.0 81.1 77.0 30.8 74.8 G1-LoRA (L) 89.9 83.6 85.0 81.1 71.8 81.0 78.8 35.3 75.8 G1-(P, L) 89.1 82.8 85.1 81.2 71.9 81.5 79.1 35.0 75.7 G1-(A, P) 89.8 82.8 84.8 81.1 72.2 81.3 79.2 36.4 75.9 G1-(A, L) 89.6 83.8 85.6 81.3 72.9 81.7 79.5 36.8 76.4 G1-(A, P, L) 89.6 83.5 85.2 81.5 72.2 81.4 79.2 35.2 75.9 G1-(P, B, L) 89.3 83.6 85.5 81.6 72.3 81.0 78.8 35.7 76.0 G1-(A, P, B) 89.2 83.3 84.8 81.8 72.5 81.1 78.6 35.6 75.8 G1-(A, B, L) 89.8 83.4 84.8 81.1 72.6 81.6 79.4 34.8 75.9 G1-(A, P, B, L) 90.0 83.1 85.3 81.6 72.6 81.4 79.2 36.5 76.1 Table 9: Average performances (low-compute, low-epoch regime: 100 random models, 3 tuning epochs) on the GLUE datasets using the T5-base pretrained backbone model. We compare adding different G2 strategy assignment con- straints with G1-(L, A) to the S3 design space. Strategy Assignment SST-2 MNLI QNLI QQP RTE STS-B MRPC CoLA Avg G2-Adapter (A) 91.6 84.3 85.5 82.3 73.5 82.8 81.3 38.8 77.5 G2-Preﬁx (P) 89.6 84.0 86.5 81.5 73.3 82.5 80.5 36.2 76.7 G2-BitFit (B) 91.2 83.6 85.7 82.9 72.6 82.6 80.8 33.1 76.5 G2-LoRA (L) 91.4 84.4 86.1 82.0 72.8 81.8 81.6 39.8 77.4 G2-(P, L) 91.6 84.6 86.8 81.8 73.8 82.8 82.0 38.5 77.7 G2-(A, P) 92.2 84.2 87.1 82.2 74.4 83.0 82.5 40.8 78.3 G2-(A, L) 92.0 84.4 86.5 81.8 73.6 82.6 82.2 40.1 77.9 G2-(A, P, L) 91.8 84.8 86.8 81.8 74.1 83.0 82.1 37.9 77.7 G2-(P, B, L) 91.6 84.1 87.1 82.0 74.0 82.9 82.4 35.8 77.4 G2-(A, P, B) 91.8 84.2 86.8 82.1 73.7 83.3 82.2 41.2 78.1 G2-(A, B, L) 92.2 84.3 86.1 82.0 74.1 83.2 82.0 37.6 77.6 G2-(A, P, B, L) 92.0 84.1 87.0 81.9 74.2 83.1 81.3 42.4 78.1 2. Once ﬁguring out the optimal grouping patterns, it is then important to explore how to allocate the trainable parameters to these different groups in order to study more subtle designs with fair comparisons (e.g., this would allow comparing different patterns of strategy assignments without the impact from different trainable parameters.). 3. Next, it becomes inﬂuential to examine which groups need to be learned during ﬁne-tuning before we dig into the strategy assignment patterns. Because it is only meaningful to study assigning strategies to different groups after we ﬁgure out which groups need to be learned. 4. Finally, we study the tuning strategy assignment, which is the most subtle design. 14Table 10: Average performances (low-compute, low-epoch regime: 100 random models, 3 tuning epochs) on the GLUE datasets using the T5-base pretrained backbone model. We compare adding different G3 strategy assignment constraints with G1-(L, A) – G2-(P, A) to the S3 design space. Strategy Assignment SST-2 MNLI QNLI QQP RTE STS-B MRPC CoLA Avg G3-Adapter (A) 92.5 85.3 87.5 83.3 73.9 84.0 83.8 44.9 79.4 G3-Preﬁx (P) 91.5 84.7 86.7 82.6 74.2 83.8 82.9 40.5 78.4 G3-BitFit (B) 91.9 84.3 87.0 82.0 73.6 84.1 83.3 36.1 77.8 G3-LoRA (L) 92.8 85.4 87.8 83.5 74.7 82.4 84.0 44.0 79.3 G3-(P, L) 93.0 85.2 88.3 83.8 75.2 84.4 84.2 37.9 79.0 G3-(A, P) 92.4 85.6 88.1 83.6 75.0 84.2 84.0 41.8 79.3 G3-(A, L) 92.0 85.9 88.2 83.1 75.3 84.3 83.9 42.2 79.4 G3-(A, P, L) 92.6 86.0 87.5 83.4 75.6 84.6 83.5 43.9 79.6 G3-(P, B, L) 92.7 85.8 87.2 83.7 75.2 84.5 83.8 40.8 79.2 G3-(A, P, B) 93.3 85.8 88.6 84.0 75.5 84.9 84.1 42.1 79.8 G3-(A, B, L) 93.7 86.5 88.0 83.2 75.8 84.2 84.2 39.7 79.4 G3-(A, P, B, L) 93.3 85.6 87.7 83.8 75.2 84.3 84.4 41.6 79.4 Table 11: Average performances (low-compute, low-epoch regime: 100 random models, 3 tuning epochs) on the GLUE datasets using the T5-base pretrained backbone model. We compare adding different G4 strategy assignment constraints with G1-(A, L) – G2-(A, P) – G3-(A, P, B) to the S3 design space. Strategy Assignment SST-2 MNLI QNLI QQP RTE STS-B MRPC CoLA Avg G4-Adapter (A) 93.8 85.8 88.6 84.8 76.3 85.8 86.0 48.5 81.2 G4-Preﬁx (P) 93.5 85.2 88.3 83.6 76.8 85.3 85.6 44.8 80.3 G4-BitFit (B) 94.1 85.3 88.9 84.4 77.1 85.4 86.2 46.1 80.9 G4-LoRA (L) 94.0 86.0 89.2 85.0 77.2 85.5 85.8 47.7 81.3 G4-(P, L) 94.3 86.2 89.3 85.8 78.0 86.0 88.2 47.2 81.8 G4-(A, P) 94.1 86.2 89.6 85.4 77.9 86.2 86.9 45.3 81.4 G4-(A, L) 94.2 85.9 89.2 85.5 77.8 86.2 88.0 46.8 81.7 G4-(A, P, L) 94.1 85.8 88.8 85.7 77.4 86.5 87.9 44.8 81.3 G4-(P, B, L) 94.6 86.4 90.4 86.1 78.2 86.8 88.5 47.2 82.3 G4-(A, P, B) 94.5 86.0 89.6 86.0 78.0 86.2 88.1 44.8 81.6 G4-(A, B, L) 94.3 86.4 89.2 85.6 78.2 86.4 88.3 46.6 81.9 G4-(A, P, B, L) 94.2 86.2 89.2 85.9 78.5 86.1 88.0 45.3 81.6 Table 12: Average performances (low-compute, low-epoch regime: 100 random models, 3 tuning epochs) on the GLUE datasets using the T5-3b pretrained backbone model. We compare adding different layer grouping constraints to the S0 design space. Grouping Patterns SST-2 MNLI QNLI QQP RTE STS-B MRPC CoLA Avg S0-models 80.3 72.1 74.7 72.8 76.9 75.2 71.0 32.2 69.4 Increasing 84.4 75.7 83.0 78.3 82.7 80.3 76.3 42.1 75.3 Uniform 86.8 77.1 82.6 76.2 83.8 81.6 77.3 48.9 76.8 Decreasing 83.2 74.3 81.8 77.3 82.8 79.9 76.5 40.8 74.5 Spindle 88.6 78.8 83.7 77.7 84.2 80.9 78.3 44.6 77.1 Bottleneck 86.3 77.0 82.2 75.6 83.3 80.2 77.1 41.5 75.4 15Table 13: Average performances (low-compute, low-epoch regime: 100 random models, 3 tuning epochs) on the GLUE datasets using the T5-3b pretrained backbone model. We compare adding different layer parameter constraints to the S1 design space. Parameter Allocation SST-2 MNLI QNLI QQP RTE STS-B MRPC CoLA Avg Increasing 90.3 79.3 84.9 79.3 85.2 82.8 79.2 50.1 78.9 Uniform 90.6 80.8 84.6 79.7 85.5 82.4 78.9 50.8 79.1 Decreasing 88.6 78.2 83.5 78.1 84.4 81.5 78.1 49.6 77.7 Table 14: Average performances (low-compute, low-epoch regime: 100 random models, 3 tuning epochs) on the GLUE datasets using the T5-3b pretrained backbone model. We compare adding different tuning groups constraints to the S2 design space. Tunable Groups SST-2 MNLI QNLI QQP RTE STS-B MRPC CoLA Avg G1 88.3 78.3 82.2 77.4 82.1 80.7 76.1 49.4 76.8 G2 89.1 78.8 82.1 77.2 82.3 81.2 76.4 49.6 77.1 G3 89.6 78.5 82.6 78.1 83.8 81.9 77.4 48.7 77.5 G4 89.8 79.3 82.7 77.9 83.5 81.9 77.9 48.5 77.1 G1, G2 90.1 80.2 83.4 78.5 84.3 82.4 78.5 51.1 78.5 G3, G4 90.5 80.6 83.8 78.7 84.2 83 78.2 50.3 78.6 G1, G2, G3 90.6 80.3 84.9 79.3 84.7 82.9 79.3 50.2 79.0 G2, G3, G4 90.8 80.9 84.6 79.1 85.1 83.1 79.1 49.2 78.9 G1, G2, G3, G4 91.1 81.4 85.2 80.4 85.9 83.5 80.0 51.6 79.9 16Table 15: Average performances (low-compute, low-epoch regime: 100 random models, 3 tuning epochs) on the GLUE datasets using the T5-3b pretrained backbone model. We compare adding different strategy assignment con- straints following the process in Section 4.2.5. Strategy Assignment SST-2 MNLI QNLI QQP RTE STS-B MRPC CoLA Avg G1-Adapter (A) 91.1 81.4 86.1 80.5 86.7 83.3 80.1 50.8 80.0 G1-Preﬁx (P) 90.8 81.1 85.5 80.2 86.2 83.1 79.8 50.2 79.6 G1-BitFit (B) 90.2 81.3 85.1 79.6 85.8 82.8 79.6 49.5 79.2 G1-LoRA (L) 91.4 81.9 86.2 80.8 86.4 83.9 80.8 49.6 80.0 G1-(P, L) 91.8 82.9 86.8 81.3 87.1 84.2 81.6 52.3 81.0 G1-(A, P) 91.3 81.9 86.4 81.1 85.6 83.7 80.7 52.8 80.1 G1-(A, L) 91.6 82.3 86.1 81.5 85.8 84.9 81.5 51.8 80.6 G1-(A, P, L) 91.1 81.7 85.8 81.2 86.4 84.2 80.9 52.3 80.4 G1-(P, B, L) 91.5 82.8 86.3 81.4 86.1 83.6 81.2 51.5 80.5 G1-(A, P, B) 91.3 82.3 86.7 80.8 86.8 84.3 80.7 51.8 80.5 G1-(A, B, L) 91.7 82.5 86.2 81.3 86.3 84.6 81.3 51.7 80.7 G1-(A, P, B, L) 91.6 82.3 86.2 81.1 86.6 84.2 81.1 51.1 80.5 G2-Adapter (A) 92.1 82.5 86.4 81.8 87.2 84.8 81.8 53.8 81.3 G2-Preﬁx (P) 91.8 83.1 87.2 81.6 86.2 84.4 81.1 52.8 81.0 G2-BitFit (B) 91.2 82.1 86.4 81.1 86.3 84.6 80.3 53.1 80.6 G2-LoRA (L) 92.6 82.9 87.5 81.3 87.4 85.1 81.9 52.2 81.4 G2-(P, L) 91.6 82.7 87.6 81.6 87.8 85.3 82.1 52.8 81.4 G2-(A, P) 92.1 83.3 87.5 81.9 87.4 85.5 81.8 53.1 81.5 G2-(A, L) 92.5 83.7 88.1 82.2 87.4 85.7 82.9 53.6 82.1 G2-(A, P, L) 92.3 83.4 87.4 81.6 87.1 85.3 81.4 53.2 81.4 G2-(P, B, L) 91.8 83.1 87.4 81.5 87.2 85.1 82.7 53.8 81.5 G2-(A, P, B) 91.5 82.6 87.8 81.3 86.5 85.2 82.1 54.2 81.4 G2-(A, B, L) 92.6 83.5 87.2 82 87.3 86.5 82.5 52.8 81.8 G2-(A, P, B, L) 92.8 83.2 87.6 81.6 87.5 85.5 82.4 51.2 81.5 G3-Adapter (A) 92.6 84.1 88.3 81.8 87.8 85.4 82.8 55.2 82.2 G3-Preﬁx (P) 92.1 83.3 87.6 81.4 87.1 85.4 82.6 53.5 81.6 G3-BitFit (B) 92.4 83.9 88.4 82.1 87.2 85.8 82.4 53.3 81.9 G3-LoRA (L) 93.1 84.3 87.7 82.4 87.8 86.2 83.1 54.3 82.3 G3-(P, L) 92.8 84.1 88.7 82.6 88.2 86.2 83.3 54.7 82.6 G3-(A, P) 93.1 83.8 89.1 82.3 88.1 85.8 82.6 55.1 82.5 G3-(A, L) 92.7 84.5 88.4 82.8 88.2 86.1 83.5 54.6 82.6 G3-(A, P, L) 92.8 84.6 88.1 82.5 87.7 85.5 83.2 53.8 82.3 G3-(P, B, L) 93.6 84.9 89.3 83.1 88.2 86.5 83.9 55.8 83.2 G3-(A, P, B) 93.3 83.9 88.5 82.2 88.4 86.2 83.5 55.3 82.6 G3-(A, B, L) 93.4 84.2 88.9 82.6 87.8 85.8 84.2 54.9 82.7 G3-(A, P, B, L) 92.2 84.4 88.7 82.3 88.5 86.2 84.2 54.2 82.5 G4-Adapter (A) 92.8 85.2 89.1 83.5 87.8 86.5 84.2 56.3 83.2 G4-Preﬁx (P) 92.8 84.6 89.5 82.6 87.4 86.5 83.8 55.8 82.8 G4-BitFit (B) 93.8 84.9 89.5 83.3 88.7 86.8 84.4 55.2 83.3 G4-LoRA (L) 93.3 84.7 89.3 82.7 88.3 86.2 82.7 54.7 82.7 G4-(P, L) 93.8 85.3 89.6 83.6 88.6 86.8 84.6 56.3 83.5 G4-(A, P) 93.8 84.9 89.8 84.3 88.5 86.6 84.8 56.7 83.6 G4-(A, L) 93.7 85.6 89.5 84.1 88.2 86.6 85.2 55.4 83.5 G4-(A, P, L) 94.2 85.2 89.6 83.9 88.2 86.4 84.9 55.9 83.5 G4-(P, B, L) 93.8 85.9 89.8 83.6 88.6 86.9 85.2 56.3 83.7 G4-(A, P, B) 94.4 85.7 90.1 84.8 88.9 87.2 85.3 57.3 84.2 G4-(A, B, L) 93.8 85.3 89.5 84.1 88.8 86.7 85.5 56.6 83.7 G4-(A, P, B, L) 94.1 85.4 89.7 84.4 88.5 86.5 85.2 56.8 83.8 17Table 16: Average performances (low-compute, low-epoch regime: 100 random models, 3 tuning epochs) on the GLUE datasets using the T5-base pretrained backbone model. We compare adding different layer grouping constraints to the S0 design space. Layer grouping is based on 8 groups. Layer Grouping SST-2 MNLI QNLI QQP RTE STS-B MRPC CoLA Avg S0-models 76.9 70.1 72.5 73.3 63.6 71.7 73.8 24.3 65.7 Increasing 83.2 74 .1 76 .6 77 .1 67 .7 76.8 74.7 30.0 70.0 Uniform 83.6 73.4 78.0 77.9 68.2 76.4 78.6 34.2 71.3 Decreasing 80.3 71.6 77.4 75.5 67.0 75.3 77.2 26.4 68.9 Spindle 86.2 74.3 79.1 78.6 68.5 77.4 79.5 35.1 72.3 Bottleneck 83.2 73.1 75.8 77.6 67.9 75.3 78.2 31.4 70.3 Table 17: Performances of different tuning methods on the SuperGLUE datasets using the XLNet-base (upper part) and XLNet-large (lower part) pretrained backbone models, respectively. The results are averaged over 10 random runs. The S4-model and S4-3b-model perform signiﬁcantly better than the second-best PEFT methods in all the eight datasets at the signiﬁcance level p <0.05 (*) or even p <0.01 (**). Method BoolQ CB COPA MultiRC ReCoRD RTE WiC WSC Average Adapter 72.8 71.3/78.0 64.0 67.0/24.5 71.0/71.8 76.2 65.0 60.8 66.2 Preﬁx 72.0 70.5/77.0 63.3 66.4/23.8 69.9/71.0 75.5 64.4 60.8 65.9 BitFit 71.8 70.0/76.2 62.8 65.8/22.6 69.4/70.6 74.5 64.8 60.6 65.2 LoRA 72.2 71.1/77.8 64.7 67.4/24.8 70.8/71.3 76.8 65.1 61.1 66.4 S4-model 73.8∗∗ 71.7/78.4∗ 65.9∗∗ 68.2/25.5∗∗ 71.1/72.0∗ 78.4∗∗ 65.8∗ 62.6∗ 67.5 Adapter 74.4 71.4/81.1 67.4 68.8/26.4 71.7/72.4 80.8 68.0 64.6 68.8 Preﬁx 72.4 70.0/78.3 66.9 68.8/25.8 70.9/71.2 78.8 66.9 64.0 67.7 BitFit 71.1 70.7/79.8 68.0 68.6/25.4 71.1/71.6 80.4 67.2 64.3 68.1 LoRA 74.1 72.1/80.9 67.9 69.1/26.8 72.0/72.8 81.0 67.8 64.4 69.0 S4-3b-model 76.8∗∗ 74.6/81.9∗∗ 68.6∗∗ 69.5/27.1∗ 72.4/73.3∗ 81.2∗ 68.2∗∗ 64.8∗ 69.7 Table 18: Total training time (low-compute, low-epoch regime: 100 random models, 3 tuning epochs) on the GLUE datasets using the T5-base pretrained backbone model with 8 A100 GPUs from S0 to S1. SST-2 MNLI QNLI QQP RTE STS-B MRPC CoLA 18 mins 22 mins 20 mins 40 mins 8 mins 12 mins 8 mins 6 mins 18",
      "meta_data": {
        "arxiv_id": "2301.01821v1",
        "authors": [
          "Jiaao Chen",
          "Aston Zhang",
          "Xingjian Shi",
          "Mu Li",
          "Alex Smola",
          "Diyi Yang"
        ],
        "published_date": "2023-01-04T21:00:18Z",
        "pdf_url": "https://arxiv.org/pdf/2301.01821v1.pdf",
        "github_url": "https://github.com/amazon-science/peft-design-spaces"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper addresses the lack of systematic design patterns in parameter-efficient fine-tuning (PEFT) strategies, which are typically hand-crafted and suffer from unclear applicability across different settings. The main contribution is the introduction of PEFT design spaces, parameterized by layer grouping, trainable parameter allocation, tunable groups, and strategy assignment. Through progressive refinement of these design spaces, the authors discover four key design patterns: (i) grouping layers in a spindle pattern, (ii) allocating trainable parameters uniformly across layers, (iii) tuning all layer groups, and (iv) assigning proper tuning strategies to different groups. These discovered patterns lead to new PEFT methods (S4-model) that consistently and significantly outperform existing individual PEFT strategies across various backbone models (T5, RoBERTa, BART, XLNet) and NLP tasks (GLUE, XSum, WMT, SuperGLUE).",
        "methodology": "The methodology involves defining parameter-efficient fine-tuning (PEFT) design spaces characterized by four components: layer grouping, trainable parameter allocation, tunable groups, and strategy assignment. Starting from an unconstrained initial design space (S0), the authors progressively refine the space by applying different constraints to each component. The refinement process is guided by comparing the overall model quality (average GLUE performance) of models randomly sampled from design spaces under various constraints in a low-compute, low-epoch regime. Greedy selection is applied at each stage to identify the most effective design pattern for each component (e.g., spindle grouping, uniform parameter allocation, tuning all groups, and specific strategy assignments like G1-(A, L) – G2-(A, P) – G3-(A, P, B) – G4-(P, B, L) for T5-base). This empirical discovery process is applied to T5-base and T5-3b models.",
        "experimental_setup": "The design patterns were discovered and validated using several pretrained backbone models and NLP tasks. For pattern discovery, T5-base and T5-3b were primarily used, with GLUE benchmark datasets (SST-2, CoLA, QQP, STS-B, MRPC, MNLI, QNLI, RTE) as the main evaluation platform. Performance metrics included Matthews correlation for CoLA, Spearman correlation for STS-B, and accuracy for other GLUE tasks. The discovery process involved sampling 100 random models from a design space, fine-tuning for 3 epochs, and computing average GLUE performances. For evaluating the general effectiveness of the discovered patterns (S4-model and S4-3b-model), additional backbone models like RoBERTa-base/large, BART-base/large, and XLNet-base/large were used. Evaluation extended to generation tasks, including abstractive summarization (XSum dataset, ROUGE scores) and machine translation (WMT 2016 en-ro dataset, BLEU scores), and SuperGLUE datasets. The total trainable parameters were set to 0.5% for Adapter, Prefix, LoRA, and the new methods, and 0.1% for BitFit. Training employed a linear decay scheduler with a 0.06 warmup ratio, batch sizes of 128 (base) or 64 (large), a maximum learning rate of 5e-5, and 5-10 training epochs, utilizing 8 A100 GPUs. Baselines included full fine-tuning and individual PEFT strategies: Adapter, Prefix, BitFit, and LoRA, with statistical significance testing.",
        "limitations": "The primary limitation noted by the authors is that their goal was to demonstrate how the perspective of design spaces can inform PEFT research, rather than to exhaustively enumerate all possible design spaces or find the 'best' individual method. The computational efficiency necessitated a focused exploration of constraints and a specific discovery sequence for the design patterns. While the discovered patterns are shown to generalize, the exhaustive exploration of all potential constraints for all components remains beyond the scope of this work due to computational cost.",
        "future_research_directions": "Not mentioned",
        "experimental_code": "if is_torch_available():_import_structure[\"adapters\"] = [\"ADAPTER_CACHE\",\"ADAPTER_CONFIG_MAP\",\"ADAPTERFUSION_CONFIG_MAP\",\"ADAPTER_MODEL_MAPPING\",\"DEFAULT_ADAPTER_CONFIG\",\"DEFAULT_ADAPTERFUSION_CONFIG\",\"MODEL_WITH_HEADS_MAPPING\",\"AdapterArguments\",\"AdapterConfig\",\"AdapterConfigBase\",\"AdapterFusionConfig\",\"AdapterInfo\",\"AdapterLayer\",\"AdapterLayerBase\",\"AdapterSetup\",\"AdapterTrainer\",\"AdapterType\",\"AutoAdapterModel\",\"AutoModelWithHeads\",\"BartAdapterModel\",\"BartModelWithHeads\",\"BeitAdapterModel\",\"BertAdapterModel\",\"BertModelWithHeads\",\"CompacterConfig\",\"CompacterPlusPlusConfig\",\"ConfigUnion\",\"DebertaAdapterModel\",\"DebertaV2AdapterModel\",\"DistilBertAdapterModel\",\"DistilBertModelWithHeads\",\"DynamicAdapterFusionConfig\",\"EmbeddingAdaptersMixin\",\"ForwardContext\",\"GPT2AdapterModel\",\"GPT2ModelWithHeads\",\"GPTJAdapterModel\",\"HoulsbyConfig\",\"HoulsbyInvConfig\",\"IA3Config\",\"InvertibleAdaptersMixin\",\"LoRAConfig\",\"MAMConfig\",\"MBartAdapterModel\",\"MBartModelWithHeads\",\"ModelAdaptersConfig\",\"ModelAdaptersMixin\",\"ModelWithFlexibleHeadsAdaptersMixin\",\"ModelWithHeadsAdaptersMixin\",\"MultiLingAdapterArguments\",\"ParallelConfig\",\"PfeifferConfig\",\"PfeifferInvConfig\",\"PrefixTuningConfig\",\"RobertaAdapterModel\",\"RobertaModelWithHeads\",\"Seq2SeqAdapterTrainer\",\"StaticAdapterFusionConfig\",\"T5AdapterModel\",\"T5ModelWithHeads\",\"PEFTConfig\",\"ViTAdapterModel\",\"XLMRobertaAdapterModel\",\"XLMRobertaModelWithHeads\",\"get_adapter_config_hash\",\"get_adapter_info\",\"list_adapters\",]",
        "experimental_info": "The repository is explicitly named \"adapter-transformers\", indicating a focus on adding Adapters (PEFT) to PyTorch language models. The `transformers/__init__.py` file defines a comprehensive `adapters` module, which lists various `AdapterConfig` classes (e.g., `HoulsbyConfig`, `LoRAConfig`, `PfeifferConfig`, `MAMConfig`, `IA3Config`, `PrefixTuningConfig`, `CompacterConfig`, `ParallelConfig`, `DynamicAdapterFusionConfig`, `StaticAdapterFusionConfig`), which directly correspond to the 'trainable parameter allocation' and 'strategy assignment' components of the PEFT design spaces mentioned in the method. The `AdapterTrainer` class is also present, which would facilitate the 'low-compute, low-epoch regime' and 'average GLUE performance' evaluations. The `setup.py` lists `datasets` as a dependency, and `glue_compute_metrics` is listed in `data.metrics.__init__.py`, supporting the use of GLUE benchmarks for model quality assessment. Specific details about the low-compute/low-epoch regime (e.g., exact epoch numbers, compute budget) or the precise implementation of greedy selection were not found in the provided content, but the necessary components for such experimentation are clearly part of the library infrastructure."
      }
    },
    {
      "title": "On Robust Prefix-Tuning for Text Classification",
      "abstract": "Recently, prefix-tuning has gained increasing attention as a\nparameter-efficient finetuning method for large-scale pretrained language\nmodels. The method keeps the pretrained models fixed and only updates the\nprefix token parameters for each downstream task. Despite being lightweight and\nmodular, prefix-tuning still lacks robustness to textual adversarial attacks.\nHowever, most currently developed defense techniques necessitate auxiliary\nmodel update and storage, which inevitably hamper the modularity and low\nstorage of prefix-tuning. In this work, we propose a robust prefix-tuning\nframework that preserves the efficiency and modularity of prefix-tuning. The\ncore idea of our framework is leveraging the layerwise activations of the\nlanguage model by correctly-classified training data as the standard for\nadditional prefix finetuning. During the test phase, an extra batch-level\nprefix is tuned for each batch and added to the original prefix for robustness\nenhancement. Extensive experiments on three text classification benchmarks show\nthat our framework substantially improves robustness over several strong\nbaselines against five textual attacks of different types while maintaining\ncomparable accuracy on clean texts. We also interpret our robust prefix-tuning\nframework from the optimal control perspective and pose several directions for\nfuture research.",
      "full_text": "Published as a conference paper at ICLR 2022 ON ROBUST PREFIX -TUNING FOR TEXT CLASSIFICA - TION Zonghan Yang, Yang Liu Department of Computer Science and Technology, Institute for AI Industry Research Institute for Artiﬁcial Intelligence, Tsinghua University, Beijing, 100084, China yangzh20@mails.tsinghua.edu.cn, liuyang2011@tsinghua.edu.cn ABSTRACT Recently, preﬁx-tuning has gained increasing attention as a parameter-efﬁcient ﬁnetuning method for large-scale pretrained language models. The method keeps the pretrained models ﬁxed and only updates the preﬁx token parameters for each downstream task. Despite being lightweight and modular, preﬁx-tuning still lacks robustness to textual adversarial attacks. However, most currently developed de- fense techniques necessitate auxiliary model update and storage, which inevitably hamper the modularity and low storage of preﬁx-tuning. In this work, we propose a robust preﬁx-tuning framework that preserves the efﬁciency and modularity of preﬁx-tuning. The core idea of our framework is leveraging the layerwise activa- tions of the language model by correctly-classiﬁed training data as the standard for additional preﬁx ﬁnetuning. During the test phase, an extra batch-level preﬁx is tuned for each batch and added to the original preﬁx for robustness enhance- ment. Extensive experiments on three text classiﬁcation benchmarks show that our framework substantially improves robustness over several strong baselines against ﬁve textual attacks of different types while maintaining comparable ac- curacy on clean texts. We also interpret our robust preﬁx-tuning framework from the optimal control perspective and pose several directions for future research 1. 1 I NTRODUCTION Large-scale pretrained language models (LMs) (Peters et al., 2018; Devlin et al., 2019; Radford et al., 2019; Liu et al., 2019; Yang et al., 2019; Raffel et al., 2020; Lewis et al., 2020; Brown et al., 2020; Xue et al., 2021) have proven effective for downstream NLP tasks. While ﬁnetuning a pre- trained model for a speciﬁc task has been the common practice, it comes at the cost of maintaining a full copy of the LM with the parameters entirely modiﬁed. The prohibitively huge memory de- mand poses a severe challenge for the deployment of practical NLP systems, which motivates the development of low-storage adaptation methods (Houlsby et al., 2019; Li & Liang, 2021). Recently, increasing interest has been focused on prompt-based tuning approaches for pretrained language models (Wallace et al., 2019; Puri & Catanzaro, 2019; Shin et al., 2020; Jiang et al., 2020b; Zhong et al., 2021; Gao et al., 2021; Hu et al., 2021; Liu et al., 2021). By prepending several elaborately-selected tokens to the given input sequences, the LM is triggered to respond with appropriate outputs without updating its parameters. Preﬁx-tuning (Li & Liang, 2021) introduces the idea of replacing the discrete prompt tokens at the input with the virtual ones at the start of each layer in the LM. By optimizing the layerwise continuous preﬁx embedding instead of selecting candidates in the vocabulary list, the expressive ability of prompts is further enhanced with a rather small amount of parameters to be updated. As a result, preﬁx-tuning requires near 1000×fewer task-speciﬁc parameters than ﬁnetuning the entire pretrained model (Bommasani et al., 2021). Despite being lightweight and modular, preﬁx-tuning is still lacking in robustness. In the NLP community, a variety of techniques for generating adversarial examples have been proposed to attack a text classiﬁer by perturbing inputs (Zhang et al., 2020). Conventional attack techniques include character-level (Eger et al., 2019; He et al., 2021), word-level (Alzantot et al., 2018; Ren et al., 1We release the code at https://github.com/minicheshire/Robust-Prefix-Tuning 1 arXiv:2203.10378v1  [cs.CL]  19 Mar 2022Published as a conference paper at ICLR 2022 Figure 1: Overview of preﬁx-tuning as well as our robust preﬁx-tuning framework for text classiﬁ- cation. We frame the samples into a SQuAD-like scheme consisting ofcontext, question, and label and optimize the original preﬁx Pθ. For the robust preﬁx-tuning framework, we ﬁx the obtained pre- ﬁx Pθ and tune an additional preﬁx P′ ψ for each test batch. The additional tuning follows the three steps indicated in the ﬁgure, which aims to lead the summed preﬁx to steer correct activations at the position of the [ANS] token with those activated by correctly classiﬁed training data as the standard. 2019; Garg & Ramakrishnan, 2020), sentence-level modiﬁcation (Iyyer et al., 2018; Ribeiro et al., 2018; Xu et al., 2021), or a mixture of them (Ebrahimi et al., 2018; Li et al., 2019). Instead of perturbing each input sentence separately, recently, universal adversarial triggers (UAT) (Wallace et al., 2019) becomes powerful by prepending the same adversarial tokens to all test inputs. UAT prompts the model to generate malicious outputs, which shares the same spirit with the prompt- based tuning approaches. It remains a mystery whether preﬁx-tuning, a variant of prompt-based tuning techniques, can defend against UAT as well as other different kinds of attacking techniques. In defense of adversarial attacks, different types of defense techniques are developed, including model functional improvement (Li & Sethy, 2019; Jones et al., 2020), certiﬁcation (Jia et al., 2019; Huang et al., 2019; Shi et al., 2020; Xu et al., 2020; Ye et al., 2020), adversary detection (Pruthi et al., 2019; Zhou et al., 2019), and adversarial training (Miyato et al., 2017; 2019; Zhu et al., 2020; Jiang et al., 2020a; Liu et al., 2020; Wang et al., 2021; Dong et al., 2021; Zhou et al., 2021). While these approaches have enhanced model robustness, difﬁculties emerge when ﬁtted to preﬁx-tuning. Most of the techniques require modiﬁcation to the architecture and the parameters of the LM or additional maintenance of adversary detectors. Directly applying such techniques necessitates auxiliary model update and storage, which will inevitably hamper the modularity of preﬁx-tuning. Moreover, The excessively long time for adversarial training is also a hindrance to the efﬁcient use of preﬁx-tuning. We ask the following question: Can we improve the robustness of preﬁx-tuning while preserving its efﬁciency and modularity, without modifying the pretrained model parameters? In this work, we propose a robust preﬁx-tuning framework for text classiﬁcation. The main idea of our framework is to add an extra batch-level preﬁx tuned for each batch to the original preﬁx embed- ding during test time for robustness enhancement. We ﬁrst record the layerwise activations in the 2Published as a conference paper at ICLR 2022 LM at the position of generating label prediction with correctly classiﬁed training data. We project the collected activation matrices of each layer onto low-level canonical manifolds as the charac- terization of “correct” model behavior. In this way, the correctness of any layerwise activations at the position of prediction generation can be estimated by projecting to the canonical manifolds and measuring the distance between them. For each test batch during inference, the added extra preﬁx is tuned on the ﬂy with the original preﬁx ﬁxed to minimize the calculated distance. Triggered by the summed preﬁx, the LM is prone to generating correct label predictions. We conduct extensive experiments on three text classiﬁcation benchmarks and show that the proposed framework sub- stantially improves model robustness against ﬁve strong textual attack approaches including input perturbation attack of different levels as well as the UAT attack. To the best of our knowledge, we are the ﬁrst to propose the defense approach for preﬁx-tuning while keeping its lightweightness and modularity. Moreover, we provide an interpretation of our robust preﬁx-tuning framework from the optimal control perspective and pose several directions for future research. 2 P REFIX -TUNING FOR TEXT CLASSIFICATION Preﬁx-tuning is a lightweight alternative to ﬁnetuning when using large-scale pretrained language models to solve downstream NLP tasks. The intuition of preﬁx-tuning follows prompt-based meth- ods that a proper context prepended to input sentences triggers the desired response of the LM without changing the large amount of LM parameters. Instead of instantiating the prepended con- text with discrete tokens, preﬁx-tuning uses trainable preﬁx embeddings as a replacement, which is also known as soft prompts. The continuous preﬁx embeddings enable continuous optimization and are prepended to all Transformer layers to improve expressiveness. Following the notation of Li & Liang (2021), the activation at the i-th position of the j-th layer in an L-layer autoregressive Transformer LM is denoted as h(j) i . hi = [h(0) i ; ··· ; h(L−1) i ] represents the stacked activations: hi = {Pθ[i,:], if i∈Pidx, LMφ(zi,h<i), otherwise. (1) where Pidx is the sequence of preﬁx indices and zi is the i-th token in the input sequence. The activations of the ﬁrst |Pidx|positions are directly calculated by Pθ. All of the activations at the following positions depend on the preﬁx as the autoregressive LM follows the left-to-right calcula- tion process. To stabilize the optimization, the preﬁx embedding matrix Pθ is reparameterized as Pθ[i,:] = MLPθ( ˆPθ[i,:]) by a feedforward network MLPθ with a smaller matrix ˆPθ. While preﬁx-tuning is proposed for conditional generation tasks, in this work, we use preﬁx-tuning for text classiﬁcation. As shown in Figure 1, following the protocol of decaNLP (McCann et al., 2018), we frame the samples in classiﬁcation tasks into a SQuAD-like scheme consisting ofcontext, question, and label: the context and the label part refer to the text sequence to be classiﬁed and the ground-truth label, while the question part is a prescribed task description sentence ﬁxed for all samples. We denote x= [context,question,[ANS]], where [ANS] is a special token that separates question and label. We let y = [label] and |y|= 1 as the label is one token. At the position that [ANS] is inputted, the LM generates the prediction of the next label token, and we denote this position as the output position o. While o can be different for different input x’s, in this paper, we omit the relation o = o(x) for simplicity. Preﬁx-tuning aims to steer the LM to maximize the probability of the label. We use all samples in the training setDtr to optimize the preﬁx Pθ[i,:]. The objective is min θ E(x,y)∼Dtr L(y|x; θ) = max θ E(x,y)∼Dtr log [ W ( h(L) o )] y , (2) where W in the LM transforms the top-layer outputh(L) o to a probability vector over the vocabulary. With continuous optimization on training samples, preﬁx-tuning is expected to steer the LM to gen- erate correct label predictions for test data. With the large-scale LM parameters ﬁxed, the obtained task-speciﬁc preﬁx is lightweight and modular. However, preﬁx-tuning is still vulnerable to text attacks. With the context part perturbed by text attack techniques, the LM can be fooled to generate erroneous label prediction at the output position. Figure 1 shows an example of perturbation: by modifying a single character m in the word remember with k, the prediction of the LM is shifted from positive to negative. Therefore, it remains under exploration how to robustify preﬁx-tuning without hampering its modularity or introducing additional large model updates and storage. 3Published as a conference paper at ICLR 2022 3 R OBUST PREFIX -TUNING We propose a robust preﬁx-tuning framework for text classiﬁcation. Our intuition follows preﬁx- tuning that proper preﬁx embeddings prepended to inputs can steer a LM with correct responses. When the inputs are adversarially perturbed, the LM activations at the output position fail to be steered in the correct way by the original preﬁx Pθ[i,:]. Inspired by Khoury & Hadﬁeld-Menell (2018) that the perturbed data often deviates from the low-dimensional data manifold, our robust preﬁx-tuning framework uses the layerwise activations by correctly classiﬁed training data to con- struct canonical manifolds M. When provided with perturbed inputs during inference, we add an extra preﬁx P′ ψ[i,:] tuned for each test batch to Pθ[i,:] that aims to rectify the erroneous activations at the output position so that they stay close to the canonical manifolds. In this way, we expect the summed preﬁx to steer the LM with correct label generation against input perturbations. As shown in Figure 1, our robust preﬁx-tuning framework consists of three steps. The ﬁrst step is collecting correct LM activations at the output position o triggered by Pθ[i,:]. We denote SC as the set of correctly classiﬁed training examples. For the j-th layer, the collected activation matrixH(j) C stacks the j-th layer LM activation at the output position owith the input of all c∈SC: H(j) C = [ h(j) o,c ] ∈R|SC|×d. (3) The drepresents the dimension of the LM hidden state. In practice, we always have |SC|>>d. The second step is constructing canonical manifolds. We project the collected j-th layer activation matrix H(j) C onto a low-level manifoldM(j) as the characterization of the correctj-th layer behavior. We use PCA (Pearson, 1901) to get the projectionQ(j) onto the canonical manifold of thej-th layer: ˜H(j) C = U(j)Σ(j)V(j)T , (4) Q(j) = V(j) p T V(j) p , (5) where ˜H(j) C = H(j) C −11TH(j) C /|SC|normalizes the rows ofH(j) C to mitigate the randomness among samples in SC before projection. V(j) p consists of the ﬁrst psingular vectors and Q(j) ∈Rd×d. The third step is tuningP′ ψ[i,:] to robustify preﬁx-tuning during inference. Here the vectorP′ ψ[i,:] is not reparameterized by MLP. With the additional preﬁx P′ ψ[i,:], the token-wise activations become hi = {Pθ[i,:] + P′ ψ[i,:], if i∈Pidx, LMφ(zi,h<i), otherwise. (6) For the j-th layer at the output position, the LM activation matrix triggered byPθ[i,:]+ P′ ψ[i,:] with the potentially perturbed test input batch ST is stacked as H(j) T = [ h(j) o,t ] ∈R|ST |×d (7) for all t ∈ST. We use the distance from H(j) T to the j-th canonical manifold M(j) as the loss for the tuning of P′ ψ[i,:] for each batch. Projecting H(j) T to M(j) yields H(j) T Q(j), thus the objective is min ψ N−1∑ j=0 L ( ψ(j),Q(j) ) = N−1∑ j=0 H(j) T ( I−Q(j) ) 2 . (8) We also replace the H(j) T in Eq. (8) with H(j) T −11TH(j) T /|ST|as normalization before projection to mitigate randomness among test samples when|ST|>1. After tuning P′ ψ[i,:], the activated H(j) T is closer to M(j). As the manifold characterizes the correct behavior of the j-th layer activation, by regulating the layerwise activations at the output position, the summed preﬁxPθ[i,:] +P′ ψ[i,:] is prone to steering the LM to generate correct label predictions. Our framework is also applicable to other soft prompt-based tuning methods (Qin & Eisner, 2021; Hambardzumyan et al., 2021; Lester et al., 2021; Cho et al., 2021; Tsimpoukelli et al., 2021) by recording the activations of correctly classiﬁed training data, constructing canonical manifolds for the soft prompts, and tuning additional soft prompts for robustness during inference. In this work, we conduct experiments on preﬁx-tuning. Remark. From the optimal control (OC) perspective, preﬁx-tuning can be formalized as seeking the OC of the pretrained LM for downstream tasks, and our robust preﬁx-tuning can be interpreted as seeking the close-loop control for robust downstream tasks. We attach the details in Appendix G. 4Published as a conference paper at ICLR 2022 Table 1: Results of different baselines with our framework applied. Our framework substantially improves robustness of both standard and adversarial preﬁx-tuning against all types of attacks. Benchmark Method Clean PWWS VIPER SCPN BUG UAT SST-2 std. preﬁx-tuning 92.48 16.64 1.92 31.58 8.84 5.05 + our framework 92.59 50.36 44.65 58.54 46.68 85.72 adv. preﬁx-tuning 93.57 30.64 7.25 35.42 25.04 4.88 + our framework 93.79 57.55 43.60 60.68 57.17 91.87 AG’s News std. preﬁx-tuning 86.42 43.00 24.55 43.20 43.22 52.20 + our framework 86.50 53.91 24.93 48.38 51.79 76.47 adv. preﬁx-tuning 89.46 50.91 26.30 45.25 47.11 59.74 + our framework 90.26 56.45 31.25 48.03 54.66 87.26 SNLI std. preﬁx-tuning 72.48 25.51 29.69 42.46 32.88 30.20 + our framework 72.74 34.68 33.64 43.37 36.59 71.03 adv. preﬁx-tuning 77.22 27.43 28.58 46.83 34.94 35.76 + our framework 77.65 33.88 33.98 46.92 38.21 76.15 4 E XPERIMENTS 4.1 S ETUP While we have also provided the regular ﬁnetuning as baselines We consider three text classiﬁcation benchmarks in our experiments: binary Stanford Sentiment Treebank (SST-2) (Socher et al., 2013), AG’s News (Zhang et al., 2015), and Stanford Natural Language Inference (SNLI) (Bowman et al., 2015). We evaluate our robust preﬁx-tuning with ﬁve text attacks: PWWS (Ren et al., 2019), VIPER (Eger et al., 2019), SCPN (Iyyer et al., 2018), TextBugger (“BUG” for short) (Li et al., 2019), and UAT (Wallace et al., 2019). We use the GPT2-medium (with L = 24 layers in total) as the large- scale LM and set preﬁx length= 10 for all preﬁx-tuning experiments. We train 100 epochs for SST-2 and 25 epochs for AG’s News and SNLI. We setN = 3 and record the bottomN-layer activations of the LM at the output position for the additional tuning. Other experiment conﬁgurations and details can be found in Appendix B. We have also conducted both standard and adversarial full tuning experiments (Appendix A) to discuss the challenges and opportunities in robustifying preﬁx-tuning. 4.2 R ESULTS ON ADVERSARIAL TRAINING We ﬁrst apply adversarial training to preﬁx-tuning as our baselines named adversarial preﬁx-tuning. Following Miyato et al. (2017), we inject perturbation restricted within the ℓ2 ball of the ﬂattened word embedding of original sentences during training to adversarially optimize the preﬁx Pθ[i,:]. We have also attempted to use other types of adversarial training, (i.e., adding the KL-divergence term for regularization (Miyato et al., 2019; Zhang et al., 2019)) but obtained poor accuracy, suggest- ing the difﬁculty of optimizing the small amount of preﬁx embedding parameters. The experimental details can be found in Appendix C.1. Our framework is applicable to the adversarially-trained preﬁx embedding, as it keeps the acquired preﬁx Pθ[i,:] ﬁxed and tunes extra P′ ψ[i,:] for each test batch. The experimental results are listed in Table 1. According to Table 1, our approach signiﬁ- cantly improves the robustness of preﬁx-tuning over all baselines against each type of text attack. To the best of our knowledge, our framework is the ﬁrst to defend against UAT on large-scale pre- trained LMs (see Appendix A for details). Compared with the standard preﬁx-tuning baseline, the adversarial baselines achieve better robustness and clean accuracy. In fact, fewer training epochs are needed to achieve certain clean accuracy when applying adversarial training to preﬁx-tuning. Figure 2-(a) illustrates the loss on the training set and the clean accuracy on the validation set for each epoch during standard and adversarial preﬁx-tuning. From Figure 2-(a), it is clear that adversarial preﬁx-tuning outperforms standard preﬁx-tuning in both the convergence rate and generalization. However, the adversarial preﬁx-tuning approaches suffer from the catastrophic drawback of taking far greater training time than the standard preﬁx-tuning baseline. Figure 2-(b) changes the horizon- tal axis in Figure 2-(a) from epoch to clock time. According to Figure 2-(b), given the same time budget, standard preﬁx-tuning ﬁnishes training for 100 epochs while adversarial preﬁx-tuning only manages to train for 20 epochs. The training losses of the two methods at the time are also roughly 5Published as a conference paper at ICLR 2022 (a)  (b) Figure 2: Comparison between standard and adversarial preﬁx-tuning for 100 epochs with respect to (a) epoch and (b) clock time. While adversarial preﬁx-tuning gains strengths in epoch-wise con- vergence rate and generalization, it takes far greater training time than standard preﬁx-tuning. the same ( 0.06). Earlystopping adversarial preﬁx-tuning at 20 epochs achieves slightly better ro- bustness results than 100 epochs, but the clean accuracy becomes lower than standard preﬁx-tuning (results can be found in Appendix C.2). To conclude, the adversarial preﬁx-tuning approaches make regularization effect on preﬁx-tuning but lack competitiveness in practice. In contrast with adversarial preﬁx-tuning, our approach is more practical as the duration of the inference-phase on-the-ﬂy optimization of the additional preﬁx P′ ψ[i,:] is negligible compared with training-phase adversarial preﬁx-tuning. From Table 1, the robustness of the std. preﬁx-tuning applied with our approach has surpassed that of the adv. preﬁx-tuning without our approach. The adv. preﬁx-tuning with our framework achieves the strongest robustness against most attacks. 4.3 R ESULTS ON ADVERSARIAL DATA AUGMENTATION Table 2: Results of PWWS adversarial data augmentation baselines as well as our methods on the SST-2 benchmark. Our methods consistently improve robustness over all baselines. Method Clean PWWS VIPER SCPN BUG UAT std. preﬁx-tuning 92.48 16.64 1.92 31.58 8.84 5.05 + our framework 92.59 50.36 44.65 58.54 46.68 85.72 20-epoch adv. aug. 87.15 48.11 42.12 48.11 47.01 10.43 + our framework 86.93 55.57 51.62 57.33 52.00 70.07 50-epoch adv. aug. 86.82 55.08 35.04 44.65 49.37 3.13 + our framework 86.66 58.26 48.05 58.98 51.73 83.47 100-epoch adv. aug. 91.21 56.62 23.01 38.55 45.14 7.91 + our framework 90.39 62.55 46.95 59.25 54.75 86.38 In this section, we conduct adversarial data augmentation during the training phase of standard preﬁx-tuning as our baseline methods. During training, we use a speciﬁed text attack method to perturb each batch and augment the perturbed data to the training set. Due to the computational inefﬁciency of discrete optimization for text attacks, the training of adversarial data augmentation is even far slower than that of adversarial preﬁx-tuning introduced in Section 4.2. We set milestones for the training of adversarial data augmentation at 20, 50, and 100 epochs for comparison. When applying our framework, we calculate the canonical manifolds Musing the correctly classiﬁed samples from both the clean training set and the perturbed training set by the speciﬁed text attack. Considering the training time budget, we select SST-2 as the benchmark. We use PWWS as the attack method to generate perturbed inputs with results listed in Table 2. On one hand, the clean accuracy of the PWWS adversarial data augmentation method is affected at the 20-epoch and the 50- epoch milestones. This might be attributed to the consideration of perturbed inputs during training. The robustness results, on the other hand, are improved compared with standard preﬁx-tuning. The robustness against PWWS obtains steady improvement thanks to the adversarial data augmentation 6Published as a conference paper at ICLR 2022 generated by the attack method of the same type. However, for other types of attack, the robustness improvement is decreasing with the training process, suggesting the possibility that overﬁtting to the speciﬁed type of adversarial data can be harmful to robustness against other types of attacks. With our robust preﬁx-tuning framework applied, all milestones of the adversarial data augmenta- tion baseline obtain substantial robustness improvement. While we calculate the layerwise canonical manifolds with the correctly classiﬁed samples from both the clean training set and the PWWS- perturbed training set, it can be seen that the robustness against PWWS attack is still enhanced by several percent, while the clean accuracy is slightly affected. This shows that when calculating the layerwise manifolds, the used samples from the two types of training data, though are all correctly classiﬁed, trigger the LM with activations of different properties at the output position. It is left for future work about how to construct the dataset to calculate the canonical manifolds to achieve the best performance on both accuracy and robustness. We have also used VIPER and SCPN as the at- tack methods for adversarial data generation and draw similar conclusions to the PWWS adversarial data generation experiments, results of which can be found in Appendix D. 4.4 E FFECT OF TUNING DIFFERENT LAYERS In this section, we study the effect of tuning our robust preﬁxes with different layers. While we tuned the robust preﬁxes of the bottom N = 3 layers for all previously reported experiments, different results are obtained when tuning the robust preﬁxes of alternative layers. We experiment on the SST-2 development set by tuning the bottom N-layer robust preﬁxes as well as the top ones with N enumerating from 0 to 24 with the step size = 3 . N = 0 represents the original standard preﬁx-tuning method, and N = 24 means tuning all layers. The results are shown in Figure 3. According to the results, tuning the bottom N layers achieves better robustness improvement than (a) Tuning bottom N layers  (b) Tuning top N layers Figure 3: Results on the SST-2 development set when applying robust preﬁxes of different layers. Tuning bottom N layers outperforms tuning top N layers in terms of robustness improvement. tuning the top ones. This can be attributed to layerwise bottom-up error accumulation of the output position activations triggered with perturbed inputs. For the in-sentence text attacks, tuning the bottom N ≤15 layers achieves comparable robustness improvement and slightly outperforms the larger N’s. For the UAT attack, setting the bottom N = 3 is signiﬁcantly better than the choices of larger N’s. One possible reason for the exceptional performance is that the collected low-layer activations capture richer positional information (V oita et al., 2019), which helps to defend against UAT as UAT shifts right all inputs with the same trigger. We proceed to study how our method steers the LM with different N’s for a better understanding of its robustness improvement in Section 5. 4.5 P ERFORMANCE UNDER MORE REALISTIC THREAT MODELS In the previous experiments, the robustness evaluation metric for each method is the accuracy under speciﬁc attack on the test set. While this has been adopted in previous work on adversarial defense in NLP (Dong et al., 2021; Si et al., 2021; Li et al., 2021), in this section, we consider more realistic threat models. We ﬁrst study the effect of test batch size on the performance of our framework. We used an adaptive test batch size to make full use of GPU memory in previous experiments (detailed in Appendix B). However, the loss in Eq. (8) depends on the size of the inference batch, and models in deployment should respond with real-time data. Based on standard preﬁx-tuning on SST-2, we 7Published as a conference paper at ICLR 2022 Table 3: Performance of our method with std. preﬁx-tuning with various test batch sizes on SST-2. Method Clean PWWS VIPER SCPN BUG UAT std. preﬁx-tuning 92.48 16.64 1.92 31.58 8.84 5.05 ours, bsz adaptive 92.59 50.36 44.65 58.54 46.68 85.72 ours, bsz = 1 92.64 51.46 43.99 58.32 46.73 73.09 ours, bsz = 2 92.48 49.92 45.74 59.14 48.93 84.73 ours, bsz = 4 92.64 50.36 45.85 58.54 49.31 86.66 additionally evaluate our framework with small test batch sizes:1, 2, and 4. According to the results in Table 3, a ﬁxed small test batch size achieves comparable or slightly better results under clean data and in-sentence attacks. Under UAT, however, there exists a performance gap of the robustness between our framework with test batch size of 1 and others. We discuss the phenomenon in depth and attach our attempt for improvement in Appendix E.1). Nevertheless, our framework with test batch size of 1 still substantially outperforms the baseline, and we also ﬁnd the performance gap is smaller on other benchmarks with other preﬁx-tuning baselines (see Appendix E.1). We continue to consider a more challenging yet more realistic setting where the test data is mixed with unperturbed and perturbed samples or perturbed samples under different attacks. We use test batch size of 1 in our framework based on adversarial preﬁx-tuning for SST-2. While it is unknown under which attack each test sample is perturbed (or whether it is perturbed), we ﬁnd the results shown in Table 4 still impressive when using a ﬁxed learning rate tuned on the development set under UAT for the additional tuning ofP′ ψ. We attach the detailed analysis in Appendix E.2. Table 4: Performance of our framework under mixed test data on SST-2. The ‘+’ denotes combina- tion of test set clean or under attack, and ‘C’ is short for “Clean”, ‘B’ for “BUG”, etc. Method C + B C + P V + B V + P S + B S + P U + B U + P adv. preﬁx-tuning 60.65 62.27 17.76 20.43 29.57 31.85 18.12 20.81 + ours, bsz = 1 69.71 71.11 46.02 47.23 55.63 56.92 70.04 71.67 5 H OW DOES ROBUST PREFIX -TUNING STEER THE LM? As the robust preﬁx-tuning framework with the proper choice of N substantially improves the ro- bustness of preﬁx-tuning, it deserves to be explored how the robust preﬁx-tuning steers the LM compared with the original preﬁx. In this section, we leverage different proxies to study the be- havior of our framework with the standard preﬁx-tuning baseline. We ﬁnd that our robust preﬁx- tuning framework steers different LM behavior between the in-sentence attacks and UAT. For the in-sentence attacks, the behavior of the ﬁnal layer of the LM can be summarized as averaging the attention. For UAT, with the proper setting ofN, the LM functions as ignoring the distraction. Table 5: Inputs from the SST-2 dev set and their perturbed versions by BUG/UAT attack under which the prediction is ﬂipped. The two examples are used as case studies in Sections 5.1 and 5.2. Type Input (the context part) Predict Label Original one from the heart . positive positive BUG-attack One from te hart . negative positive Original it ’s just ﬁller . negative negative UAT-attack lifts mates who it ’s just ﬁller . positive negative 5.1 I N-SENTENCE ATTACKS : AVERAGING THE ATTENTION In this subsection, we leverage attention weights in the ﬁnal layer of the LM as the proxy and use the adversarial inputs generated by the BUG attack on SST-2 development set for our following in-sentence case studies. We ignore the preﬁx embeddings and renormalize the attention weights of the rest of the tokens in visualization. Note that the observed behavior is of the ﬁnal layer only : the attention is shown to be averaged only over the vectors inputted to the ﬁnal layer, as attention is unreliable for indicating importance over input tokens (Serrano & Smith, 2019; Jain & Wallace, 8Published as a conference paper at ICLR 2022 (a) Predict:positive  (b) Predict:negative  (c) Predict:positive  (d) Predict:positive Figure 4: Visualized attention weight maps of the ﬁnal layer in the LM for the case study with BUG attack. Each row represents the time step at which the token (labeled on the left) is inputted. Each column in the row illustrates the attention weight assigned to the speciﬁc token (labeled on the top). (a): the original input by the original preﬁx-tuning; (b): the BUG-perturbed input by the original preﬁx-tuning. Compared with (a), the attention weights are perplexed in (b) due to the perturbed input. (c) and (d): the BUG-perturbed input by our robust preﬁx-tuning with (c) N = 24 and (d) N = 3. Our robust preﬁx-tuning (with both N = 24 and N = 3) steers the behavior of the ﬁnal layer to average the attention over the input vectors of the ﬁnal layer (not the input tokens). 2019; Wiegreffe & Pinter, 2019; Abnar & Zuidema, 2020). Table 5 shows an example of BUG- perturbed input, and Figure 4 shows the attention map visualization. Similar behavior is observed as well by other in-sentence attacks; more visualization can be found in Appendix F.1. 5.2 U NIVERSAL ADVERSARIAL TRIGGERS : I GNORING THE DISTRACTION In this subsection, we leverage an alternative proxy based on the product of gradient and attention as token importance rankings (detailed in Appendix F.2.1). While the proxy is not a gold-standard indicator of importance value either, it can provide more reliable predictions of token importance orderings (Serrano & Smith, 2019). We use UAT to attack SST-2 development set, and Table 5 shows an example of input with triggers “lifts mates who” with visualization in Figure 5. As a result, the (a) Predict:negative  (b) Predict:positive  (c) Predict:positive  (d) Predict:negative Figure 5: Importance visualization for the UAT case study. (a): the original input by the original preﬁx-tuning; (b): the UAT-attacked input by the original preﬁx-tuning. Compared with (a), the trigger tokens in (b) attract major importance. (c) and (d): the UAT-attacked input by our robust preﬁx-tuning with (c) N = 24 and (d) N = 3 . For N = 3 , the LM is steered to ignore the distraction of the trigger tokens and assign high importance to “ﬁller” at the time step of token “.”. LM is steered toignore the distractionfrom the trigger and assign the highest importance to the most essential token as the baseline does with clean inputs. The ﬁndings are also supported by quantitative studies. By designing metrics of corpus-level Degree of Distraction (cDoD) and Recognition of the Essential token (cRoE), we ﬁnd that the scores of our framework with N = 3 are better than those of other methods with statistical signiﬁcance (p< 0.001), which also explains the results in Figure 3. We attach more visualization and details of the quantitative studies in Appendices F.2.2 and F.2.3. 6 C ONCLUSION AND FUTURE WORK In this work, we propose robust preﬁx-tuning for text classiﬁcation that maintains only one extra amount of embedding parameters of preﬁx-tuning while achieving substantial robustness improve- ment. We conduct both quantitative and qualitative studies to analyze the behavior of our framework. We also interpret preﬁx-tuning and our framework from the optimal control perspective. Future work includes constructing better canonical manifolds and extending our framework to text generation. 9Published as a conference paper at ICLR 2022 ETHICS STATEMENT Large-scaled pretrained language models have been criticized in terms of bias and stereotypes (Zhao et al., 2019; Bender et al., 2021; Field et al., 2021), and can be steered to generate malicious outputs with universal adversarial triggers (Wallace et al., 2019). The lack of robustness leads to more uncertain behavior of LM, especially under text attacks. In our work, we contribute to improving the robustness of LM with the robust preﬁx-tuning framework. Figure 11 illustrates an example of LM fooled to predict a gender-biased sentence as positive. With our framework, the prediction of the LM is rectiﬁed. The huge resource consumption of training large-scaled LMs has also led to sustainability concerns (Strubell et al., 2019). As speciﬁed in Appendix B and Table 11, our framework has preserved the efﬁciency and modularity of preﬁx-tuning by requiring signiﬁcantly less time than all baseline methods and keeping the pretrained models unmodiﬁed. REPRODUCIBILITY STATEMENT For all of our experiments, we have listed the detailed settings in Appendices B and C.1. For the theory part, we have provided our detailed formulation as well as a brief introduction to the optimal control theory in Appendix G. We have also provided the statistics of the datasets used, as well as the time and storage taken by all methods in Tables 10 and 11 and Appendix B, respectively. ACKNOWLEDGEMENTS This work was supported by the National Key R&D Program of China (No. 2018YFB1005103), National Natural Science Foundation of China (No.61925601), and Huawei Noah’s Ark Lab. We thank all anonymous reviewers for their valuable comments and suggestions on this work. REFERENCES Samira Abnar and Willem H. Zuidema. Quantifying Attention Flow in Transformers. In Dan Ju- rafsky, Joyce Chai, Natalie Schluter, and Joel R. Tetreault (eds.), Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pp. 4190–4197. Association for Computational Linguistics, 2020. doi: 10.18653/v1/2020.acl-main. 385. URL https://doi.org/10.18653/v1/2020.acl-main.385. Moustafa Alzantot, Yash Sharma, Ahmed Elgohary, Bo-Jhang Ho, Mani Srivastava, and Kai-Wei Chang. Generating Natural Language Adversarial Examples. In Proceedings of the 2018 Con- ference on Empirical Methods in Natural Language Processing , pp. 2890–2896, Brussels, Bel- gium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/ D18-1316. URL https://aclanthology.org/D18-1316. Ahmadreza Azizi, Ibrahim Asadullah Tahmid, Asim Waheed, Neal Mangaokar, Jiameng Pu, Mobin Javed, Chandan K. Reddy, and Bimal Viswanath. T-Miner: A Generative Approach to De- fend Against Trojan Attacks on DNN-based Text Classiﬁcation. In Michael Bailey and Rachel Greenstadt (eds.), 30th USENIX Security Symposium, USENIX Security 2021, August 11-13, 2021, pp. 2255–2272. USENIX Association, 2021. URL https://www.usenix.org/ conference/usenixsecurity21/presentation/azizi. Emily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, FAccT ’21, pp. 610–623, New York, NY , USA, 2021. Association for Computing Machinery. ISBN 9781450383097. doi: 10. 1145/3442188.3445922. URL https://doi.org/10.1145/3442188.3445922. Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, Erik Brynjolfsson, Shyamal Buch, Dallas Card, Rodrigo Castellon, Niladri Chatterji, Annie S. Chen, Kathleen Creel, Jared Quincy Davis, Dorottya Demszky, Chris Donahue, Moussa Doumbouya, Esin Durmus, Ste- fano Ermon, John Etchemendy, Kawin Ethayarajh, Li Fei-Fei, Chelsea Finn, Trevor Gale, Lauren Gillespie, Karan Goel, Noah D. Goodman, Shelby Grossman, Neel Guha, Tatsunori Hashimoto, Peter Henderson, John Hewitt, Daniel E. Ho, Jenny Hong, Kyle Hsu, Jing Huang, Thomas Icard, 10Published as a conference paper at ICLR 2022 Saahil Jain, Dan Jurafsky, Pratyusha Kalluri, Siddharth Karamcheti, Geoff Keeling, Fereshte Khani, Omar Khattab, Pang Wei Koh, Mark S. Krass, Ranjay Krishna, Rohith Kuditipudi, and et al. On the Opportunities and Risks of Foundation Models. CoRR, abs/2108.07258, 2021. URL https://arxiv.org/abs/2108.07258. Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. A Large An- notated Corpus for Learning Natural Language Inference. In Proceedings of the 2015 Confer- ence on Empirical Methods in Natural Language Processing , pp. 632–642, Lisbon, Portugal, September 2015. Association for Computational Linguistics. doi: 10.18653/v1/D15-1075. URL https://aclanthology.org/D15-1075. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhari- wal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agar- wal, Ariel Herbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Ma- teusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCan- dlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language Models are Few-Shot Learners. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Ad- vances in Neural Information Processing Systems , volume 33, pp. 1877–1901. Curran Asso- ciates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/ 1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf. Zhuotong Chen, Qianxiao Li, and Zheng Zhang. Towards Robust Neural Networks via Close-loop Control. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. URLhttps://openreview.net/forum? id=2AL06y9cDE-. Jaemin Cho, Jie Lei, Hao Tan, and Mohit Bansal. Unifying Vision-and-Language Tasks via Text Generation. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Con- ference on Machine Learning , volume 139 of Proceedings of Machine Learning Research , pp. 1931–1942. PMLR, 18–24 Jul 2021. URL https://proceedings.mlr.press/v139/ cho21a.html. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of the 2019 Confer- ence of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 4171–4186, Minneapolis, Min- nesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https://aclanthology.org/N19-1423. Xinshuai Dong, Anh Tuan Luu, Rongrong Ji, and Hong Liu. Towards Robustness Against Natural Language Word Substitutions. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021 . OpenReview.net, 2021. URL https:// openreview.net/forum?id=ks5nebunVn_. Javid Ebrahimi, Anyi Rao, Daniel Lowd, and Dejing Dou. HotFlip: White-Box Adversarial Examples for Text Classiﬁcation. In Proceedings of the 56th Annual Meeting of the Associ- ation for Computational Linguistics (Volume 2: Short Papers) , pp. 31–36, Melbourne, Aus- tralia, July 2018. Association for Computational Linguistics. doi: 10.18653/v1/P18-2006. URL https://aclanthology.org/P18-2006. Steffen Eger, G ¨ozde G ¨ul S ¸ahin, Andreas R¨uckl´e, Ji-Ung Lee, Claudia Schulz, Mohsen Mesgar, Krishnkant Swarnkar, Edwin Simpson, and Iryna Gurevych. Text Processing Like Humans Do: Visually Attacking and Shielding NLP Systems. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 1634–1647, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1165. URL https: //aclanthology.org/N19-1165. Anjalie Field, Su Lin Blodgett, Zeerak Waseem, and Yulia Tsvetkov. A Survey of Race, Racism, and Anti-Racism in NLP. In Proceedings of the 59th Annual Meeting of the Association for Compu- tational Linguistics and the 11th International Joint Conference on Natural Language Processing 11Published as a conference paper at ICLR 2022 (Volume 1: Long Papers), pp. 1905–1925, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.149. URL https://aclanthology.org/ 2021.acl-long.149. Tianyu Gao, Adam Fisch, and Danqi Chen. Making Pre-trained Language Models Better Few-shot Learners. In Proceedings of the 59th Annual Meeting of the Association for Computational Lin- guistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 3816–3830, Online, August 2021. Association for Computational Linguis- tics. doi: 10.18653/v1/2021.acl-long.295. URL https://aclanthology.org/2021. acl-long.295. Siddhant Garg and Goutham Ramakrishnan. BAE: BERT-based Adversarial Examples for Text Classiﬁcation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 6174–6181, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.498. URL https://aclanthology.org/ 2020.emnlp-main.498. Karen Hambardzumyan, Hrant Khachatrian, and Jonathan May. W ARP: Word-level Adversarial ReProgramming. In Proceedings of the 59th Annual Meeting of the Association for Computa- tional Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 4921–4933, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.381. URL https://aclanthology.org/ 2021.acl-long.381. Charles R. Harris, K. Jarrod Millman, St ´efan J. van der Walt, Ralf Gommers, Pauli Virtanen, David Cournapeau, Eric Wieser, Julian Taylor, Sebastian Berg, Nathaniel J. Smith, Robert Kern, Matti Picus, Stephan Hoyer, Marten H. van Kerkwijk, Matthew Brett, Allan Haldane, Jaime Fern´andez del R´ıo, Mark Wiebe, Pearu Peterson, Pierre G´erard-Marchant, Kevin Sheppard, Tyler Reddy, Warren Weckesser, Hameer Abbasi, Christoph Gohlke, and Travis E. Oliphant. Ar- ray programming with NumPy. Nature, 585(7825):357–362, September 2020. doi: 10.1038/ s41586-020-2649-2. URL https://doi.org/10.1038/s41586-020-2649-2 . Xuanli He, Lingjuan Lyu, Lichao Sun, and Qiongkai Xu. Model Extraction and Adversarial Trans- ferability, Your BERT is Vulnerable! In Proceedings of the 2021 Conference of the North Amer- ican Chapter of the Association for Computational Linguistics: Human Language Technologies , pp. 2006–2012, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/ 2021.naacl-main.161. URL https://aclanthology.org/2021.naacl-main.161. Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, An- drea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-Efﬁcient Transfer Learning for NLP. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th In- ternational Conference on Machine Learning , volume 97 of Proceedings of Machine Learning Research, pp. 2790–2799. PMLR, 09–15 Jun 2019. URL https://proceedings.mlr. press/v97/houlsby19a.html. Shengding Hu, Ning Ding, Huadong Wang, Zhiyuan Liu, Juanzi Li, and Maosong Sun. Knowledge- able Prompt-tuning: Incorporating Knowledge into Prompt Verbalizer for Text Classiﬁcation. CoRR, abs/2108.02035, 2021. URL https://arxiv.org/abs/2108.02035. Po-Sen Huang, Robert Stanforth, Johannes Welbl, Chris Dyer, Dani Yogatama, Sven Gowal, Kr- ishnamurthy Dvijotham, and Pushmeet Kohli. Achieving Veriﬁed Robustness to Symbol Sub- stitutions via Interval Bound Propagation. In Proceedings of the 2019 Conference on Empir- ical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP) , pp. 4083–4093, Hong Kong, China, Novem- ber 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1419. URL https://aclanthology.org/D19-1419. Mohit Iyyer, John Wieting, Kevin Gimpel, and Luke Zettlemoyer. Adversarial Example Gener- ation with Syntactically Controlled Paraphrase Networks. In Proceedings of the 2018 Con- ference of the North American Chapter of the Association for Computational Linguistics: Hu- man Language Technologies, Volume 1 (Long Papers), pp. 1875–1885, New Orleans, Louisiana, 12Published as a conference paper at ICLR 2022 June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-1170. URL https://aclanthology.org/N18-1170. Sarthak Jain and Byron C. Wallace. Attention is not Explanation. In Jill Burstein, Christy Doran, and Thamar Solorio (eds.), Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers) , pp. 3543–3556. Association for Computational Linguistics, 2019. doi: 10.18653/v1/n19-1357. URL https: //doi.org/10.18653/v1/n19-1357. Robin Jia, Aditi Raghunathan, Kerem G¨oksel, and Percy Liang. Certiﬁed Robustness to Adversarial Word Substitutions. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Process- ing (EMNLP-IJCNLP) , pp. 4129–4142, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1423. URL https://aclanthology. org/D19-1423. Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, and Tuo Zhao. SMART: Robust and Efﬁcient Fine-Tuning for Pre-trained Natural Language Models through Principled Regularized Optimization. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 2177–2190, Online, July 2020a. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.197. URL https://aclanthology.org/ 2020.acl-main.197. Zhengbao Jiang, Frank F. Xu, Jun Araki, and Graham Neubig. How Can We Know What Lan- guage Models Know? Transactions of the Association for Computational Linguistics, 8:423–438, 2020b. doi: 10.1162/tacl a 00324. URL https://aclanthology.org/2020.tacl-1. 28. Erik Jones, Robin Jia, Aditi Raghunathan, and Percy Liang. Robust Encodings: A Framework for Combating Adversarial Typos. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 2752–2765, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.245. URL https://aclanthology.org/ 2020.acl-main.245. Marc Khoury and Dylan Hadﬁeld-Menell. On the Geometry of Adversarial Examples. CoRR, abs/1811.00525, 2018. URL http://arxiv.org/abs/1811.00525. Diederik P. Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. In Yoshua Bengio and Yann LeCun (eds.),3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http: //arxiv.org/abs/1412.6980. Philipp Koehn. Statistical Signiﬁcance Tests for Machine Translation Evaluation. In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing , EMNLP 2004, A meeting of SIGDAT, a Special Interest Group of the ACL, held in conjunction with ACL 2004, 25- 26 July 2004, Barcelona, Spain, pp. 388–395. ACL, 2004. URL https://aclanthology. org/W04-3250/. Richard E Kopp. Pontryagin maximum principle. In Mathematics in Science and Engineering , volume 5, pp. 255–279. Elsevier, 1962. doi: 10.1016/S0076-5392(08)62095-0. Thai Le, Noseong Park, and Dongwon Lee. A Sweet Rabbit Hole by DARCY: Using Honeypots to Detect Universal Trigger’s Adversarial Attacks. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (eds.), Proceedings of the 59th Annual Meeting of the Association for Computa- tional Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021 , pp. 3831–3844. Association for Computational Linguistics, 2021. doi: 10.18653/v1/2021.acl-long.296. URL https://doi.org/10.18653/v1/2021.acl-long.296. Brian Lester, Rami Al-Rfou, and Noah Constant. The Power of Scale for Parameter-Efﬁcient Prompt Tuning. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih (eds.), 13Published as a conference paper at ICLR 2022 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021, pp. 3045– 3059. Association for Computational Linguistics, 2021. URL https://aclanthology. org/2021.emnlp-main.243. Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. BART: Denoising Sequence-to-Sequence Pre- training for Natural Language Generation, Translation, and Comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pp. 7871–7880, On- line, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.703. URL https://aclanthology.org/2020.acl-main.703. Alexander Hanbo Li and Abhinav Sethy. Knowledge Enhanced Attention for Robust Natural Lan- guage Inference. CoRR, abs/1909.00102, 2019. URL http://arxiv.org/abs/1909. 00102. Jinfeng Li, Shouling Ji, Tianyu Du, Bo Li, and Ting Wang. TextBugger: Generating Adversar- ial Text Against Real-world Applications. In 26th Annual Network and Distributed System Security Symposium, NDSS 2019, San Diego, California, USA, February 24-27, 2019 . The Internet Society, 2019. URL https://www.ndss-symposium.org/ndss-paper/ textbugger-generating-adversarial-text-against-real-world-applications/ . Qianxiao Li, Long Chen, Cheng Tai, and Weinan E. Maximum Principle Based Algorithms for Deep Learning. J. Mach. Learn. Res., 18:165:1–165:29, 2017. URL http://jmlr.org/papers/ v18/17-653.html. Xiang Lisa Li and Percy Liang. Preﬁx-Tuning: Optimizing Continuous Prompts for Generation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 4582–4597, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/ v1/2021.acl-long.353. URL https://aclanthology.org/2021.acl-long.353. Zongyi Li, Jianhan Xu, Jiehang Zeng, Linyang Li, Xiaoqing Zheng, Qi Zhang, Kai-Wei Chang, and Cho-Jui Hsieh. Searching for an Effective Defender: Benchmarking Defense against Ad- versarial Word Substitution. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih (eds.), Proceedings of the 2021 Conference on Empirical Methods in Nat- ural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021, pp. 3137–3147. Association for Computational Linguistics, 2021. URL https://aclanthology.org/2021.emnlp-main.251. Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. Pre- train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing. CoRR, abs/2107.13586, 2021. URL https://arxiv.org/abs/2107.13586. Xiaodong Liu, Hao Cheng, Pengcheng He, Weizhu Chen, Yu Wang, Hoifung Poon, and Jianfeng Gao. Adversarial Training for Large Neural Language Models. CoRR, abs/2004.08994, 2020. URL https://arxiv.org/abs/2004.08994. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A Robustly Optimized BERT Pre- training Approach. CoRR, abs/1907.11692, 2019. URL http://arxiv.org/abs/1907. 11692. Ilya Loshchilov and Frank Hutter. Decoupled Weight Decay Regularization. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019 . OpenReview.net, 2019. URL https://openreview.net/forum?id=Bkg6RiCqY7. Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards Deep Learning Models Resistant to Adversarial Attacks. In 6th International Confer- ence on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018. URL https://openreview.net/ forum?id=rJzIBfZAb. 14Published as a conference paper at ICLR 2022 Benjamin Marie, Atsushi Fujita, and Raphael Rubino. Scientiﬁc Credibility of Machine Transla- tion Research: A Meta-Evaluation of 769 Papers. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (eds.), Proceedings of the 59th Annual Meeting of the Association for Computa- tional Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021 , pp. 7297–7306. Association for Computational Linguistics, 2021. doi: 10.18653/v1/2021.acl-long.566. URL https://doi.org/10.18653/v1/2021.acl-long.566. Bryan McCann, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher. The Natural Language Decathlon: Multitask Learning as Question Answering. CoRR, abs/1806.08730, 2018. URL http://arxiv.org/abs/1806.08730. Takeru Miyato, Andrew M. Dai, and Ian J. Goodfellow. Adversarial Training Methods for Semi- Supervised Text Classiﬁcation. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017. URL https://openreview.net/forum?id=r1X3g2_xl. Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, and Shin Ishii. Virtual Adversarial Training: A Regularization Method for Supervised and Semi-Supervised Learning. IEEE Trans. Pattern Anal. Mach. Intell., 41(8):1979–1993, 2019. doi: 10.1109/TPAMI.2018.2858821. URL https: //doi.org/10.1109/TPAMI.2018.2858821. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas K¨opf, Edward Z. Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. PyTorch: An Imperative Style, High-Performance Deep Learning Library. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d’Alch ´e-Buc, Emily B. Fox, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Pro- cessing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada , pp. 8024–8035, 2019. URL https://proceedings.neurips.cc/paper/2019/hash/ bdbca288fee7f92f2bfa9f7012727740-Abstract.html. Karl Pearson. LIII. On lines and planes of closest ﬁt to systems of points in space. The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science , 2(11):559–572, 1901. doi: 10.1080/14786440109462720. Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. Deep Contextualized Word Representations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguis- tics: Human Language Technologies, Volume 1 (Long Papers) , pp. 2227–2237, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-1202. URL https://aclanthology.org/N18-1202. Danish Pruthi, Bhuwan Dhingra, and Zachary C. Lipton. Combating Adversarial Misspellings with Robust Word Recognition. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 5582–5591, Florence, Italy, July 2019. Association for Compu- tational Linguistics. doi: 10.18653/v1/P19-1561. URL https://aclanthology.org/ P19-1561. Raul Puri and Bryan Catanzaro. Zero-shot Text Classiﬁcation With Generative Language Models. CoRR, abs/1912.10165, 2019. URL http://arxiv.org/abs/1912.10165. Guanghui Qin and Jason Eisner. Learning How to Ask: Querying LMs with Mixtures of Soft Prompts. In Proceedings of the 2021 Conference of the North American Chapter of the Asso- ciation for Computational Linguistics: Human Language Technologies , pp. 5203–5212, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.410. URL https://aclanthology.org/2021.naacl-main.410. Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language Models are Unsupervised Multitask Learners. 2019. URL https://openai.com/blog/ better-language-models/. 15Published as a conference paper at ICLR 2022 Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the Limits of Transfer Learning with a Uniﬁed Text- to-Text Transformer. Journal of Machine Learning Research, 21(140):1–67, 2020. URL http: //jmlr.org/papers/v21/20-074.html. Shuhuai Ren, Yihe Deng, Kun He, and Wanxiang Che. Generating Natural Language Adver- sarial Examples through Probability Weighted Word Saliency. In Proceedings of the 57th An- nual Meeting of the Association for Computational Linguistics , pp. 1085–1097, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1103. URL https://aclanthology.org/P19-1103. Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. Semantically Equivalent Adversarial Rules for Debugging NLP models. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 856–865, Melbourne, Australia, July 2018. Association for Computational Linguistics. doi: 10.18653/v1/P18-1079. URL https: //aclanthology.org/P18-1079. Soﬁa Serrano and Noah A. Smith. Is Attention Interpretable? In Anna Korhonen, David R. Traum, and Llu´ıs M`arquez (eds.), Proceedings of the 57th Conference of the Association for Computa- tional Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers , pp. 2931–2951. Association for Computational Linguistics, 2019. doi: 10.18653/v1/p19-1282. URL https://doi.org/10.18653/v1/p19-1282. Zhouxing Shi, Huan Zhang, Kai-Wei Chang, Minlie Huang, and Cho-Jui Hsieh. Robustness Veriﬁ- cation for Transformers. In International Conference on Learning Representations, 2020. Taylor Shin, Yasaman Razeghi, Robert L. Logan IV , Eric Wallace, and Sameer Singh. Auto- Prompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Process- ing (EMNLP), pp. 4222–4235, Online, November 2020. Association for Computational Linguis- tics. doi: 10.18653/v1/2020.emnlp-main.346. URL https://aclanthology.org/2020. emnlp-main.346. Chenglei Si, Zhengyan Zhang, Fanchao Qi, Zhiyuan Liu, Yasheng Wang, Qun Liu, and Maosong Sun. Better Robustness by More Coverage: Adversarial and Mixup Data Augmentation for Ro- bust Finetuning. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (eds.), Find- ings of the Association for Computational Linguistics: ACL/IJCNLP 2021, Online Event, Au- gust 1-6, 2021 , volume ACL/IJCNLP 2021 of Findings of ACL , pp. 1569–1576. Association for Computational Linguistics, 2021. doi: 10.18653/v1/2021.ﬁndings-acl.137. URL https: //doi.org/10.18653/v1/2021.findings-acl.137. Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pp. 1631–1642, Seattle, Washington, USA, October 2013. Association for Computa- tional Linguistics. URL https://aclanthology.org/D13-1170. Emma Strubell, Ananya Ganesh, and Andrew McCallum. Energy and Policy Considerations for Deep Learning in NLP. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 3645–3650, Florence, Italy, July 2019. Association for Compu- tational Linguistics. doi: 10.18653/v1/P19-1355. URL https://aclanthology.org/ P19-1355. Maria Tsimpoukelli, Jacob Menick, Serkan Cabi, S. M. Ali Eslami, Oriol Vinyals, and Felix Hill. Multimodal Few-Shot Learning with Frozen Language Models. CoRR, abs/2106.13884, 2021. URL https://arxiv.org/abs/2106.13884. Elena V oita, Rico Sennrich, and Ivan Titov. The Bottom-up Evolution of Representations in the Transformer: A Study with Machine Translation and Language Modeling Objectives. In Pro- ceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP) , pp. 4396–4406, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1448. URL https://aclanthology.org/D19-1448. 16Published as a conference paper at ICLR 2022 Eric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner, and Sameer Singh. Universal Adversar- ial Triggers for Attacking and Analyzing NLP. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Confer- ence on Natural Language Processing (EMNLP-IJCNLP) , pp. 2153–2162, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1221. URL https://aclanthology.org/D19-1221. Boxin Wang, Shuohang Wang, Yu Cheng, Zhe Gan, Ruoxi Jia, Bo Li, and Jingjing Liu. InfoBERT: Improving Robustness of Language Models from An Information Theoretic Perspective. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021 . OpenReview.net, 2021. URL https://openreview.net/forum?id= hpH98mK5Puk. Sarah Wiegreffe and Yuval Pinter. Attention is not not Explanation. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan (eds.), Proceedings of the 2019 Conference on Empirical Meth- ods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019 , pp. 11–20. Association for Computational Linguistics, 2019. doi: 10.18653/v1/D19-1002. URL https://doi.org/10.18653/v1/D19-1002. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R ´emi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the-Art Nat- ural Language Processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations , pp. 38–45, Online, October 2020. As- sociation for Computational Linguistics. URL https://www.aclweb.org/anthology/ 2020.emnlp-demos.6. Kaidi Xu, Zhouxing Shi, Huan Zhang, Yihan Wang, Kai-Wei Chang, Minlie Huang, Bhavya Kailkhura, Xue Lin, and Cho-Jui Hsieh. Automatic Perturbation Analysis for Scalable Certi- ﬁed Robustness and Beyond. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems , volume 33, pp. 1129–1141. Cur- ran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/ file/0cbc5671ae26f67871cb914d81ef8fc1-Paper.pdf. Ying Xu, Xu Zhong, Antonio Jimeno Yepes, and Jey Han Lau. Grey-box Adversarial Attack And Defence For Sentiment Classiﬁcation. In Proceedings of the 2021 Conference of the North Amer- ican Chapter of the Association for Computational Linguistics: Human Language Technologies , pp. 4078–4087, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/ 2021.naacl-main.321. URL https://aclanthology.org/2021.naacl-main.321. Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies , pp. 483–498, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.41. URLhttps: //aclanthology.org/2021.naacl-main.41. Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le. XLNet: Generalized Autoregressive Pretraining for Language Understanding. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch ´e-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems , volume 32. Curran Asso- ciates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/ dc6a7e655d7e5840e66733e9ee67cc69-Paper.pdf. Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He, Alexander J. Smola, and Eduard H. Hovy. Hi- erarchical Attention Networks for Document Classiﬁcation. In Kevin Knight, Ani Nenkova, and Owen Rambow (eds.),NAACL HLT 2016, The 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, San Diego Cal- ifornia, USA, June 12-17, 2016, pp. 1480–1489. The Association for Computational Linguistics, 2016. doi: 10.18653/v1/n16-1174. URL https://doi.org/10.18653/v1/n16-1174. 17Published as a conference paper at ICLR 2022 Mao Ye, Chengyue Gong, and Qiang Liu. SAFER: A Structure-free Approach for Certiﬁed Robustness to Adversarial Word Substitutions. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pp. 3465–3475, Online, July 2020. Asso- ciation for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.317. URL https: //aclanthology.org/2020.acl-main.317. Guoyang Zeng, Fanchao Qi, Qianrui Zhou, Tingji Zhang, Zixian Ma, Bairu Hou, Yuan Zang, Zhiyuan Liu, and Maosong Sun. OpenAttack: An Open-source Textual Adversarial Attack Toolkit. In Proceedings of the 59th Annual Meeting of the Association for Computational Lin- guistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations, pp. 363–371, Online, August 2021. Association for Computational Linguis- tics. doi: 10.18653/v1/2021.acl-demo.43. URL https://aclanthology.org/2021. acl-demo.43. Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric P. Xing, Laurent El Ghaoui, and Michael I. Jordan. Theoretically Principled Trade-off between Robustness and Accuracy. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, volume 97 of Proceedings of Machine Learning Research, pp. 7472–7482. PMLR, 2019. URL http://proceedings. mlr.press/v97/zhang19p.html. Wei Emma Zhang, Quan Z. Sheng, Ahoud Alhazmi, and Chenliang Li. Adversarial Attacks on Deep-Learning Models in Natural Language Processing: A Survey. ACM Trans. Intell. Syst. Technol., 11(3), April 2020. ISSN 2157-6904. doi: 10.1145/3374217. URL https://doi. org/10.1145/3374217. Xiang Zhang, Junbo Zhao, and Yann LeCun. Character-level Convolutional Networks for Text Classiﬁcation. In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Gar- nett (eds.), Advances in Neural Information Processing Systems , volume 28. Curran Asso- ciates, Inc., 2015. URL https://proceedings.neurips.cc/paper/2015/file/ 250cf8b51c773f3f8dc8b4be867a9a02-Paper.pdf. Jieyu Zhao, Tianlu Wang, Mark Yatskar, Ryan Cotterell, Vicente Ordonez, and Kai-Wei Chang. Gender Bias in Contextualized Word Embeddings. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) , pp. 629–634, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1064. URL https: //aclanthology.org/N19-1064. Zexuan Zhong, Dan Friedman, and Danqi Chen. Factual Probing Is [MASK]: Learning vs. Learning to Recall. In Proceedings of the 2021 Conference of the North American Chapter of the Asso- ciation for Computational Linguistics: Human Language Technologies , pp. 5017–5033, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.398. URL https://aclanthology.org/2021.naacl-main.398. Yi Zhou, Xiaoqing Zheng, Cho-Jui Hsieh, Kai-Wei Chang, and Xuanjing Huang. Defense against Synonym Substitution-based Adversarial Attacks via Dirichlet Neighborhood Ensemble. In Pro- ceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 5482–5492, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/ v1/2021.acl-long.426. URL https://aclanthology.org/2021.acl-long.426. Yichao Zhou, Jyun-Yu Jiang, Kai-Wei Chang, and Wei Wang. Learning to Discriminate Perturba- tions for Blocking Adversarial Attacks in Text Classiﬁcation. In Proceedings of the 2019 Con- ference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP) , pp. 4904–4913, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1496. URL https://aclanthology.org/D19-1496. Chen Zhu, Yu Cheng, Zhe Gan, Siqi Sun, Tom Goldstein, and Jingjing Liu. FreeLB: Enhanced Adversarial Training for Natural Language Understanding. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020 . OpenRe- view.net, 2020. URL https://openreview.net/forum?id=BygzbyHFvB. 18Published as a conference paper at ICLR 2022 A C OMPARISON WITH REGULAR FINETUNING In this section, we provide comparison between our work with both standard and adversarial regular ﬁnetuning baselines on SST-2. For regular ﬁnetuning experiments, we use the 345M GPT2-medium LM, same as the LM adopted in the preﬁx-tuning experiments. We trained for 9 epochs for stan- dard regular ﬁnetuning and 3 epochs for adversarial regular ﬁnetuning (earlystopping). The initial learning rate is also set as 5e-5 for both standard and adversarial regular ﬁnetuning. The settings of perturbation range ϵ, step size αand iteration of the inner maximization M is the same as the those adopted in adversarial preﬁx-tuning experiments. The results are listed in Table 6. Table 6: Performance of our robust preﬁx-tuning with regular ﬁnetuning baselines for comparison. Method #Epochs Clean PWWS VIPER SCPN BUG UAT std. regular ﬁnetuning 9 93.68 24.77 4.61 30.15 17.74 23.89 std. preﬁx-tuning 100 92.48 16.64 1.92 31.58 8.84 5.05 + our framework - 92.59 50.36 44.65 58.54 46.68 85.72 adv. regular ﬁnetuning 3 93.63 48.38 33.44 48.27 44.59 14.17 adv. preﬁx-tuning 20 89.51 32.02 17.35 43.33 27.38 8.57 + our framework - 89.57 53.93 48.38 57.88 49.70 73.97 adv. preﬁx-tuning 100 93.74 30.53 8.18 33.11 27.51 8.95 + our framework - 93.79 57.55 43.60 60.68 57.17 91.05 Due to time limit, we have only conducted regular ﬁnetuning experiments on SST-2 with one adver- sarial training baseline (Miyato et al., 2017) for comparison. The aim of the comparison, however, is neither to set a new SOTAnor to demonstrate that our robust preﬁx-tuning framework has beaten the robust ﬁnetuning techniques. On the contrary, we aim to demonstrate several special properties in the scenario of preﬁx-tuning, as well as challenges and opportunities in robustness: • Difﬁculty in optimization. It can be seen that preﬁx-tuning requires more epochs to con- verge. A potential explanation is that the loss landscape of the preﬁx parameters is highly non-convex, as the downstream task should be learned with far fewer amount of free pa- rameters. The difﬁculty in optimization might have also brought more challenges on ro- bustness to preﬁx-tuning. According to Table 6, all preﬁx-tuning baselines underperform the corresponded regular ﬁnetuning approaches against almost all types of attacks. • Trade-off between space and time. It is worthy to be mentioned that for each preﬁx-tuning experiment, the saved preﬁx parameters consume disk storage of only 2MB. In contrast, each regular ﬁnetuning experiment takes 725MB disk storage. However, preﬁx-tuning also takes more epochs (and thus longer time) to converge. This brings even more challenges in robustifying preﬁx-tuning, as an ideal solution should be neither harmful to the beneﬁts of preﬁx-tuning (lightweightness, modularity, not modifying the LM parameters) nor too slow for fear that the time complexity of preﬁx-tuning further deteriorates. Our framework serves as a possible solution that keeps the strengths without weakening the weakness. • Lack of robustness against UAT. According to Table 6, both adversarial regular ﬁnetuning and adversarial preﬁx-tuning fail to defend against UAT. A possible explanation is that the adversaries induced during adversarial training are more similar to in-sentence attacks, yet few inductive bias has been introduced against UAT. Existing defense approaches against UAT maintain additional adversary detectors. For example, DARCY (Le et al., 2021) searches for the potential triggers ﬁrst, and then retrain a classiﬁer using the exploited trig- gers as a UAT adversary detector; T-Miner (Azizi et al., 2021) leverages Seq2Seq model to probe the hidden representation of the suspicious classiﬁer into a synthetic text sequence that is likely to contain adversarial triggers. In addition, neither of the methods have been tested on large-scale pretrained LM. To the best of our knowledge, our framework is the ﬁrst solution to defend against UAT by tuning an additional preﬁx based on preﬁx-tuning for pretrained LM. Positioned before the adversarial triggers, the robust preﬁx regulates the activation at the output position so that “the distraction can be ignored”. 19Published as a conference paper at ICLR 2022 B D ETAILS OF EXPERIMENTAL SETTINGS We use the SST-2, AG’s News, and SNLI benchmark for our text classiﬁcation experiments. We use the 345M GPT2-medium (725MB storage) as the large-scale pretrained LM and implement preﬁx- tuning with preﬁx length of 10. We initialize the preﬁx token embeddings with parameters from the LM word embedding matrix to ease the optimization. We train100 epochs for SST-2 and25 epochs for AG’s News and SNLI. We use the AdamW optimizer (Loshchilov & Hutter, 2019) provided by the HuggingFace Transformers library (Wolf et al., 2020) to optimize the preﬁx with initial learning rate as 5e-5 in all experiments. Other settings of preﬁx-tuning follows Li & Liang (2021). During inference in our framework, unless speciﬁed (Section 4.5 and Appendix E), we use an adaptive test batch size with dynamic padding to make full use of GPU memory. The total number of tokens in a test batch is determined by the setting of the ratio of GPU memory (0.12 for 24GB NVIDIA-3090 GPUs) allocated for loading data. In contrast, the number of sentences in a test batch is ﬁxed in experiments in Section 4.5 and Appendix E. The size of obtained preﬁx vectors are 1.9MB for all tasks. We also save the projection matrices of the layerwise canonical manifolds, but only the matrices of the bottom N = 3 canonical manifolds need to be saved as we tune the bottomN = 3 layers in our robust preﬁx-tuning. The saved matrices take 12MB storage and can be used for all types of text attacks. We use the Adam optimizer (Kingma & Ba, 2015) to tune the additional preﬁx P′ ψ[i,:]. The initial learning rate and the number of steps (5 or 10) for its optimization are tuned on the development set. We use the following ﬁve text attack approaches in our experiments: • PWWS (Ren et al., 2019), a score-based word-level text attack that fools a text classiﬁer by applying greedy word substitution on inputs. • VIPER (Eger et al., 2019), a blind character-level text attack that applies visually similar character substitution on inputs. • SCPN (Iyyer et al., 2018), a blind sentence-level text attack that paraphrases inputs. • TextBugger (Li et al., 2019), a gradient-based word- and character-level attack that applies greedy word substitution and character manipulation. We refer to it as “BUG” for short. • UAT (Wallace et al., 2019), a gradient-based word-level text attack that fools a text classiﬁer by prepending the same adversarial tokens to all test inputs. We use the OpenAttack toolkit (Zeng et al., 2021) for the in-sentence attacks (PWWS, VIPER, SCPN, TextBugger) and implement UAT by ourselves. For the in-sentence attacks, we use the default hyperparameter settings provided by OpenAttack. Following prior arts (Ren et al., 2019; Jia et al., 2019; Dong et al., 2021), we do not attack the premise on SNLI. For UAT, following Wallace et al. (2019), we search for a 3-token trigger for inputs of each class in the test set. We set beam size as 5 and iterate over the test set for5 epochs. We use NVIDIA-3090 GPUs for all of our experiments. C A DVERSARIAL TRAINING C.1 A DVERSARIAL TRAINING BASELINES We start with adversarial training for text classiﬁcation that restricts perturbation within ℓ2 balls of word or sentence level (Miyato et al., 2017). The objective is deﬁned as min θ E(x,y)∼Dtr [ max ∥r∥2≤ϵ L(y|emb(x) + r; θ) ] , (9) where ris the perturbation injected to the word embeddings of x. The inner maximization of rcan be implemented by projected gradient descent (Madry et al., 2018) with M iterative updates: r←Proj{∥r∥2≤ϵ} [ r+ α ∇emb(x)L(y|emb(x) + r; θ) ∥∇emb(x)L(y|emb(x) + r; θ)∥2 ] . (10) In all of our adversarial preﬁx-tuning experiments, we set ϵ= 5, α= 1.25, and M = 10. For texts, the ℓ2 ball can be deﬁned for both word level and sentence level. For the word level, theℓ2 balls are constructed for each token in the context part of the input by offsetting each word vector 20Published as a conference paper at ICLR 2022 Table 7: Results of different baselines with our framework applied. This table covers the results of adversarial preﬁx-tuning with both word- and sentence-levelℓ2 balls and serves as the supplementary for Table 1. Our framework consistently improves robustness of all baselines on all benchmarks. Benchmark Method Clean PWWS VIPER SCPN BUG UAT SST-2 std. preﬁx-tuning 92.48 16.64 1.92 31.58 8.84 5.05 + our framework 92.59 50.36 44.65 58.54 46.68 85.72 word-level adv. 93.74 30.53 8.18 33.11 27.51 8.95 + our framework 93.79 57.55 43.60 60.68 57.17 91.05 sent-level adv. 93.57 30.64 7.25 35.42 25.04 4.88 + our framework 93.52 53.21 39.65 54.91 50.69 88.25 AG News std. preﬁx-tuning 86.42 43.00 24.55 43.20 43.22 52.20 + our framework 86.50 53.91 24.93 48.38 51.79 76.47 word-level adv. 89.46 50.91 26.30 45.25 47.11 59.74 + our framework 90.26 56.45 31.25 48.03 54.66 87.26 sent-level adv. 90.18 49.97 24.87 44.54 46.41 38.11 + our framework 90.37 57.75 26.50 48.43 54.39 80.63 SNLI std. preﬁx-tuning 72.48 25.51 29.69 42.46 32.88 30.20 + our framework 72.74 34.68 33.64 43.37 36.59 71.03 word-level adv. 77.22 27.43 28.58 46.83 34.94 35.76 + our framework 77.65 33.88 33.98 46.92 38.21 76.15 sent-level adv. 78.99 25.75 28.26 44.54 31.94 41.77 + our framework 79.56 30.91 34.16 44.66 36.41 77.47 within the range of radius of ϵ. For the sentence level, the single ℓ2 ball is constructed for the entire context part of the input by ﬂattening the word embedding vectors and offsetting the ﬂattened vector within the range of radiusϵ. The word-level and the sentence-level preﬁx-tuning take similar training time. As a consequence, adversarial preﬁx-tuning with both levels of ℓ2 balls are far slower than standard preﬁx-tuning. The adversarial training approach used in Table 1 is of word-level ℓ2 balls. The complete results of our approach applied with both word-level and sentence-level adversarial preﬁx tuning, as well as standard preﬁx-tuning, are shown in Table 7. It can be seen that our robust preﬁx-tuning framework substantially improves robustness of different baseline methods. We have also experimented with other types of adversarial training for preﬁx-tuning, such as adding the KL-divergence term for regularization (Miyato et al., 2019; Zhang et al., 2019). The objective is min θ E(x,y)∼Dtr [ L(y|x; θ) + max ∥r∥2≤ϵ βKL [L(y|emb(x); θ) ∥L(y|emb(x) + r; θ)] ] . (11) We set β = 1 and β = 4 and train for 20 epochs for different attempts of KL-divergence regularized adversarial preﬁx-tuning with Eq. (11). The training loss and the clean accuracy of the development set on the SST-2 benchmark is shown in Figure 6. In this ﬁgure, we also visualize the word-level adversarial preﬁx-tuning method shown in Figure 2 for comparison. The KL-divergence regularized adversarial preﬁx-tuning of both settings start with very large training loss. For β = 1, the clean accuracy on the development set remains 0 for the ﬁrst 15 epochs. The accuracy raises to around 27% at the 17-th epoch, but lacks stability (back to 0 again at the 19-th epoch). For β = 4 , the clean accuracy remains 0 for all 20 epochs. In comparison, the word-level adversarial preﬁx-tuning used in Section 4.2 achieves low training loss and high clean accuracy on the development set. The training loss and the accuracy on the development set have suggested the difﬁculty in optimizing the preﬁx embedding, which might be attributed to its rather small amount of parameters compared with the whole language model. While there are other variants of adversarial training approaches for text classiﬁcation, in this work, we did not use them for our baselines as most of them (Zhu et al., 2020; Jiang et al., 2020a; Liu et al., 2020; Dong et al., 2021) are based on the KL-divergence regularizer, and also require excessively long training time. It is also of future interest on how to empower our robust preﬁx-tuning framework with faster and better adversarial preﬁx-tuning approaches. 21Published as a conference paper at ICLR 2022 Figure 6: Training loss and clean accuracy on the dev set of SST-2 with KL-divergence regularized adversarial preﬁx-tuning as well as the word-level adversarial preﬁx-tuning used in Section 4.2. C.2 A DVERSARIAL TRAINING WITH EARLYSTOPPING We apply earlystopping to both word-level and sentence-level adversarial preﬁx-tuning approaches on the SST-2 benchmark at 20 epochs. The results are shown in Table 8. According to the results, the robustness results against different types of in-sentence attacks have been slightly improved. The robustness against UAT, in contrast, slightly drops. In terms of clean accuracy, the earlystopped adversarial preﬁx-tuning is even surpassed by the standard preﬁx-tuning baseline. Our robust preﬁx- tuning framework still works on the preﬁxes obtained by the earlystopped adversarial preﬁx-tuning approaches, as the robustness results are consistently improved. Table 8: Results of earlystopping of adv. preﬁx-tuning at 20 epochs with our framework on SST-2. Method Clean PWWS VIPER SCPN BUG UAT std. preﬁx-tuning 92.48 16.64 1.92 31.58 8.84 5.05 + our framework 92.59 50.36 44.65 58.54 46.68 85.72 100-epoch word-level 93.57 30.64 7.25 35.42 25.04 4.88 + our framework 93.52 53.21 39.65 54.91 50.69 88.25 100-epoch sent-level 93.74 30.53 8.18 33.11 27.51 8.95 + our framework 93.79 57.55 43.60 60.68 57.17 91.05 20-epoch word-level 90.88 34.71 22.41 39.37 29.00 6.97 + our framework 90.88 51.18 46.46 58.26 50.74 84.95 20-epoch sent-level 89.51 32.02 17.35 43.33 27.38 8.57 + our framework 89.57 53.93 48.38 57.88 49.70 73.97 D A DVERSARIAL DATA AUGMENTATION In this section, we report additional experimental results of adversarial data augmentation using VIPER or SCPN to generate adversarial data for augmentation during training. We also set the milestones of 20, 50, and 100 epochs for training. According to Table 9, the robustness against the attack of the same type used for adversarial data augmentation during training has gained signiﬁcant improvements compared with the standard preﬁx-tuning baseline. Robustness against other types of attacks, however, fails to improve with the process of training. For our approach, we use the correctly classiﬁed data from both the clean training set and the training data perturbed by the speciﬁc type of attack used for augmentation to construct the canonical manifolds. The experimental results indicate that our robust preﬁx-tuning framework still improves the robustness against all types of text attacks over all types of baselines (except for the VIPER attack in VIPER adversarial data augmentation that incurs slight drop at the 20-epoch and 50-epoch milestones). 22Published as a conference paper at ICLR 2022 Table 9: Results of VIPER and SCPN data augmentation baselines as well as our methods on SST-2. Method Clean PWWS VIPER SCPN BUG UAT std. preﬁx-tuning 92.48 16.64 1.92 31.58 8.84 5.05 + our framework 92.59 50.36 44.65 58.54 46.68 85.72 20-epoch VIPER aug. 82.59 38.93 53.32 50.85 37.51 10.49 + our framework 83.09 50.03 51.89 54.09 50.91 59.75 50-epoch VIPER aug. 90.06 36.02 63.64 45.47 30.97 21.03 + our framework 89.73 51.89 63.21 58.15 47.83 81.49 100-epoch VIPER aug. 93.63 25.54 52.22 36.19 21.75 5.22 + our framework 92.97 51.18 52.61 59.75 49.53 84.57 20-epoch SCPN aug. 72.27 41.35 40.14 54.64 42.17 16.20 + our framework 72.38 50.74 49.97 54.75 50.80 62.11 50-epoch SCPN aug. 87.64 34.32 43.33 62.93 35.04 10.38 + our framework 87.64 48.82 48.76 63.98 49.82 82.43 100-epoch SCPN aug. 89.13 28.45 39.32 59.91 23.39 2.69 + our framework 89.24 51.67 48.60 62.11 48.33 79.35 We attach the clock time that each method takes on each benchmark for comparison in Table 11. The time differs for each benchmark because of different sizes (Table 10). The training-phase adversarial preﬁx-tuning takes far greater time than standard preﬁx-tuning. The adversarial data augmentation methods during training phase take the longest time due to the computationally-challenging discrete search/optimization of text attacks. For our framework, steps 1 and 2 that construct the canonical manifolds are viewed as preprocessing since it can be used against all attacks during inference. As ∥SC∥>> d(mentioned in Section 3), we can further speed up step 1 by random sampling in SC. The additional preﬁx P′ ψ[i,:] is tuned on the ﬂy during test phase in step 3 of our framework, which slackens the standard testing but is negligible compared with the training-phase baseline methods. In conclusion, our framework has preserved the efﬁciency and modularity of preﬁx-tuning. Table 10: Dataset statistics for each benchmark. We have also included the number of classes in each benchmark and the accuracy of random classiﬁer in theory for better understanding. Benchmark #Classes Train Dev Test Random Acc(%) SST-2 2 6,920 872 1,821 50.00 AG’s News 4 115,000 5,000 7,600 25.00 SNLI 3 550,152 10,000 10,000 33.33 Table 11: Time used by different methods for all benchmarks. Compared with the time-consuming training-phase baseline methods, our test-phase robust preﬁx-tuning is signiﬁcantly faster. Phase Method Time (GPU min) SST-2 AG’s News SNLI Train std. preﬁx-tuning 3.7 ×100 19.7 ×25 55.4 ×25 adv. preﬁx-tuning 17.7 ×100 223.0 ×25 641.1 ×25 adv. aug. PWWS 82.5 ×100 - - adv. aug. VIPER 47.3 ×100 - - adv. aug. SCPN 285.3 ×100 - - Preprocess ours: steps 1 and 2 1.0 69.9 721.5 Test std. testing 0.3 1.2 0.7 ours: step 3 (adaptive) 1.0 6.5 6.5 23Published as a conference paper at ICLR 2022 E M ORE REALISTIC EVALUATION SETTINGS E.1 P ERFORMANCE WITH SMALL TEST BATCH SIZES In this section, we continue to study why our methods with test batch size of2 or 4 achieve stronger robustness against UAT (shown in Table 3). We note that the j-th layer activation at the output position H(j) T is normalized by rows before being projected to the canonical manifold M(j) when the test batch size is larger than > 1. According to Eq. (8) and the description in Section 3, when the size of a test batch is larger than 1, namely, when H(j) T = [ h(j) o,t ] ∈R|ST |×d (12) has more than 1 row (|ST|>1), the objective for tuning P′ ψ is min ψ N−1∑ j=0 L ( ψ(j),Q(j) ) = N−1∑ j=0  ( H(j) T −11TH(j) T /|ST| )( I−Q(j) ) 2 . (13) The normalization is performed to alleviate the randomness among the samples in the test batch. It is not applicable when |ST|= 1, as H(j) T −11TH(j) T /|ST|would be 0. Therefore, for test batch size = 1, the loss for tuning P′ φ is Eq. (8) without normalization before projection. We conduct experiments with test batch sizes of 2 and 4 without normalizing the test batch before projection during inference for comparison. According to the results in Table 12, for test batch sizes of 2 and 4, robustness results against UAT have degenerated without the normalization, but they are still better than that with test batch size of 1 as mini-batch optimization may achieve better results than the fully online setting. The robust performance of our framework against other in-sentence attacks remains similar, with or without normalizing. Table 12: Performance of our method with std. preﬁx-tuning with various test batch sizes on SST-2. For test batch sizes of 2 and 4, we compare the robustness performance of our framework with and without normalization before projection during inference. Method Clean PWWS VIPER SCPN BUG UAT std. preﬁx-tuning 92.48 16.64 1.92 31.58 8.84 5.05 ours, bsz adaptive 92.59 50.36 44.65 58.54 46.68 85.72 ours, bsz = 1 (unnormalized) 92.64 51.46 43.99 58.32 46.73 73.09 ours, bsz = 2 (normalized) 92.48 49.92 45.74 59.14 48.93 84.73 ours, bsz = 4 (normalized) 92.64 50.36 45.85 58.54 49.31 86.66 ours, bsz = 2, unnormalized 92.53 51.13 42.89 57.99 48.00 78.20 ours, bsz = 4, unnormalized 92.59 51.84 43.44 58.59 47.89 80.23 The results lead to a temporary conclusion that normalization before projection during inference can be important when using our framework to defend against UAT. However, the performance gap between the unnormalized test batch size = 1 setting and the normalized test batch size >1 setting is much smaller on other benchmarks and other preﬁx-tuning baselines. We set test batch size as 1 and evaluate the performance of our framework with both standard and adversarial preﬁx-tuning on both SST-2 and AG’s News benchmarks. The results in Table 13 show that the robustness gap against UAT between the unnormalized test batch size= 1 setting and the normalized test batch size >1 setting, though still exists, is relatively small compared with the results in Table 12. As a result, for conventional evaluation where accuracy under speciﬁc attack is reported, we will use test batch size larger than 1 (2 or 4 should be satisfactory), as normalization before projection for additional tuning during inference can be beneﬁcial. In contrast, we will set test batch size as 1 in more realistic settings, as it is able to cope with real-time or mixed test data. Our another attempt for improvement is to set the normalization before projection during inference with pre-computed vectors beforehand. Currently the normalization 11TH(j) T /|ST|in Eq. (13) 24Published as a conference paper at ICLR 2022 Table 13: Performance of our framework with both standard and adversarial preﬁx-tuning on both SST-2 and AG’s News benchmarks, with test batch size of1 or adaptive. Benchmark Method Clean PWWS VIPER SCPN BUG UAT SST-2 std. preﬁx-tuning 92.48 16.64 1.92 31.58 8.84 5.05 + ours, bsz adaptive 92.59 50.36 44.65 58.54 46.68 85.72 + ours, bsz = 1 92.64 51.46 43.99 58.32 46.73 73.09 adv. preﬁx-tuning 93.57 30.64 7.25 35.42 25.04 4.88 + ours, bsz adaptive 93.79 57.55 43.60 60.68 57.17 91.87 + ours, bsz = 1 94.12 56.67 46.13 60.30 55.19 87.37 AG’s News std. preﬁx-tuning 86.42 43.00 24.55 43.20 43.22 52.20 + ours, bsz adaptive 86.50 53.91 24.93 48.38 51.79 76.47 + ours, bsz = 1 86.75 46.47 35.82 45.61 49.00 73.08 adv. preﬁx-tuning 89.46 50.91 26.30 45.25 47.11 59.74 + ours, bsz adaptive 90.26 56.45 31.25 48.03 54.66 87.26 + ours, bsz = 1 89.79 55.72 34.33 49.43 53.75 87.16 depends on H(j) T , with limitation in the setting of test batch size = 1 and concern in reproducibility. In our additional experiments, we pre-compute the normalization of the j-th layer activation during inference using SC, the set of correctly classiﬁed training examples. Consequently, we can replace 11TH(j) T /|ST|in Eq. (13) with 11TH(j) C /|SC|, which can be obtained in the second step in Section 3. In this way, the normalization is static for different batches and applicable to test batch with the size of 1. The results are shown in Table 14. Table 14: Performance of our method with std. preﬁx-tuning with static or dynamic normalization before projection during inference on SST-2. Method Clean PWWS VIPER SCPN BUG UAT std. preﬁx-tuning 92.48 16.64 1.92 31.58 8.84 5.05 ours, bsz adaptive 92.59 50.36 44.65 58.54 46.68 85.72 ours, bsz = 1 (unnormalized) 92.64 51.46 43.99 58.32 46.73 73.09 ours, bsz = 2 (dynamic norm.) 92.48 49.92 45.74 59.14 48.93 84.73 ours, bsz = 4 (dynamic norm.) 92.64 50.36 45.85 58.54 49.31 86.66 ours, bsz = 1 static norm. 92.53 49.48 40.25 58.43 48.98 87.92 ours, bsz = 2, static norm. 92.37 50.58 41.57 58.48 47.17 87.70 ours, bsz = 4, static norm. 92.48 49.70 40.69 58.15 48.22 87.64 On the one hand, it can be seen that our framework with static normalization outperforms the ones with dynamic normalization under UAT. The improvement is especially substantial for the setting with test batch size = 1 , indicating that while the dynamic normalization 11TH(j) T /|ST|is not applicable for test batch size = 1 , the static normalization 11TH(j) C /|SC|can be helpful in this setting. On the other hand, for in-sentence attacks, the robustness results are comparable between static and dynamic normalization, though there is a performance drop under VIPER. Due to time limit, we have only experimented on SST-2 with standard preﬁx-tuning as the baseline. We will conduct more experiments to further evaluate static normalization with other baselines on different benchmarks. In the following experiments under more realistic threat models, we adopt the setting of the test batch size = 1 without normalization before projection. 25Published as a conference paper at ICLR 2022 E.2 P ERFORMANCE UNDER MIXED TEST DATA In this section, we provide more experimental results of our framework under mixed test data. We combine the clean test set with the test set under one attack, or perturb the test set with two different attacks separately and combine the two sets as mixed test data. The mixed test data can be from a more realistic scenario. To cope with it, we set the test batch size as 1 in order to avoid the situation where both perturbed and unperturbed test sentences or perturbed sentences under different attacks exist in a test batch. Besides, by setting the test batch size as1 we can deal with real-time data, which is also a practical approach. We use adversarial preﬁx-tuning as the baseline to obtain a strong preﬁx Pθ[i,:]. While we are not able to tune the robust preﬁx under speciﬁc attack in this setting, we ﬁnd it effective to tune the initial learning rate and the number of steps (5 or 10) for our inference-phase robust preﬁx on the UAT-attacked development set and ﬁx them for every incoming test sentence. The results are listed in Table 15. Table 15: Performance of our framework under mixed test data on SST-2 and AG’s News. The ‘+’ denotes combination of test set clean or under attack, and ‘C’ is short for “Clean”, ‘B’ for “BUG”, etc. The test batch size is set as 1 for all experiments with our framework. We also provide the averaged results with the robust preﬁx in our framework separately tuned on the two test sets as an upper bound for comparison. Benchmark Method C + B C + P V + B V + P S + B S + P U + B U + P SST-2 adv. PT 60.65 62.27 17.76 20.43 29.57 31.85 18.12 20.81 + ours mixed 69.71 71.11 46.02 47.23 55.63 56.92 70.04 71.67 ours, separate 74.66 75.40 50.66 51.40 57.75 58.49 71.28 72.02 AG’s News adv. PT 52.38 53.41 26.54 27.14 34.86 35.36 40.97 42.16 + ours mixed 71.36 71.39 42.84 43.99 49.04 50.12 69.88 71.30 ours, separate 71.77 72.76 44.04 45.03 51.59 52.58 70.46 71.44 According to the results, the performance under mixed test data is consistently improved using our framework. We also provide the averaged results with the robust preﬁx in our framework separately tuned on the two test sets as an upper bound for comparison. It can be seen that small performance gap exists between the results under mixed data and the averaged results with separately tuning for speciﬁc test sets. A potential reason is that our framework can better adapt to the data under speciﬁc attack. To reduce the performance gap, we can construct more powerful canonical manifolds that capture richer data information of multiple granularity so that our framework also achieves optimal performance under mixed test data. We leave the exploration for future work. 26Published as a conference paper at ICLR 2022 F M ODEL BEHAVIOR ANALYSIS F.1 I N-SENTENCE ATTACKS : AVERAGING THE ATTENTION We provide the attention weights from the LM ﬁnal layer over the entire x = [context,question,[ANS]] with PWWS as the text attack from the development set of SST-2. The perturbed inputs are shown in Table 16, and the visualizations are shown in Figure 7, Figure 8, and Figure 9. Similar behavior of averaging the attention of the ﬁnal layer in the LM can be also ob- served as the BUG attack visualized in Figure 4. Caveat: the observed behavior is of the ﬁnal layer only, as attention is unreliable for indicating importance over input tokens (Jain & Wallace, 2019; Wiegreffe & Pinter, 2019; Abnar & Zuidema, 2020; Serrano & Smith, 2019). Table 16: Original inputs from the development set of SST-2 with perturbation by PWWS. Type Input (the context part) Predict Label Original a giggle a minute. positive positive Perturbed A giggle a arc minute. negative positive Original a marvel like none you’ve seen. positive positive Perturbed A wonder I none you’ve get. negative positive Original rarely has so much money delivered so little entertainment. negative negative Perturbed Rarely has so much money delivered so picayune entertainment. positive negative (a) Predict:positive  (b) Predict:negative (c) Predict:positive  (d) Predict:positive Figure 7: Visualized attention weights from the ﬁnal layer in the LM for the ﬁrst example in Table 16. (a): Original input with original preﬁx-tuning; (b): PWWS-perturbed input with original preﬁx- tuning; (c) and (d): PWWS-perturbed input with robust preﬁx-tuning of (c) N = 24 and (d) N = 3. 27Published as a conference paper at ICLR 2022 (a) Predict:positive  (b) Predict:negative (c) Predict:positive  (d) Predict:positive Figure 8: Visualized attention weights from the ﬁnal layer in the LM for the second example in Table 16. (a): Original input with original preﬁx-tuning; (b): PWWS-perturbed input with original preﬁx-tuning; (c) and (d): PWWS-perturbed input with robust preﬁx-tuning of (c) N = 24 and (d) N = 3. 28Published as a conference paper at ICLR 2022 (a) Predict:negative  (b) Predict:positive (c) Predict:negative  (d) Predict:negative Figure 9: Visualized attention weights from the ﬁnal layer in the LM for the third example in Table 16. (a): Original input with original preﬁx-tuning; (b): PWWS-perturbed input with original preﬁx- tuning; (c) and (d): PWWS-perturbed input with robust preﬁx-tuning of (c) N = 24 and (d) N = 3. In this example, robust preﬁx-tuning with N = 24 steers the ﬁnal layer in the LM with the absolute behavior of averaging the attention , while that with N = 3 steers it to assign higher attention weights to the input vector at the position of the input token “entertainment” as well as that of each token in the question part. We use this visualization to show the slightly different behavior of our framework against in-sentence attacks when tuning different bottom N layers. 29Published as a conference paper at ICLR 2022 F.2 U NIVERSAL ADVERSARIAL TRIGGERS : I GNORING THE DISTRACTION F.2.1 T HE ALTERNATIVE PROXY In this section, we introduce the alternative proxy for the analysis of token importance as a replace- ment of attention. To interpret token importance, following Serrano & Smith (2019), we leverage an alternative gradient-based importance ranking proxy. As stated in Serrano & Smith (2019), while it does not provide a gold-standard indicator of importance value for each token to the model, the proxy is more reliable in reﬂecting the importance ordering for tokens in the sequence. In the proxy, the importance for each token is given by the multiplication of the gradient of the decision function with respect to each attention weight and the attention weight magnitude. As there are either2, 3, or 4 classes in our text classiﬁcation benchmarks, the decision function at the output position is given by do ( W ( h(L) o )) = exp maxi [ W ( h(L) o )] i ∑ iexp [ W ( h(L) o )] i , (14) where W in the LM transforms the top-layer output h(L) o to a probability vector over the dictionary, and i ∈ Slabel traverses all the class label tokens in the vocabulary ( |Slabel| = the number of classes in the benchmark). The decision function is proposed in Serrano & Smith (2019) to analyze hierarchical attention network (Yang et al., 2016). We further augment the decision function for time steps < oto study the entire behavior of how the autoregressive LM assigns importance to each token visible at each time step. As introduced in Section 2, the autoregressive LM generates sequence [x,˜y] with [[CLS],x] as the input sequence for text classiﬁcation tasks ( ˜yis the generated label token as prediction). For each time step < o, the output of the LM is a token from x. As a result, we deﬁne the decision function at the k-th position as dk ( W ( h(L) k )) = exp [ W ( h(L) k )] xk ∑ j∈Vexp [ W ( h(L) k )] j , (15) with xk−1 (or [CLS] for k= 0) as input and xk as output. The complete decision function is then d ( W ( h(L) 0 ) ,··· ,W ( h(L) k ) ,··· ,W ( h(L) o )) = o∑ k=0 dk ( W ( h(L) k )) . (16) In this way, at each time stepi, the gradient taken from the decision function (Eq. (16)) with respect to each attention weight aij (j ≤i) in the ﬁnal layer of the LM can be obtained with automatic differentiation in PyTorch (Paszke et al., 2019). We multiply the gradient with the attention weight magnitude as the value of token importance, which ranks a token with both high attention weight and high calculated gradient to be most important. Formally, for the attention weight on the j-th token at the i-th time step in the h-th head of the ﬁnal layer (namely a(h) ij ), its importance value is I(h) ij = ∂d ∂a(h) ij a(h) ij , (17) where j ≤i. The upper triangle part of the importance matrix is still masked, namelyI(h) ij = 0 when j > i. We average the importance matrix over all heads in the ﬁnal layer. Note that the averaged importance value can be negative, as each calculated gradient∂d/∂a(h) ij in Eq. (17) can be negative. For better visualization, we further normalize all the token importance values at each time step iby subtracting the minimum value so that the negative values are eliminated. We ﬁnally normalize all the (now positive) token importance values at each time step iinto a probability vector. Formally, I′ ij = 1 H ∑ h I(h) ij , ˆIij = I′ ij − min 0≤j≤i I′ ij, Iij = ˆIij ∑i j=0 ˆIij , (18) where His the number of heads in the ﬁnal layer. The latter two equations transformI′[i,:] to I[i,:] as a probability vector. This is similar to an attention row, which is a better form for visualization. The importance matrix Iis plotted for all the case studies under UAT attack (Figure 5 and Appendix F.2.2). Also note that the transformation from I′to I is order-preserving for each row, and we will only use the importance ordering information in quantitative studies F.2.3. 30Published as a conference paper at ICLR 2022 F.2.2 Q UALITATIVE ANALYSIS We use UAT to attack inputs from the development set of SST-2 for visualization. The perturbed inputs are shown in Table 17, and importance visualizations are shown in Figures 10, 11, 12. Here we also plot the importance between each token pairs of the adversarial triggers. We can observe the behavior of ignoring the distraction for our robust preﬁx-tuning framework with N = 3. Table 17: Original inputs from the development set of SST-2 with perturbation by UAT. Type Input (the context part) Predict Label Original this is so bad. negative negative Perturbed lifts mates who this is so bad. positive negative Original it treats women like idiots. negative negative Perturbed lifts mates who it treats women like idiots. positive negative Original beautifully observed, miraculously unsentimental comedy-drama. positive positive Perturbed grades lame Invalid beautifully observed, miraculously unsentimental comedy-drama. negative positive (a) Predict:negative  (b) Predict:positive (c) Predict:positive  (d) Predict:negative Figure 10: Visualized importance matrices for the ﬁrst example in Table 17. (a): Original input with original preﬁx-tuning; (b): UAT-attacked input with original preﬁx-tuning; (c) and (d): UAT- attacked input with robust preﬁx-tuning of (c) N = 24 and (d) N = 3. 31Published as a conference paper at ICLR 2022 (a) Predict:negative  (b) Predict:positive (c) Predict:positive  (d) Predict:negative Figure 11: Visualized importance matrices for the second example in Table 17. (a): Original input with original preﬁx-tuning; (b): UAT-attacked input with original preﬁx-tuning; (c) and (d): UAT- attacked input with robust preﬁx-tuning of (c) N = 24 and (d) N = 3. 32Published as a conference paper at ICLR 2022 (a) Predict:positive  (b) Predict:negative (c) Predict:negative  (d) Predict:positive Figure 12: Visualized importance matrices for the third example in Table 17. (a): Original input with original preﬁx-tuning; (b): UAT-attacked input with original preﬁx-tuning; (c) and (d): UAT- attacked input with robust preﬁx-tuning of (c) N = 24 and (d) N = 3. We omit the visualization of the question part for this example due to its context length. 33Published as a conference paper at ICLR 2022 F.2.3 Q UANTITATIVE ANALYSIS In this section, we conduct quantitative analysis to show that for UAT, the robust preﬁx steers the LM to ignore the distraction of the adversarial triggers. We ﬁrst investigate whether the trigger is assigned with less importance under the robust preﬁx. Based on the proxy, at each time step i, we get the rankings Kij for all visible tokens xj: K[i,: i+ 1] = argsort(I[i,: i+ 1]) (19) where the argsort operator, provided in frameworks like NumPy (Harris et al., 2020) and PyTorch (Harris et al., 2020), returns the ranking of each value in the vector by increasing order. Our quan- titative analysis is based on token importance rankings at each time step, as the proxy I does not provide a gold indicator for importance value but rather an alternative ordering method. We inves- tigate the importance of the adversarial trigger by calculating the relative ranking of the token with maximal importance in the trigger. We set the trigger length as 3 in our experiments, so for each time step i≥3, the relative ranking is max(K[i,: 3])/(i+ 1) ×100%. (20) The example-level Degree of Distraction ( DoD) is deﬁned as the importance of the adversarial trigger averaged at all time steps since the generation of the ﬁrst token in context: DoD(x) = 1 o−3 o∑ i=3 max(K[i,: 3])/(i+ 1) ×100% (21) where xrepresents the input example and K is the calculated importance ranking matrix. Corpus- level Degree of Distraction (cDoD) is then deﬁned as the example-level degree of distraction aver- aged over all UAT-attacked examples. We calculate the cDoD of the original preﬁx-tuning as well as our framework with N = 24 and N = 3 under UAT attack on the SST-2 development set. The results are listed in Table 18. Similar to a trustworthy evaluation for machine translation (Koehn, Table 18: Calculated corpus-level Degree of Distraction of the original preﬁx-tuning as well as our framework under UAT attack. Results with†are statistically signiﬁcant over the that of the standard preﬁx-tuning baseline with p< 0.001. Method cDoD ↓(%) std. preﬁx-tuning 67.67 ours, N = 24 62.06† ours, N = 3 61.68† 2004; Marie et al., 2021), we also conduct statistical signiﬁcance tests by bootstrapping in the sets of DoD of all test samples calculated for different methods. According to Table 18, the cDoD results of our framework (both N = 24 and N = 3) are lower than that of the original preﬁx-tuning with statistical signiﬁcance (p< 0.001). We proceed to study whether the robust preﬁx manages to steer the LM to assign the highest impor- tance to the most essential token as the original preﬁx-tuning does with inputs without adversarial triggers. Let I0 be the importance matrix for the baseline method with clean input, andIu be the im- portance matrix for some method with the same input but prepended with adversarial trigger tokens. Let the notation oindicate the output position for the UAT-attacked input, then the output position for the clean input is o−3 (3 is the trigger length). At each time step i, We use the indicator Eu(i) = 1 (argmax (I0[i,: i+ 1]) = argmax (Iu[i+ 3,: i+ 4]) −3) , 0 ≤i≤o−3 (22) to determine whether the method with UAT-attacked inputs recognize the essential token by assign- ing the highest importance to the same token ( argmax (Iu[i+ 3,: i+ 4]) −3, left-shifted by the trigger length 3) as the baseline with clean inputs does (argmax (I0[i,: i+ 1])). The subtraction of 3 in the RHS of Eq. (22) is due to the right shift of the original input by the trigger tokens. Similar to the calculation of DoD and cDoD, we compute the example-level recognition of the essential token (RoE) for different methods under UAT attack by averagingEu(i) over all time steps: RoE(x) = 1 o−3 o−3∑ i=0 Eu(i). (23) 34Published as a conference paper at ICLR 2022 Table 19: Calculated corpus-level Recognition of the Essential token of the original preﬁx-tuning as well as our framework under UAT attack. Result with †is statistically signiﬁcant over that of the standard preﬁx-tuning baseline with p< 0.001. Result with ‡is statistically signiﬁcant over that of our framework with N = 24 with p< 0.001. Method cRoE ↑(%) std. preﬁx-tuning 48.05 ours, N = 24 34.60 ours, N = 3 71.63†‡ We then compute the corpus-level Recognition of the Essential token ( cRoE) by averaging RoE over all test samples. We calculate the cRoE of the original preﬁx-tuning as well as our framework with N = 24 and N = 3 under UAT attack on the SST-2 development set. The results are listed in Table 19. It can be seen that our framework withN = 3 achieves the highest cRoE score, which is also statistically signiﬁcant compared with the standard preﬁx-tuning baseline and our framework with N = 24. We also ﬁnd that the cRoE score of our framework with N = 24 is lower than that of the baseline. As a result, though thecDoD score shows that our framework with N = 24 ignores the distraction of the adversarial triggers to some extent, it seems that the robust preﬁx with N = 24 fails to steer the LM to assign the highest importance to the most essential token under UAT attack. In contrast, when the LM is attacked by UAT, the robust preﬁx-tuning with N = 3 steers the LM so that it not only ignores the distraction from adversarial triggers, but is also (very much likely to be) able to assign the highest importance to the most essential token as the baseline method does with the input without adversarial trigger tokens. The results from Tables 18 and 19 provide an explanation to the performance shown in Figure 3-(a) that under UAT attack, our framework with N = 3 substantially outperforms that with N = 24 , and both of them achieve higher robustness compared with the standard preﬁx-tuning baseline. We hope that our quantitative studies can better support the observed behavior and provide a deeper understanding of the robust preﬁx-tuning framework. G I NTERPRETATION FROM THE OPTIMAL CONTROL PERSPECTIVE In this section, we provide an interpretation from the optimal control perspective for preﬁx-tuning as well as our robust preﬁx-tuning framework. Preﬁx-tuning seeks Pθ[i,:] with forward propagation of the j-th layer of the L-layer LM at the output position oas h(j+1) o = LM(j) Φ ( h(j) o ⏐⏐⏐h(j) <o ) (24) with h(j) i = Pθ(j) [i,:] for all j = 0 to L−1, i∈Pidx and h(0) i = zi for i /∈Pidx to optimize min {θ(0),...,θ(L−1)} E(x,y)∼Dtr  S ( h(L) o ,y ) + L−1∑ j=0 R ( θ(j) )  , (25) with Sas the softmax scoring function, Ras the regularizer, Φ as the LM parameters, zi as the i-th token in the input and yas the label. According to Li et al. (2017), the Sand Rcan be viewed as the terminal and the running loss with Pθ as the control variables, and the forward and backward propa- gation of preﬁx-tuning are equivalent to the calculation of co-state process in Pontryagin’s Maximum Principle (Kopp, 1962). Therefore, preﬁx-tuning can be formalized as seeking the optimal control of the pretrained models for speciﬁc downstream tasks. Similarly, Our robust preﬁx-tuning framework seeks P′ ψ[i,:] with h(j) i = Pθ(j) [i,:] + Pψ′(j) [i,:] for all j = 0 to L−1, i∈Pidx in the forward propagation of Eq. (24). Our goal is given by min {ψ(0),...,ψ(L−1)} E(x,y)∼Dte L−1∑ j=0 ℓ ( ψ(j),Q(j) ) , (26) where the running loss ℓis given by Eq. (8) for j ≤N −1 and set to 0 for j ≥N. Compared with Eq. (25), there is no terminal loss in Eq. (26) as the label is unknown during test phase. As 35Published as a conference paper at ICLR 2022 suggested by Chen et al. (2021), our robust preﬁx-tuning can be formalized as seeking the close-loop control for robust downstream tasks. Before using optimal control theory to prove the above formalizations, we ﬁrst provide a brief intro- duction to Li et al. (2017) that reveal the relationship between the optimal control theory and deep learning. We borrow the theorems in Section 4 of Li et al. (2017) and directly follow their notations: Theorem G.1. (discrete-time PMP) Consider the discrete-time control problem min {θ0,...,θT−1}∈ΘT Φ(xT) + δ T−1∑ t=0 L(θt), xt+1 = xt + δft(xt,θt), x0 = x, 0 ≤t≤T −1 (27) where Φ is the termination loss and Lis the running loss. Then there exists a co-process x∗ t+1 = gt(x∗ t,θ∗ t), x ∗ 0 = x, (28) p∗ t = ∇xHt(x∗ t,p∗ t+1,θt), p ∗ T+1 = −∇xΦ(x∗ T+1) (29) such that Ht(x∗ t,p∗ t+1,θ∗ t) ≥Ht(x∗ t,p∗ t+1,θ), θ∈Θ, 0 ≤t≤T −1. (30) Here gt(xt,θt) := xt + δft(xt,θt) and Ht(x,p,θ ) = p·gt(x,θ) −δL(θ) (31) is the scaled discrete Hamiltonian. Theorem G.2. (discrete-time MSA) The co-process in Theorem G.1 can be determined by applying the discrete-time method of successive approximations (MSA). For each iterationk, set xk 0 = x, and xk t+1 = gt(xk t,θk t) (32) with tenumerating from 0 to T −1; then set pk T = −∇xΦ(xk T), and pk t = ∇xHt(xk t,pk t+1,θk t) (33) with tenumerating from T −1 to 0; ﬁnally, with tenumerating from 0 to T −1, set θk+1 t = θk t + η∇θHt(xk t,pk t+1,θk t). (34) Theorem G.3. (equivalence between MSA and back-propagation) The MSA in Theorem G.2 is equivalent to gradient descent with back-propagation. The proofs of Theorems G.1, G.2 and G.3 are provided in Li et al. (2017). We now investigate the forward propagation of preﬁx-tuning. In Eq. (24), LM(j) Φ , the j-th layer of the LM, can be decomposed into a self-attention layer ( SAN(j) Φ ) and a FFN layer ( FFN(j) Φ ). Formally, h(j+1) o = h(j) o + SAN(j) Φ ( h(j) o ,h(j) <o ) + FFN(j) Φ ( h(j) o + SAN(j) Φ ( h(j) o ,h(j) <o )) (35) with h(j) i = Pθ(j) [i,:] for i ∈Pidx. As Eq. (35) is recursive and according to the fact that the pretrained LM is autoregressive, after unrolling the recursion for all h(j) <o in Eq. (35), h(j+1) o can be represented in the form of h(j+1) o = h(j) o + G(j) Φ ( h(j) o ,θ(j) ) . (36) Now we take Eq. (25), the objective of preﬁx-tuning, into consideration. The optimization problem is now formulated as min {θ0,...,θL−1}∈ΘL E(x,y)∼Dtr  S ( h(L) o ,y ) + L−1∑ j=0 R ( θ(j) )   h(j+1) o = h(j) o + G(j) Φ ( h(j) o ,θ(j) ) , h(0) o = zo = [ANS], 0 ≤j ≤L−1. (37) 36Published as a conference paper at ICLR 2022 Using Theorem G.1, we know that the objective of preﬁx-tuning can be formulated as a discrete- time control problem. We then use the MSA described in Theorem G.2 to determine the co-process that solves the control problem and realize that the MSA is equivalent to back-propagation with Theorem G.3, which recovers the training phase of preﬁx-tuning. As a result, we can conclude that preﬁx-tuning seeks the optimal control of the pretrained models for speciﬁc downstream tasks. Our robust preﬁx-tuning framework can also be formalized as the close-loop control of the obtained preﬁx for robust downstream tasks. According to Chen et al. (2021), the close-loop control frame- work for robust neural networks is deﬁned as min {π0,...,πL−1} L−1∑ t=0 ℓ(xt,πt(xt)), xt+1 = ft(xt,πt(xt)), x0 = x, 0 ≤t≤L−1. (38) where ℓ(xt,πt(xt)) is the running loss. According to Eq. (26), the goal of our robust preﬁx-tuning framework can be formulated into the form min {ψ(0),...,ψ(L−1)} E(x,y)∼Dte L−1∑ j=0 R(j) Φ,θ(j) ( h(j) o ,ψ(j) ( h(j) o )) , h(j+1) o = h(j) o + ˜G(j) Φ,θ(j) ( h(j) o ,ψ(j) ( h(j) o )) , h(0) o = zo = [ANS], 0 ≤j ≤L−1. (39) Since Theorems G.1, G.2 and G.3 can also be applied to Eq. (39), the optimization of our robust preﬁx is equivalent to seeking the close-loop control for robust downstream tasks. In conclusion, recent years have witnessed the paradigm shift in NLP from adapting LMs to down- stream tasks to adapting downstream tasks to LMs (Liu et al., 2021). In this work, we provide the interpretation of preﬁx-tuning as well as our robust preﬁx-tuning framework from the optimal con- trol perspective. We hope that our work can bring insight to the future design of robust and efﬁcient ﬁnetuning approaches for large-scale pretrained models. 37",
      "meta_data": {
        "arxiv_id": "2203.10378v1",
        "authors": [
          "Zonghan Yang",
          "Yang Liu"
        ],
        "published_date": "2022-03-19T18:52:47Z",
        "pdf_url": "https://arxiv.org/pdf/2203.10378v1.pdf",
        "github_url": "https://github.com/minicheshire/Robust-Prefix-Tuning"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper proposes a robust prefix-tuning framework for text classification that addresses the lack of robustness of existing prefix-tuning methods against textual adversarial attacks, without compromising their efficiency and modularity. The core idea involves leveraging layerwise activations of the language model by correctly-classified training data to set a standard for additional prefix finetuning. During the test phase, an extra batch-level prefix is tuned on-the-fly for each batch and added to the original prefix to enhance robustness. This framework substantially improves robustness against five types of textual attacks while maintaining comparable accuracy on clean texts, making it the first defense approach for prefix-tuning that preserves its lightweightness and modularity. The paper also provides an interpretation of the framework from an optimal control perspective.",
        "methodology": "The robust prefix-tuning framework involves three main steps. First, it collects layerwise activations (H(j)C) at the output position from correctly classified training examples (SC) using the original prefix (Pθ). Second, these collected activation matrices are projected onto low-level canonical manifolds (M(j)) using Principal Component Analysis (PCA) to characterize 'correct' model behavior. Before PCA, rows of H(j)C are normalized to mitigate randomness among samples. Third, during inference, an additional batch-level prefix (P'ψ) is tuned on-the-fly for each test batch (ST). This tuning aims to minimize the distance between the activations (H(j)T), triggered by the combined prefix (Pθ + P'ψ) with potentially perturbed inputs, and the canonical manifolds M(j). The objective function is the sum of squared Frobenius norms of the difference between H(j)T and its projection onto M(j) across N bottom layers. Normalization is also applied to H(j)T for test batches larger than one. This approach is also noted to be applicable to other soft prompt-based tuning methods.",
        "experimental_setup": "The framework was evaluated on three text classification benchmarks: binary Stanford Sentiment Treebank (SST-2), AG’s News, and Stanford Natural Language Inference (SNLI). The large-scale language model used was GPT2-medium (24 layers). Prefix length was set to 10 for all prefix-tuning experiments. The models were trained for 100 epochs on SST-2 and 25 epochs on AG’s News and SNLI, using the AdamW optimizer with an initial learning rate of 5e-5. For the robust prefix-tuning, the bottom N=3 layers' activations were recorded for additional tuning, with the additional prefix P'ψ optimized using Adam. Five text attacks were used for evaluation: PWWS (word-level), VIPER (character-level), SCPN (sentence-level), TextBugger (word- and character-level), and Universal Adversarial Triggers (UAT). Baselines included standard prefix-tuning, adversarial prefix-tuning (word- and sentence-level L2 balls, ϵ=5, α=1.25, M=10 iterations), and adversarial data augmentation (using PWWS, VIPER, or SCPN). Experiments were conducted on NVIDIA-3090 GPUs. Model behavior was analyzed using attention weights and a gradient-based token importance ranking proxy, along with quantitative metrics like corpus-level Degree of Distraction (cDoD) and Recognition of the Essential token (cRoE).",
        "limitations": "The paper acknowledges several limitations: adversarial training for prefix-tuning, while improving robustness, suffers from substantially longer training times and difficulty in optimizing the small number of prefix embedding parameters, leading to poor clean accuracy with certain regularization methods (e.g., KL-divergence). Adversarial data augmentation methods are even more time-consuming due to discrete optimization and can lead to a drop in clean accuracy and potential overfitting to specific attack types, hindering robustness against others. In inference, a performance gap in UAT robustness was observed for a test batch size of 1 compared to larger batch sizes, though this was mitigated by static normalization. A small performance gap still exists when dealing with mixed test data, suggesting that the current canonical manifolds might not capture sufficiently rich information for highly diverse inputs. Finally, the interpretability proxies used (attention weights, gradient-based importance) are noted as not being gold-standard indicators of importance but rather reliable for ordering.",
        "future_research_directions": "Future research directions include constructing better canonical manifolds, extending the proposed framework to text generation tasks, and exploring how to optimally construct the dataset used to calculate the canonical manifolds to achieve the best performance across both accuracy and robustness. Additionally, the authors suggest empowering the robust prefix-tuning framework with faster and more effective adversarial prefix-tuning approaches. Further exploration is also needed to reduce the performance gap under mixed test data by designing more powerful canonical manifolds that capture richer data information of multiple granularities.",
        "experimental_code": "def SVD(A, idd, tpos, THD=0.99):\n    U, Sigma, V = torch.svd(A)\n    sum_singu = Sigma.sum()\n    singus = 0.\n    cnt = 0\n    for sigma in Sigma:\n        singus += sigma\n        cnt += 1\n        if (singus / sum_singu) > THD:\n            break\n    logger.info('The rank of token_pos ' + str(tpos) + ' layer ' + str(idd) + ' is: ' + str(cnt))\n    return V[:, 0:cnt], Sigma[0:cnt], cnt\n\ndef PCA_proj(data, proj, isTestbs1=False):\n    if isTestbs1:\n        data_proj = torch.mm(data, proj)\n    else:\n        mean = data.mean(dim=0, keepdim=True)\n        data = data - mean\n        data_proj = torch.mm(data, proj) + mean\n    return data_proj\n\n# Excerpt from get_proj.py for Manifold Projection (Steps 1 & 2 of the method)\n# Model configuration for hidden states extraction\nmodel.transformer.output_hidden_states = True\n# ... (within test_one_to_one loop, after model forward pass and filtering for correctly classified examples 'sel') ...\nProj = [[] for jj in range(args.control_len)]\nfor jj in range(args.control_len):\n    for i in range(MODEL_CONFIG.n_layer):\n        G = save_hidden_states[jj][i] - save_hidden_states[jj][i].mean(dim=0, keepdim=True) # Layerwise mean normalization\n        Princ_axis, _, _ = SVD(G, i, jj) # Principal Component Analysis\n        Proj[jj].append(torch.mm(Princ_axis, Princ_axis.t())) # Store projection matrix for canonical manifold M(j)\n\n# Excerpt from test_robust_prefix_psi_insent.py for Robust Prefix Tuning (Step 3 of the method)\n# Initialize the additional batch-level prefix P'ψ\ns_tokens_control = torch.zeros_like(s_tokens).requires_grad_()\noptimizer = torch.optim.Adam([s_tokens_control], PMP_lr)\ncriterion_layer = nn.MSELoss() # Objective function: sum of squared Frobenius norms\n\n# On-the-fly tuning loop during inference\nfor ii in range(PMP_iter):\n    with torch.enable_grad():\n        past = (s_tokens + s_tokens_control).unsqueeze(0).expand(n_inputs, -1, -1).cuda() # Combine Pθ and P'ψ\n        bsz, seqlen, _ = past.shape\n        past = past.view(bsz, seqlen, MODEL_CONFIG.n_layer * 2,\n                         MODEL_CONFIG.n_head, MODEL_CONFIG.n_embd // MODEL_CONFIG.n_head)\n        past = past.permute([2, 0, 3, 1, 4]).split(2)\n        all_outputs = model(input_ids=now_cqs.cuda(), past=past) # Forward pass with combined prefix\n        all_hidden_states = all_outputs[2] # Extract layerwise activations H(j)T\n\n        loss_layer = torch.tensor(0.)\n        for jj in range(args.tune_layer): # Iterate across N bottom layers\n            for kk in range(args.control_len): # Iterate across activation positions\n                tmp = all_hidden_states[jj][range(n_inputs), len_cqs-1-kk, :] # H(j)T for current layer and position\n                # Minimize distance between H(j)T and its projection onto M(j)\n                loss_layer = loss_layer + criterion_layer(PCA_proj(tmp, projs[kk][jj], args.test_batch_size==1), tmp)\n\n    optimizer.zero_grad()\n    loss_layer.backward()\n    optimizer.step() # Update P'ψ",
        "experimental_info": "{\n  \"Model Architecture\": \"GPT2-medium\",\n  \"Original Prefix (Pθ) Length\": \"Configured by `args.preseqlen`\",\n  \"Manifold Learning (Offline Phase)\": {\n    \"Data for PCA\": \"Activations collected from correctly classified training examples, identified by `sel` (matching ground truth after normalization).\",\n    \"Normalization before PCA\": \"Layerwise mean subtraction applied to activations (`G = ... - G.mean(...)`).\",\n    \"Manifold characterization\": \"Principal Component Analysis (PCA) via SVD.\",\n    \"PCA explained variance threshold\": \"THD=0.99 for cumulative singular value sum to determine the rank of the manifold.\",\n    \"Number of layers for manifold learning\": \"`MODEL_CONFIG.n_layer` (all layers of the model).\",\n    \"Activation positions for manifold learning\": \"`args.control_len` positions from the output, specifically `len_cqs-1-jj`.\"\n  },\n  \"Robust Prefix (P'ψ) Tuning (Online Inference Phase)\": {\n    \"Initialization\": \"`P'ψ` is initialized to a tensor of zeros with the same shape as `Pθ` (`torch.zeros_like(s_tokens)`).\",\n    \"Optimizer for P'ψ\": \"Adam optimizer (`torch.optim.Adam`).\",\n    \"Learning Rate for P'ψ\": \"`args.PMP_lr`.\",\n    \"Optimization Iterations\": \"`args.PMP_iter` per test batch.\",\n    \"Objective Function\": \"Mean Squared Error (`nn.MSELoss()`) between collected activations `H(j)T` and their projections onto the canonical manifolds `M(j)`. This minimizes the squared Frobenius norm.\",\n    \"Normalization during inference\": \"Activations `H(j)T` are normalized via mean subtraction before projection if `test_batch_size > 1` (within `PCA_proj`).\",\n    \"Number of bottom layers for tuning\": \"`args.tune_layer`.\",\n    \"Activation positions for tuning\": \"`args.control_len` positions from the output, specifically `len_cqs-1-kk`.\"\n  },\n  \"General Settings\": {\n    \"Random seed\": \"`args.seed = 101`.\",\n    \"Logit temperature\": \"`args.temperature_qa`.\",\n    \"Top-k/Top-p sampling\": \"`args.top_k_qa`, `args.top_p_qa`.\",\n    \"Batch size during testing for inference\": \"`args.test_batch_size` (can be a list for adaptive sizing or an integer for fixed size).\"\n  }\n}"
      }
    },
    {
      "title": "When Do Prompting and Prefix-Tuning Work? A Theory of Capabilities and Limitations",
      "abstract": "Context-based fine-tuning methods, including prompting, in-context learning,\nsoft prompting (also known as prompt tuning), and prefix-tuning, have gained\npopularity due to their ability to often match the performance of full\nfine-tuning with a fraction of the parameters. Despite their empirical\nsuccesses, there is little theoretical understanding of how these techniques\ninfluence the internal computation of the model and their expressiveness\nlimitations. We show that despite the continuous embedding space being more\nexpressive than the discrete token space, soft-prompting and prefix-tuning are\npotentially less expressive than full fine-tuning, even with the same number of\nlearnable parameters. Concretely, context-based fine-tuning cannot change the\nrelative attention pattern over the content and can only bias the outputs of an\nattention layer in a fixed direction. This suggests that while techniques like\nprompting, in-context learning, soft prompting, and prefix-tuning can\neffectively elicit skills present in the pretrained model, they may not be able\nto learn novel tasks that require new attention patterns.",
      "full_text": "Published as a conference paper at ICLR 2024 WHEN DO PROMPTING AND PREFIX -TUNING WORK ? A THEORY OF CAPABILITIES AND LIMITATIONS Aleksandar Petrov, Philip H.S. Torr & Adel Bibi Department of Engineering Science University of Oxford Oxford, United Kingdom {aleksandar.petrov,philip.torr,adel.bibi}@eng.ox.ac.uk ABSTRACT Context-based fine-tuning methods, including prompting, in-context learning, soft prompting (also known as prompt tuning), and prefix-tuning, have gained popu- larity due to their ability to often match the performance of full fine-tuning with a fraction of the parameters. Despite their empirical successes, there is little theoret- ical understanding of how these techniques influence the internal computation of the model and their expressiveness limitations. We show that despite the contin- uous embedding space being more expressive than the discrete token space, soft- prompting and prefix-tuning are potentially less expressive than full fine-tuning, even with the same number of learnable parameters. Concretely, context-based fine-tuning cannot change the relative attention pattern over the content and can only bias the outputs of an attention layer in a fixed direction. This suggests that while techniques like prompting, in-context learning, soft prompting, and prefix- tuning can effectively elicit skills present in the pretrained model, they may not be able to learn novel tasks that require new attention patterns. 1 I NTRODUCTION Language model advances are largely driven by larger models and more training data (Kaplan et al., 2020; Rae et al., 2021). Training cutting-edge models is out of reach for most academic researchers, small enterprises, and individuals, and it has become common to instead fine-tune open-source pre- trained models (Devlin et al., 2019; Min et al., 2021). Yet, due to escalating computational demands, even fine-tuning of the larger models has become prohibitively expensive (Lialin et al., 2023). As a result, there is an acute need for more efficient fine-tuning methods, either by sparsely modi- fying the parameters of the model or modifying its input context. Examples of the first type include adapter modules which introduce a few trainable layers to modify the behaviour of the frozen pre- trained network (Rebuffi et al., 2017; Houlsby et al., 2019; Hu et al., 2023). One can also use low-rank updates, which also results in a reduced number of trainable parameters (Hu et al., 2021). Context-based fine-tuning has been motivated by the success of few-shot and zero-shot learning (Wei et al., 2021; Kojima et al., 2022). The most popular context-based approach is prompting, where generation is conditioned on either human-crafted or automatically optimized tokens (Shin et al., 2020; Liu et al., 2023). In-context learning —prompting via providing input-label examples— is another widely used technique (Brown et al., 2020). Given the challenges of discrete optimization over tokens, there is a growing interest in methods that optimize real-valued embeddings (Lester et al., 2021). It is widely believed that these soft prompts offer greater expressiveness due to the expansive nature of continuous space. Furthermore, beyond only optimizing input embeddings, one can optimize the inputs of every attention layer (Li and Liang, 2021). This technique, prefix-tuning, has proven to be very successful and competitive to full fine-tuning (Liu et al., 2022). While context-based fine-tuning approaches have witnessed impressive empirical successes and widespread adoption, we have little theoretical understanding of how they work. In this work, we analyse the influence of prompts and prefixes on the internal computations of a pretrained model and delineate their limitations. Specifically, we address the following questions: 1 arXiv:2310.19698v2  [cs.LG]  9 Apr 2024Published as a conference paper at ICLR 2024 1. Soft prompting and prefix-tuning are motivated by the embedding space being larger than the token space. However, can a transformer utilize the additional capacity? We show that with a careful choice of transformer weights, controlling a single embedding can generate any of the V N completions of N tokens, while controlling a token can produce only V completions, with V being the vocabulary size. Thus, a transformer can indeed exploit the embedding space. 2. Since prefix-tuning is more expressive than prompting, is it as expressive as full fine-tuning? Despite the expressiveness of continuous space, prefix-tuning has structural limitations. A prefix cannot change the relative attention over the content tokens and can only bias the output of the attention block in a constant direction. In contrast, full fine-tuning can learn new attention patterns and arbitrarily modify attention block outputs, making it strictly more powerful. 3. If context-based fine-tuning methods suffer from such structural limitations, how come they have high empirical performance? We show that the prefix-induced bias can steer the model towards a pretraining task. Prefix-tuning can also combine skills picked up during pretraining to solve some new tasks similar to pretraining tasks. However, it may not learn a completely new task. This is not simply because of the small number of learnable parameters: fine-tuning the same number of parameters can be sufficient to learn the novel task. Hence, context-based fine- tuning can elicit or combine pretrained model skills but cannot learn completely new behaviors. 2 B ACKGROUND 2.1 T HE TRANSFORMER ARCHITECTURE We outline a simplified decoder-only transformer architecture (Vaswani et al., 2017). Assume that the model has vocabulary size V (also referred to as number of tokens ). The input is a sequence (x1, . . . ,xp), xi∈{1, . . . , V}, ∀i. Each token is mapped to a de-dimensional vector that is the xi- th column of an embedding matrix E∈Rde×V . The attention mechanism is position-invariant, so typically position encodings are added. For a model with maximum input lengthN (context size), we use a one-hot position encoding eN (i) concatenated with the embedding. Therefore, the embedding for the i-th position provided to the first attention block would be xi = [E⊤ :,xi, e⊤ N (i)]⊤. A transformer consists of alternating attention blocks which operate across the whole sequence and Multi-Layer Perceptrons (MLPs) that operate on each individual element. Each attention block consists of H heads. Each head h is parameterized by query, key, and value matrices Wh Q, Wh K∈Rk×din , Wh V ∈ Rdout×din .1 The attention matrix Ah∈Rp×p for head h then has elements Ah ij = exp \u0000 T/ √ k(Wh Qxi)⊤(Wh Kxj) \u0001 Pp r=1 exp \u0010 T/ √ k(Wh Qxi)⊤(Wh Kxr) \u0011, (1) where p ≤ N is the current length of the input and T > 0 is an inverse temperature parameter. 2 Equation (1) is the softmax function, hence with high enough T, it will result in approximately one-hot encoding of the maximum j. The output of the attention block A with H heads is then (t1, . . . ,tp), where each position i is the sum of the attention-weighted values across all heads: A[(W1 Q, . . . ,WH Q ), (W1 K, . . . ,WH K ), (W1 V , . . . ,WH V )](x1, . . . ,xp) = (t1, . . . ,tp), ti = PH h=1 Pp j=1 Ah ijWh V xj. (2) A transformer then applies an MLP to each output of an attention block before passing them to next attention block. We will consider linear layersL[M, b](x)=Mx+b and ReLU-activated linear layers ˆL[M, b](x)= ReLU(Mx+b). When we compose attention blocks and linear or softmax layers, we will implicitly assume that the linear layer is applied to all positions of the sequence. Furthermore, we will use the then operator # for left-to-right function composition. Therefore, a transformer model predicting confidences over the vocabulary can, for example, be represented as: (y1, . . . ,yp) = \u0010 A1 # ˆL1,1 # L1,2 # A2 # ˆL2,1 # L2,2 # softmax \u0011\u0012\u0014 E:,x1 eN (1) \u0015 , . . . , \u0014 E:,xp eN (p) \u0015\u0013 , (3) 1For the first block, din must be de + N but may be different for the deeper blocks. 2A causal model has Aij = 0for j > i. This does not affect our results so we will skip the masking step. 2Published as a conference paper at ICLR 2024 where the output dimension of the last layer has to be V . The next token for a deterministic trans- former is selected to be the last element’s largest logit: xp+1 = arg maxu∈1,...,V yp,u. Given an input (x1, . . . ,xp), the model then autoregressively extends this sequence one token at a time, fol- lowing Equation (3) either until the sequence reaches a lengthN or until a special termination token. A transformer has no separation between the system prompt S, user provided input X and the autoregressively response Y. Thus, a sequence conditional on user input is denoted as (S1, ...,SnS , X1, ...,XnX , Y1, ...,YnY ) and one without user input as (S1, ...,SnS , Y1, ...,YnY ). 2.2 C ONTEXT -BASED FINE -TUNING OF A PRETRAINED MODEL We now define prompting, soft prompting and prefix-tuning with the previously introduced notation. Prompting. The most frequently used content-based fine-tuning approach is prompting: prefix- ing the input (X1, ...,XnX ) with a token sequence S ∈ {1, ..., V}nS to guide the model response: (S1, ...,SnS , X1, ...,XnX ). This is how most people interact with language models such as ChatGPT. Soft prompting. Soft prompting replaces the embeddings of the system input E:,Si with learned vectors si ∈ Rde called virtual tokens (Hambardzumyan et al., 2021; Lester et al., 2021; Qin and Eisner, 2021). Hence, the input in Equation (3) is modified to be: \u0012\u0014 s1 eN (1) \u0015 , . . . , \u0014 snS eN (nS) \u0015 , \u0014 E:,X1 eN (nS + 1) \u0015 , . . . , \u0014 E:,XnX eN (nS + nX) \u0015\u0013 (4) with si chosen to maximize the likelihood of a target response Y =(Y1, ...,YnY ), i.e., arg maxs1,...,snS ∈Rde PnY j=1 log ynS+nX+j,Yj , where ynS+nX+j are autoregressively generated. Prefix-tuning. Prefix-tuning applies soft prompting across the depth of the model (Li and Liang, 2021; Liu et al., 2022). The first nS positions for all attention blocks are learnable parameters, replacing the input (xl1, . . . ,xlnX) for layer l with (sl1, . . . ,slnS,xl1, . . . ,xlnX), where all sl i constitute the prefix. Hence, prefix-tuning can be formulated as arg max{s1 i ,...,sL i }nS i=1 PnY j=1 logynS+nX+j,Yj . Prefix-tuning has been successful at fine-tuning models (Vu et al., 2022; Wu and Shi, 2022; Choi and Lee, 2023; Ouyang et al., 2023; Bai et al., 2023), leading to calls for language models provided as a service (La Malfa et al., 2023) to allow providing prefixes instead of prompts (Sun et al., 2022). Any token-based prompt (S1, ...,SnS ) has a corresponding soft prompt ( si=E:,Si) but the reverse does not hold. Similarly, every soft prompt (s1, ...,snS ) can be represented as a prefix by setting the deeper prefixes to be the values that the model would compute at these positions ( sl i=(A1 # ... # Ll−1,−1)([s⊤ 1 , e⊤ N (1)]⊤, ...,[s⊤ l , e⊤ N (l)]⊤)). The reverse also does not hold: there are prefixes that cannot be represented as a soft prompt. A hierarchy emerges: prompting < soft prompting < prefix-tuning, with prefix-tuning the most powerful of the three. Hence, we focus on examining its performance relative to full fine-tuning but our findings also apply to prompting and soft prompting. 3 S OFT PROMPTING HAS MORE CAPACITY THAN PROMPTING The success of soft prompting (and prefix-tuning) is commonly attributed to the larger capacity of the continuous embeddings compared to the finite tokens. Yet, increased capacity is beneficial only if the model can utilize it. We show this is indeed the case by constructing a transformer generating exponentially more completions by varying a single virtual token than by varying a hard token. Consider unconditional generation (representing a function with no inputs) with a single system token: (Y1, ...,YN )=f(S1)=fS1 . For a deterministic autoregressive function, there are a total of V functions in this family, hence the upper bound on the number of outputs of length N that one can generate by varying the first token S1 is V : the first token fully determines the rest of the sequence. Generally, if one varies the firstNS tokens, there are at mostV NS unique outputs. What if instead of the token S1 we vary a single virtual token s1: (Y1, ...,YN )=f(s1)=fs1 ? This family of functions is indexed by a real vector and hence is infinite: in principle, one could generate all V N possible 3Published as a conference paper at ICLR 2024 output sequences by only controllings1.3 Still, a transformer may not be able to represent a function that achieves that in practice, i.e., it is not obvious if there is a surjective map from{fs1 : s1 ∈ Rde} to {1, ..., V}N . We show that, in fact, there is a transformerf for which such a surjective map exists: Theorem 1 (Exponential unconditional generation capacity of a single virtual token) . For any V, N>0, there exists a transformer with vocabulary size V , context size N, embedding size de=N, one attention layer with two heads and a three-layer MLP such that it generates any token sequence (Y1, ...,YN )∈{1, ..., V}N when conditioned on the single virtual tokens1= ((Y1−1)/V , ...,(YN−1)/V ). However, conditional generation is more interesting: given a user input (X1, ...,XnX ), we want to generate a target response (Y1, ...,YnY ). Even in the simple case of one system token, the user provides one token and the model generates one token in response ( Y1=f(S1, X1)=fS1 (X1)), we cannot control response of the model to any user input with the system token. There are V V maps from X1 to Y1, but S1 can take on only V values: |{fS1 : S1 ∈ 1, ..., V}| = V < VV . Hence, tokens cannot be used to specify an arbitrary map from user input to model output. However, a single virtual token can specify any of the V V maps, i.e., there exists a transformer fs1 (X2) for which there is a surjective map from {fs1 : s1 ∈ Rde} to {1, ..., V}{1,...,V }. Theorem 2 (Conditional generation capacity for a single virtual token (nX=nY =1)). For anyV >0, there exists a transformer with vocabulary size V , context size N=2, embedding size de=V , one at- tention layer with two heads and a three-layer MLP that reproduces any mapm:[1, ..., V]→[1, ..., V] from a user input token to a model response token when conditioned on a single virtual token s1=(m(1)/V , ...,m(V )/V ). That is, by selecting s1 we control the model response to any user input. Theorem 2 builds on Theorem 1 by showing that soft prompting is also more expressive for govern- ing the conditional behavior of a transformer model. This also holds for longer responses nY > 1 by increasing the length of the soft prompt, or longer user inputs nX > 1, by increasing the depth of the model. We provide proofs in Appendix A, as well as working Python implementations. This section showed that soft prompting, and by implication, prefix-tuning, possess greater expres- siveness than prompting. As we can fully determine the map from user input to model response using virtual tokens, our findings may appear to suggest that soft prompting is as powerful as full fine-tuning. However, this is not at all the case. There are structural constraints on the capabilities of soft prompting and prefix-tuning; they cannot facilitate the learning of an entirely new task. The following section elucidates this discrepancy and reconciles these seemingly contradictory results. 4 P REFIX -TUNING CAN ONLY BIAS THE OUTPUT OF AN ATTENTION HEAD We just saw that soft prompting and prefix-tuning can fully control the conditional behavior of a transformer. However, that assumed a specific design for the network weights. Given a fixed pretrained model, as opposed to a manually crafted one, can prefix-tuning be considered equally powerful to full fine-tuning? In this section, we show that, for an arbitrary pretrained model, a prefix S cannot change the relative attention over the content X, Yand can only bias the attention block outputs in a subspace of rank nS, the prefix length, making it less powerful than full fine-tuning. While full fine-tuning can alter the attention pattern of an attention head, prefix-tuning cannot. Recall the attention Aij position i gives to position j for a trained transformer (Equation (1)): Aij = exp \u0000 T/ √ k x⊤ i W⊤ Q WKxj \u0001 Pp r=1 exp \u0010 T/ √ k x⊤ i W⊤ Q WKxr \u0011 = exp \u0000 T/ √ k x⊤ i Hxj \u0001 Pp r=1 exp \u0000 T/ √ k x⊤ i Hxr \u0001, (5) where W⊤ Q WK=H. Full fine-tuning can enact arbitrary changes toWQ and WK and hence, assum- ing the input does not change (e.g., at the first attention layer), we get the following attention: Aft ij = exp \u0000 T/ √ k x⊤ i Hxj + T/ √ k x⊤ i ∆Hxj \u0001 Pp r=1 exp \u0000 T/ √ k x⊤ i Hxr + T/ √ k x⊤ i ∆Hxr \u0001, 3For example, LLaMA-7B (Touvron et al., 2023) has 24 426 unique completions when prompted with each of its 32 000 tokens and we found a non-exhaustive set of 46 812 unique 10-token-long sequences by controlling the first virtual token. Hence, in practice, one can generate more outputs by soft prompting than by prompting. 4Published as a conference paper at ICLR 2024 where the changes to WQ and WK are folded into ∆H. It is clear that by varying ∆H full fine- tuning can change the attention patterns arbitrarily. However, let us see how is attention affected by the presence of a prefix. For now, assume we have a prefix of length one (s1) at position 0. Apt i0= exp(T/ √ k x⊤ i Hs1) exp \u0010 T√ k x⊤ i Hs1 \u0011 + pP r=1 exp \u0010 T√ k x⊤ i Hxr \u0011, Apt ij= exp(T/ √ k x⊤ i Hxj) exp \u0010 T√ k x⊤ i Hs1 \u0011 + pP r=1 exp \u0010 T√ k x⊤ i Hxr \u0011 for j≥1. The numerator ofApt ij is the same as in Equation (5), i.e., the prefix does not affect it. It only adds the term exp(T/√k x⊤ i Hs1) to the denominator. Therefore, the attention position i gives to the content positions j≥1 is simply scaled down by the attention it now gives to the prefix. Iftomato attends the most to salad in a particular context, no prefix can change that. This becomes evident by rewriting Apt ij as the attention of the pretrained model scaled by the attention “stolen” by the prefix: Apt ij = Aij Pp r=1Apt ir = Aij(1 − Apt i0). (6) Hence, prefix-tuning cannot affect the relative attention patterns across the content, it will only scale them down. In other words, one cannot modify what an attention head attends to via prefix-tuning.4 Prefix-tuning only adds a bias to the attention block output. Let us see how this attention scaling down affects the output of the attention block. Following Equation (2), the output at position i for the pretrained (ti), the fully fine-tuned (tft i ) and the prefix-tuned (tpt i ) models are as follows:5 ti = Pp j=1AijWV xj, tft i = Pp j=1Aft ij(WV + ∆WV )xj, tpt i = Apt i0WV s1+ pX j=1 Apt ijWV xj (6) = Apt i0WV s1+ pX j=1 Aij(1-Apt i0)WV xj=Apt i0WV s1+(1-Apt i0)ti. (7) Hence, prefix-tuning only biases the attention block value at each position i towards the constant vector WV s1, which is independent of the content (x1, ...,xp). I.e., the prefix-tuned activation is a linear combination of the pretrained activation and the constant vector WV s1. The content only affects the scale Apt i0 of the bias via the amount of attention on the prefix. In contrast, in full fine- tuning ∆WQ, ∆WK and ∆WV allow for a content-dependent change of the attention and value computation. These results hold for suffix-tuning (placing the prefix after the input) butnot for suffix soft-prompting. We validate that this indeed is the case when prefix-tuning real-world transformers. In Figures 5 and 6, we show that a prefix applied to LLaMA’s first layer does not change the relative attention distribution over the content positions X and results in a bias with a constant direction. Longer prefixes define larger subspaces for the bias but are not fully utilized in practice. In the case of a longer prefix (s1, . . . ,snS ), the bias vector is in a subspace of dimensionality nS: tpt i =PnS j=1Apt i,SjWV sj + (1−PnS j=1Apt i,Sj)ti, where i goes over the content and j over the prefix posi- tions. Larger prefixes thus have a larger subspace to modify the attention block output. The specific direction is determined by the relative distribution of attention across the prefix positions. However, when we examine the distribution of attention across the prefix positions for various inputs as in Appendix B, it appears that the prefixes do not span this subspace. Regardless of the input, the attention Apt i,Sj over the prefix positions remains nearly constant. Thus, prefix-tuning does not seem to make full use of the space that the vectors WV sj span. We hypothesise that this is due to the two competing optimization goals for the vectors sj: at the same time they need to “grab attention” when interacting with WK and determine the bias direction when multiplied with WV . So, is prefix-tuning equivalent to full fine-tuning or is it less powerful than full fine-tuning? In Section 3, we showed that prefix-tuning, in principle, has a large capacity to influence the behavior of the model. But then, in this section, we showed that it has some severe limitations, including not being able to affect the attention pattern and only biasing the attention layer activations. These two results seem to be contradicting one another, so how do we reconcile them? The constructions for the results in Section 3 (described in Appendix A) are simply an algorithm that extracts the completion from a lookup table encoded in the virtual tokens. The attention patterns 4Likhosherstov et al. (2021) show that a fixed attention head can approximate any sparse attention pattern. However, they require control over all the input embeddings while we can only control the prefix ones. 5He et al. (2021a) show a similar analysis but do not study the expressiveness of prefix-tuning. 5Published as a conference paper at ICLR 2024 2 6 0 1 4 6 3 0 1 2 0 0 1 1 2 2 3 4 6 2 6 0 1 4 6 3 0 1 2 0 0 1 1 2 2 3 4 6 Pre�x Pretrained modelon ascending Full �ne-tuningon descending Pre�x-tuningon descending User inputModel response 2 6 0 1 4 6 3 0 1 2 6 6 4 3 2 2 1 1 0 2 6 0 1 4 6 3 0 1 2 6 6 4 3 2 2 1 1 0 2 6 0 1 4 6 3 0 1 2 0 0 0 1 3 3 3 3 3 2 6 0 1 4 6 3 0 1 2 0 0 0 1 3 3 3 3 3 The pretrained model has seen only  sorting in ascending order. Hence,  the single attention head �rst  looks at the smaller numbers  and then to larger ones. Pre�x-tuning, on the other hand, cannot  change the key and value matrices and  the attention pattern. It can only  \"steal\" some attention. That is  why the model still focuses  on the zeros, as with the  pretrained model. Full �ne-tuning can change the key and  value matrices and hence can lead to   new attention patterns. In this case,  it modi�es the model to �rst  look at the larger numbers. Sorted ascending Sorted descending Not sorted Figure 1: Attention patterns of a small transformer pretrained on sorting in ascending order. The model is given the prefix S and user input X and generates Y autoregressively. We have highlighted the attention when the first response Y1 is being generated. Full fine-tuning sorts in descending order but prefix-tuning cannot as it cannot update the learned attention. Note how the relative attention of X to X in the left and right plots is exactly the same: the prefix cannot change the attention pattern for the same inputs. The relative attention of X to X in the center plot is very different because full fine-tuning can arbitrarily change WQ and WK. 2 6 0 1 4 6 3 0 1 2 6 6 4 3 2 2 1 1 0 2 6 0 1 4 6 3 0 1 2 6 6 4 3 2 2 1 1 0 Pre�x Head 1 User inputModel response 2 6 0 1 4 6 3 0 1 2 6 6 4 3 2 2 1 1 0 2 6 0 1 4 6 3 0 1 2 6 6 4 3 2 2 1 1 0 2 6 0 1 4 6 3 0 1 2 6 6 4 3 2 2 1 1 0 2 6 0 1 4 6 3 0 1 2 6 6 4 3 2 2 1 1 0 2 6 0 1 4 6 3 0 1 2 6 6 4 3 2 2 1 1 0 2 6 0 1 4 6 3 0 1 2 6 6 4 3 2 2 1 1 0 Head 2 Head 3 Head 4 First focuses onlarge inputs First focuses onsmall inputsFocuses on theinputs in order Focuses on theinputs in order Figure 2: Model pretrained on the four tasks. The four attention heads specialize in the skills necessary to solve these tasks: look at the elements in order, look first at the smallest elements or first at the largest elements. are simply extracting the current position embedding and the virtual token and hence the attention does not depend on the actual content in the tokens. There is no need to learn a new attention pattern to learn a different map from input to output. 6 Furthermore, the virtual token designates the map precisely by acting as a bias. Therefore, the observations in these two sections do not contradict one another. Soft prompting and prefix-tuning can be on par with full fine-tuning but only in very limited circumstances: when all the knowledge is represented in the virtual token as a lookup table and the model simply extracts the relevant entry. Transformers do not behave like this in practice. Models are typically trained with token inputs rather than virtual tokens. Moreover, if we had a lookup table of the responses to each input we would not need a learning algorithm in the first place. Therefore, the limitations from this section hold for real-world pretrained transformers. Then how come prefix-tuning has been reported to achieve high accuracy and often to be competitive to full fine-tuning? The next section aims to explain when and why prefix-tuning can work in practice. 5 T HE BIAS CAN ELICIT SKILLS FROM THE PRETRAINED MODEL Pretraining potentially exposes a model to different types of completions for the same token se- quence. For a string like I didn’t enjoy the movie , the model may have seen completions such as I found the acting to be sub par , This is negative sentiment or Je n’ai pas aim ´e le film . Hence, a pretrained model could do text completion, sentiment analysis, or translation. Still, the input does not fully determine the desired completion type and the model can generate any one of them. Hence, following our results from Section 4, we hypothesise that prefix-tuning cannot gain new knowledge 6In a follow-up work (Petrov et al., 2024), we utilize this observation to show that, in fact, there exist pretrained weights for which a transformer can be a universal approximator for sequence-to-sequence functions when prefixed. This is not in contradiction with the present results as these transformers can approximate any function without having to modify their attention mechanism. 6Published as a conference paper at ICLR 2024 Figure 3: Attention block activations for ten sequences at the last input position (10) when pretrained on the four tasks. The left plot shows the pretrained activations t10 are not predictive of the completion. The right plot shows prefixes cluster the activations tpt 10. Connecting the pretrained and prefixed activations highlights the bias. No dimen- sionality reduction is used; the clustering is solely due to the prefixes. With pre�xes Without pre�xes (pretrained model) Ascendingpre�x Response sorted ascending Dimension 94 of the residual stream 1.0 1.0 1.5 2.0 0.5 0.5 -1.0 -0.5 -0.5 0.0 0.0 Dimension 94 of the residual stream Dimension 148 of the residual stream Response sorted descending Response increased by 1 Response increased by 2 Descendingpre�x Add 1pre�x Without pre�x(points from the left plot) Add 2pre�x 1.0 1.5 2.00.5 -0.50.0 but can bring to the surface latent knowledge present in the pretrained model.7 We test this hypoth- esis by constructing small transformers trained on one or few tasks. We use a minimal transformer model (Karpathy, 2020) to show that prefix-tuning struggles to learn a new task that full fine-tuning can. Then, that prefix-tuning can easily elicit a latent skill from pretraining. Finally, we show how it can even learn some new tasks, provided they can be solved by combining pretraining skills. Table 1: A transformer pretrained on sorting in ascending order cannot be prefix-tuned to sort in descending order. 10 random seeds. Ascending Descending Pretrain on asc. 91 ±5% 0 ±0% Full fine-tune on desc. 0 ±0% 85 ±5% Prefix-tune on desc. 0 ±0% 0 ±0% Prefix-tuning may not learn a new task requiring a different attention pattern. To check if prefix-tuning can learn a new task, we train a 1-layer, 1-head trans- former to sort numbers into ascending order and then fine-tune it to sort in descending order. During training, the model sees random sequences of 10 digits from 0 to 7 followed by their ascending sorted order. The pre- trained accuracy (fully matching the sorted sequence) is 91%. Full fine-tuning on the descending task leads to 85% test accuracy, hence full fine-tuning successfully learns the new task. However, prefix-tuning with a prefix size nS=1 results in 0% accuracy, hence prefix-tuning fails to learn the new task at all. The attention patterns in Figure 1 show why this is the case: the pretrained model learns to attend first to the smallest numbers and then to the larger ones. When fully fine-tuned, the attention patterns are reversed: they now first attend to the largest values. However, following Section 4, prefix-tuning cannot change the attention pattern over the input sequence and will still attend to the smallest values. Hence, prefix-tuning may indeed struggle to learn a new task requiring new attention patterns. Table 2: A transformer pretrained on several tasks can be prefix-tuned for one of them. 10 random seeds. Accuracy on: ↗ ↘ +1 +2 Pretrained 25 ±13% 25±12% 24±11% 22±7% Prefix-tune on ↗ 95± 2% 0 ± 0% 0 ± 0% 0 ±0% Prefix-tune on ↘ 0± 0% 90 ± 3% 1 ± 1% 1 ±1% Prefix-tune on +1 0± 0% 1 ± 3% 95 ± 6% 0 ±1% Prefix-tune on +2 0± 0% 0 ± 0% 1 ± 2% 98±5% Prefix-tuning can elicit a skill from the pre- trained model. The second part of our hy- pothesis was that prefix-tuning can elicit latent skills in the pretrained model. To test that, we pretrain a 1-layer, 4-head model with solutions sorted in ascending (↗) or descending (↘) or- der, or adding one (+1) or two (+2) to each el- ement of the input sequence. Each solution is shown with 25% probability. The model has no indication of what the task is, hence, it as- signs equal probability to all tasks, as shown in the first row in Table 2. Full fine-tuning for each task naturally results in high accuracy. However, prefix-tuning ( nS=1) can also reach accuracy above 90% for all tasks. Compared to the previous case, prefix-tuning is more successful here because the pretrained model contains the attention mechanisms for solving the four tasks, as shown in Figure 2. If all a prefix does is bias the attention layer activations, how can it steer the model to collapse its distribution onto one task? This is likely due to the attention block solving all tasks in parallel and placing their solutions in different subspaces of the residual stream (intermediate representation, Elhage et al., 2021). As the MLP needs to select one solution to generate, a further indicator on the selected task (or lack of selection thereof) should also be represented. The bias induced by the prefix then acts on this “selection subspace” to nudge the MLP to select the desired solution. 7A similar hypothesis has also been proposed by Reynolds and McDonell (2021) for fine-tuning in general. 7Published as a conference paper at ICLR 2024 This can be clearly seen from the activations of the attention layer at the last input position ( XnX ): the position where the task selection happens as the first output element fully describes the task. Figure 3 shows plots of randomly selected dimensions of the residual stream with and without a prefix. The attention block activations of the pretrained model (without prefix) show no correlation with the output it is about to generate, demonstrating that the choice of completion is indeed not determined by the attention block. However, the prefix-tuned activations for the same inputs are clustered as a result of the prefix-induced bias. This indicates that the bias induced by the prefix may act as a “task selector” of the subspace of the residual stream specializing in the desired task. Prefix-tuning can combine knowledge from pretraining tasks to solve new tasks. Prefix-tuning eliciting one type of completion learned in pretraining starts to explain its practical utility. Still, prefix-tuning seems to be successful also at tasks that the pretrained model has not seen. As we showed above, a model trained to sort in one order cannot be prefix-tuned to sort in the other. Then how is it possible for prefix-tuning to learn a new task? We posit that this can happen, as long as the “skill” required to solve the new task is a combination of “skills” the pretrained model has seen. Table 3: Prefix tuning can learn a new task requiring only pretraining skills ( ↗+1) but cannot learn a completely new task (H). Average accuracy over 3 seeds. Accuracy on: ↗ ↘ +1 +2 ↗+1 H Pretrained 17% 23% 34% 25% 0% 0% Prefix-tune on ↗ 100% 0% 0% 0% 0% 0% Prefix-tune on ↘ 0% 100% 0% 0% 0% 0% Prefix-tune on +1 0% 0% 100% 0% 0% 0% Prefix-tune on +2 0% 0% 0% 100% 0% 0% Prefix-tune on ↗+1 0% 0% 0% 0% 93% 0% Prefix-tune on H 0% 0% 0% 0% 0% 1% We test this by pretraining a 40-layer 4-head model with the same four tasks. We prefix- tune (nS=12) for two new tasks: increment- ing the ascending sorted sequence (↗+1) and double histogram (mapping each element to the number of elements with the same value, e.g., 3,0,0,1 7→1,2,2,1, H). The pretrained model has not seen either task. Prefix-tuning results in 93% accuracy for ↗+1 which is a combination of the ↗ and +1 pretraining tasks and just 1% for the H task which re- quires different skills: finding other instances of the same token and counting. H is not a hard task: it requires 2 layers and 2 heads to be solved exactly (Weiss et al., 2021). Therefore, prefix-tuning is can indeed combine skills that the model has learned in order to solve a novel task but may not learn a completely new task requiring new skills. 6 E FFECTS OF PREFIX -TUNING BEYOND THE SINGLE ATTENTION LAYER Section 4 focused exclusively on a single attention layer. Still, even if a prefix only induces a bias on its output, this bias can exhibit complex behaviors via the subsequent MLPs and attention layers. This section shows how a prefix can change the attention pattern of the following attention layer but only in a linear fashion while full fine-tuning also has bilinear effects. Appendix C further argues that the representational capacity of prefix-tuning may be limited. Therefore, prefix-tuning appears to be less expressive than full fine-tuning, even with the same number of learnable parameters. Prefix-tuning can change the attention, albeit the one of the next layer Let us examine how the prefix of one attention layer affects the following one. Assume no MLPs, residual connections or layer norms: the output t(1) i of the first is the input x(2) i of the second. The pretrained outputs are t(1) i =Pp j=1A(1) ij W(1) V x(1) j , resulting in the second layer attention ˜A(2) ij =T/√k t(1)⊤ i H(2)t(1) j . Here ˜Aij is the pre-softmax attention, i.e., Aij=exp ˜Aij/Pp r=1 exp ˜Air. For prefix-tuning we then have: tpt(1) i =Apt(1) i0 WV s(1) 1 + pX j=1 Apt(1) ij W(1) V x(1) j (7)= Apt(1) i0| {z} αi WV s(1) 1| {z } µ +(1−Apt(1) i0 )t(1) i , ˜Apt(2) ij = T√k tpt(1)⊤ i H(2)tpt(1) j , = T√k(αiαj µ⊤H(2)µ| {z } constant +αj(1-αi)t(1)⊤ i H(2)µ| {z } depends only ont(1) i +αi(1-αj)µ⊤H(2)t(1) j| {z } depends only ont(1) j +(1-αi)(1-αj)t(1)⊤ i H(2)t(1) j| {z } pretrained attention˜A(2) ij ). The presence of µ shows that the prefix of layer 1 can change the attention pattern of the following layer. This change is content-specific: the second and the third terms depend on the inputs, hence a simple bias can affect the attention when passed through MLPs and further attention blocks. Com- pare with Equation (6), which showed a prefix cannot change the attention of the same layer. Still, 8Published as a conference paper at ICLR 2024 even considering this cross-layer effect, prefix-tuning is more limited in its expressiveness than full fine-tuning. While the second and the third terms are input-dependent, each depends on one input position only. The prefix does not change the bilinear dependency on both the query and key. This is something that the full fine-tuning can achieve: ˜Aft(2) ij =T/√k tft(1)⊤ i (H(2)+ ∆H(2))tft(1) j . Even if prefix-tuning could be a universal approximator, it would not be parameter-efficient. Prefix-tuning appears to be less parameter-efficient than other comparable approaches. We designed an experiment to this end. Our pretrained model in Section 5 failed to learn the double histogram task (H). A rank-1 Low Rank Adaptation (LoRA, Hu et al., 2021) applied only to the MLPs in a 4-layer 4-head model pretrained in the exact same way results in 92% accuracy on the H task. The number of parameters for the LoRA fine-tuning is exactly the same as for a prefix of size 12. However, as can be expected from the results in Section 5, training this prefix results in 0% accuracy. Hence, prefix-tuning fails at a task that LoRA with the same number of parameters can learn. In conclusion, while prefix-tuning, prompting, soft prompting and in-context learning have complex effects in deeper models, the interaction of the learnable parameters and the model inputs likely still results in very limited expressiveness. In particular, we demonstrated that LoRA can be used to learn a completely new task while prefix-tuning with the exact same number of parameters fails to. 7 D ISCUSSION AND RELATED WORKS Understanding fine-tuning and prefix-tuning. Prior works show that prefixes have low intrinsic dimension allowing transfer to similar tasks and initialization of prefixes for new tasks (Qin et al., 2021; Su et al., 2022; Zhong et al., 2022; Wang et al., 2022b; Zheng et al., 2023). In this work, we offered theoretical insights into their results: this subspace is the span of the prefix-induced bias. Another line of work shows that skills can be localized in the parameter space of pretrained models (Wang et al., 2022a; Panigrahi et al., 2023). Here, we showed that it is also possible to identify subspaces of the residual stream corresponding to individual tasks and select them via prefix-tuning. Prompting and in-context learning. Prompting and in-context learning are a special case of prefix-tuning. Therefore, the limitations and mechanisms discussed in this work apply to prompt- ing as well: prompts cannot change the distribution of attention of the first attention layer over the content following it and can only induce a bias on the output of this layer (Section 4). Even consid- ering the cross-layer effects, a prompt is strictly less expressive than full fine-tuning (Section 6) and prompting is unlikely to enable the model to solve a completely new task. Our theory thus explains why Kossen et al. (2023) observed that in-context examples cannot overcome pre-training skills. While context-based fine-tuning approaches may not learn arbitrary new tasks, as shown in Sec- tion 5, they can leverage pre-trained skills. Wies et al. (2023) have PAC-learnability results that also show that when pretraining is on a mixture tasks, they can be efficiently learned via in-context learning, Moreover, transformers can learn linear models in-context by mimicking gradient descent (V on Oswald et al., 2023) or approximating matrix inversion (Aky¨urek et al., 2022). This is consis- tent with our theory: the prediction updates are enacted as biases in the attention block activations. Hence, despite the limitations discussed in this work, context-based methods can result in powerful fine-tuning if the pretrained model has “transferable skills” such as algorithmic fundamentals. Still, in-context learning will likely fail for non-algorithmic tasks, e.g., translating to a language that the model has never seen before, even if large number of translation pairs are provided in-context. Implications for catastrophic forgetting and model alignment. The lack of expressiveness of context-based fine-tuning can be a feature: desirable properties will be maintained. Full fine-tuning can result in catastrophic forgetting (He et al., 2021b; Luo et al., 2023; Mukhoti et al., 2023). Our theory shows that context-based methods won’t lose pretrained skills. Model alignment poses the reverse problem: ensuring that the model cannot pick up undesirable skills during fine-tuning Our results show that prompting and prefix-tuning might be unable to steer the model towards new ad- versarial behaviors. Hence, the recent successes in adversarial prompting (Zou et al., 2023) indicate that current model alignment methods just mask the undesirable skills rather than removing them. Implications for model interpretability. An open question for language model interpretability is whether attention is sufficient for explainability (Jain and Wallace, 2019; Wiegreffe and Pinter, 2019). Section 5 points toward the negative: by interfering in the output of the attention layer with the bias induced by a prefix, we can change the behavior of the model, without changing its attention. 9Published as a conference paper at ICLR 2024 On the flip side, prefix-tuning can be used to understand what “skills” a model has: if prefix-tuning for a task fails, then the model likely lacks one of the key “skills” for that task. Limitations. The present analysis is largely limited to prefixing with prompts, soft prompts and for prefix-tuning. While our theoretical results hold for suffix-tuning, they do not necessarily apply to suffixing with prompts or soft prompts. That is because the deeper representations for prompt and soft prompt suffixes would depend on the previous positions. This does not apply to suffix-tuning as it fixes all intermediate representations. Therefore, whether suffixing is more expressive than prefixing remains an open question. Separately, while we provided evidence towards context-based fine-tuning methods being parameter inefficient learners, the formal analysis of the conditions under which they may be universal approximators remain an open question. Finally, we mostly considered simple toy problems. In practice, however, language models are pretrained with very large datasets and can pick up very complex behaviors. Hence, the extent to which the limitations we demonstrated apply to large-scale pretrained transformers also remains for future work. 8 C ONCLUSION This paper formally showed that fine-tuning techniques working in embedding space, such as soft prompting and prefix-tuning, are strictly more expressive than prompting which operates in the discrete token space. However, we then demonstrated that despite this larger expressivity, prefix- tuning suffers from structural limitations that prevent it from learning new attention patterns. As a result, it can only bias the output of the attention layer in a direction from a subspace of rank equal to the size of the prefix. We showed that this results in practical limitations by constructing minimal transformers where prefix tuning fails to solve a simple task. This result seems to be at odds with the empirical success of prefix-tuning. We provided explanations towards that. First, we showed that prefix-tuning can easily elicit a skill the pretrained model already has and can even learn a new task, if it has picked up the skills to solve it during pretraining. Second, we showed that the effect of the prefix-induced bias is more complicated and powerful when combined with downstream non-linear operations. However, it appears to be still less expressive than full fine-tuning. REPRODUCIBILITY STATEMENT In order to facilitate the reproduction of our empirical results, validating our theoretical results, and further studying the properties of context-based fine-tuning, we release all our code and resources used in this work. Furthermore, in Appendix A we offer explicit constructions of transformers with the properties discussed in Section 3. We also provide Python implementations of these construc- tions that validate their correctness. ACKNOWLEDGEMENTS This work is supported by a UKRI grant Turing AI Fellowship (EP/W002981/1) and the EPSRC Centre for Doctoral Training in Autonomous Intelligent Machines and Systems (EP/S024050/1). AB has received funding from the Amazon Research Awards. We also thank the Royal Academy of Engineering and FiveAI. REFERENCES Ekin Aky¨urek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. 2022. What learn- ing algorithm is in-context learning? Investigations with linear models. In International Confer- ence on Learning Representations. Jiaqi Bai, Zhao Yan, Jian Yang, Xinnian Liang, Hongcheng Guo, and Zhoujun Li. 2023. Knowprefix-tuning: A two-stage prefix-tuning framework for knowledge-grounded dialogue gen- eration. arXiv preprint arXiv:2306.15430. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in Neural Information Processing Systems. 10Published as a conference paper at ICLR 2024 YunSeok Choi and Jee-Hyong Lee. 2023. CodePrompt: Task-agnostic prefix tuning for program and language generation. In Findings of the Association for Computational Linguistics: ACL 2023. Ana Santos Costa, Montserrat Comesa ˜na, and Ana Paula Soares. 2022. PHOR-in-One: A mul- tilingual lexical database with PHonological, ORthographic and PHonographic word similarity estimates in four languages. Behavior Research Methods. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Confer- ence of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. 2021. A mathematical framework for transformer circuits. Transformer Circuits Thread. Karen Hambardzumyan, Hrant Khachatrian, and Jonathan May. 2021. W ARP: Word-level Adver- sarial ReProgramming. In Proceedings of the 59th Annual Meeting of the Association for Compu- tational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). Mohamad H Hassoun. 1995. Fundamentals of artificial neural networks. MIT press. Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, and Graham Neubig. 2021a. Towards a unified view of parameter-efficient transfer learning. In International Conference on Learning Representations. Tianxing He, Jun Liu, Kyunghyun Cho, Myle Ott, Bing Liu, James Glass, and Fuchun Peng. 2021b. Analyzing the forgetting problem in pretrain-finetuning of open-domain dialogue response mod- els. In Proceedings of the 16th Conference of the European Chapter of the Association for Com- putational Linguistics: Main Volume. Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, An- drea Gesmundo, Mona Attariyan, and Sylvain Gelly. 2019. Parameter-efficient transfer learning for NLP. In International Conference on Machine Learning. Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations. Zhiqiang Hu, Yihuai Lan, Lei Wang, Wanyu Xu, Ee-Peng Lim, Roy Ka-Wei Lee, Lidong Bing, and Soujanya Poria. 2023. LLM-Adapters: An adapter family for parameter-efficient fine-tuning of large language models. arXiv preprint arXiv:2304.01933. Sarthak Jain and Byron C. Wallace. 2019. Attention is not explanation. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361. Andrej Karpathy. 2020. minGPT GitHub Repository. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. Advances in Neural Information Processing Systems. Jannik Kossen, Tom Rainforth, and Yarin Gal. 2023. In-context learning in large language models learns label relationships but is not conventional learning. arXiv preprint arXiv:2307.12375. 11Published as a conference paper at ICLR 2024 Emanuele La Malfa, Aleksandar Petrov, Christoph Weinhuber, Simon Frieder, Ryan Burnell, An- thony G. Cohn, Nigel Shadbolt, and Michael Wooldridge. 2023. The ARRT of Language-Models- as-a-Service: Overview of a new paradigm and its challenges. arXiv preprint arXiv:2309.16573. Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The power of scale for parameter-efficient prompt tuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Lan- guage Processing. Xiang Lisa Li and Percy Liang. 2021. Prefix-Tuning: Optimizing continuous prompts for generation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). Vladislav Lialin, Vijeta Deshpande, and Anna Rumshisky. 2023. Scaling down to scale up: A guide to parameter-efficient fine-tuning. arXiv preprint arXiv:2303.15647. Valerii Likhosherstov, Krzysztof Choromanski, and Adrian Weller. 2021. On the expressive power of self-attention matrices. arXiv preprint arXiv:2106.03764. Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. 2023. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. ACM Computing Surveys. Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Tam, Zhengxiao Du, Zhilin Yang, and Jie Tang. 2022. P- Tuning: Prompt tuning can be comparable to fine-tuning across scales and tasks. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). Yun Luo, Zhen Yang, Fandong Meng, Yafu Li, Jie Zhou, and Yue Zhang. 2023. An empirical study of catastrophic forgetting in large language models during continual fine-tuning. arXiv preprint arXiv:2308.08747. Bonan Min, Hayley Ross, Elior Sulem, Amir Pouran Ben Veyseh, Thien Huu Nguyen, Oscar Sainz, Eneko Agirre, Ilana Heintz, and Dan Roth. 2021. Recent advances in natural language processing via large pre-trained language models: A survey. ACM Computing Surveys. Jishnu Mukhoti, Yarin Gal, Philip H. S. Torr, and Puneet K. Dokania. 2023. Fine-tuning can cripple your foundation model; preserving features may be the solution. arXiv preprint arXiv:2308.13320. Linyong Nan, Dragomir Radev, Rui Zhang, Amrit Rau, Abhinand Sivaprasad, Chiachun Hsieh, Xiangru Tang, Aadit Vyas, Neha Verma, Pranav Krishna, Yangxiaokang Liu, Nadia Irwanto, Jessica Pan, Faiaz Rahman, Ahmad Zaidi, Mutethia Mutuma, Yasin Tarabar, Ankit Gupta, Tao Yu, Yi Chern Tan, Xi Victoria Lin, Caiming Xiong, Richard Socher, and Nazneen Fatema Rajani. 2021. DART: Open-domain structured data record to text generation. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Yawen Ouyang, Yongchang Cao, Yuan Gao, Zhen Wu, Jianbing Zhang, and Xinyu Dai. 2023. On prefix-tuning for lightweight out-of-distribution detection. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Abhishek Panigrahi, Nikunj Saunshi, Haoyu Zhao, and Sanjeev Arora. 2023. Task-specific skill localization in fine-tuned language models. In International Conference on Machine Learning. Aleksandar Petrov, Philip HS Torr, and Adel Bibi. 2024. Prompting a pretrained transformer can be a universal approximator. arXiv preprint arXiv:2402.14753. Guanghui Qin and Jason Eisner. 2021. Learning how to ask: Querying LMs with mixtures of soft prompts. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 12Published as a conference paper at ICLR 2024 Yujia Qin, Xiaozhi Wang, Yusheng Su, Yankai Lin, Ning Ding, Jing Yi, Weize Chen, Zhiyuan Liu, Juanzi Li, Lei Hou, et al. 2021. Exploring universal intrinsic task subspace via prompt tuning. arXiv preprint arXiv:2110.07867. Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Lan- guage models are unsupervised multitask learners. Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, et al. 2021. Scaling language models: Methods, analysis & insights from training Gopher. Sylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea Vedaldi. 2017. Learning multiple visual domains with residual adapters. In Advances in Neural Information Processing Systems. Laria Reynolds and Kyle McDonell. 2021. Prompt programming for large language models: Beyond the few-shot paradigm. In Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems. Elvis Saravia, Hsien-Chi Toby Liu, Yen-Hao Huang, Junlin Wu, and Yi-Shin Chen. 2018. CARER: Contextualized affect representations for emotion recognition. In Proceedings of the 2018 Con- ference on Empirical Methods in Natural Language Processing. Taylor Shin, Yasaman Razeghi, Robert L. Logan IV , Eric Wallace, and Sameer Singh. 2020. Auto- Prompt: Eliciting knowledge from language models with automatically generated prompts. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). Mirko Solazzi and Aurelio Uncini. 2004. Regularising neural networks using flexible multivariate activation function. Neural Networks, 17(2):247–260. Yusheng Su, Xiaozhi Wang, Yujia Qin, Chi-Min Chan, Yankai Lin, Huadong Wang, Kaiyue Wen, Zhiyuan Liu, Peng Li, Juanzi Li, Lei Hou, Maosong Sun, and Jie Zhou. 2022. On transferability of prompt tuning for natural language processing. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Tianxiang Sun, Yunfan Shao, Hong Qian, Xuanjing Huang, and Xipeng Qiu. 2022. Black-box tuning for language-model-as-a-service. In International Conference on Machine Learning. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth ´ee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. LLaMA: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems. Johannes V on Oswald, Eyvind Niklasson, Ettore Randazzo, Joao Sacramento, Alexander Mordvint- sev, Andrey Zhmoginov, and Max Vladymyrov. 2023. Transformers learn in-context by gradient descent. In International Conference on Machine Learning. Tu Vu, Brian Lester, Noah Constant, Rami Al-Rfou’, and Daniel Cer. 2022. SPoT: Better frozen model adaptation through soft prompt transfer. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Xiaozhi Wang, Kaiyue Wen, Zhengyan Zhang, Lei Hou, Zhiyuan Liu, and Juanzi Li. 2022a. Find- ing skill neurons in pre-trained transformer-based language models. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. Zhen Wang, Rameswar Panda, Leonid Karlinsky, Rogerio Feris, Huan Sun, and Yoon Kim. 2022b. Multitask prompt tuning enables parameter-efficient transfer learning. In International Confer- ence on Learning Representations. 13Published as a conference paper at ICLR 2024 Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. 2021. Finetuned language models are zero-shot learners. In International Conference on Learning Representations. Gail Weiss, Yoav Goldberg, and Eran Yahav. 2021. Thinking like transformers. In International Conference on Machine Learning. Sarah Wiegreffe and Yuval Pinter. 2019. Attention is not not explanation. InProceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). Noam Wies, Yoav Levine, and Amnon Shashua. 2023. The learnability of in-context learning.arXiv preprint arXiv:2303.07895. Hui Wu and Xiaodong Shi. 2022. Adversarial soft prompt tuning for cross-domain sentiment analy- sis. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Yuanhang Zheng, Zhixing Tan, Peng Li, and Yang Liu. 2023. Black-box prompt tuning with sub- space learning. arXiv preprint arXiv:2305.03518. Qihuang Zhong, Liang Ding, Juhua Liu, Bo Du, and Dacheng Tao. 2022. PANDA: Prompt transfer meets knowledge distillation for efficient model adaptation. arXiv preprint arXiv:2208.10160. Andy Zou, Zifan Wang, J Zico Kolter, and Matt Fredrikson. 2023. Universal and transferable adver- sarial attacks on aligned language models. arXiv preprint arXiv:2307.15043. 14Published as a conference paper at ICLR 2024 A C ONSTRUCTING TRANSFORMERS THAT UTILIZE THE CAPACITY OF THE EMBEDDING SPACE A.1 U NCONDITIONAL GENERATION FOR A SINGLE VIRTUAL TOKEN This section provides an explicit construction of a transformer with the properties described in The- orem 1. The goal is to construct a transformer that, by varying the choice of the virtual token, can generate any sequence of N tokens. First, we need to specify how we encode the target sequence(Y1, . . . ,YN ) into the virtual token s1. We chose the size of the embedding (and hence of s1) to be N. This way, each element of s1 can represent one position of the target sequence. We then represent the token value by discretizing each element of s1 into V levels: s1 = ((Y1−1)/V , . . . ,(YN−1)/V ) . Note that this means that each element of s1 is in [0, 1). When predicting the token for the i + 1 position, the transformer needs to pick the i-th element of s1, and then decode the corresponding value as a one-hot encoding representing the Yi-th token. We extract the i-th element of s1 using one attention block of two heads. The fst head always looks at the first position which is our virtual token s1. For that purpose we create an attention head that always has Afst ij = 1 if j = 1 and Afst ij = 0 otherwise together with a value matrix Wfst V that extracts the embedding. This is achieved with Wfst Q = [0N , 1N ], Wfst K = [0N , 1, 1N−1], Wfst V = [IN , 0N×N ], (8) and a sufficiently high inverse temperature parameter T. The pos head instead extracts the one-hot encoding of the current position. This can be done with an attention head that always attends only to the current position and a value matrix Wpos V that extracts the position embedding as a one-hot vector: Wpos Q = [0N×N , IN ], Wpos K = [0N×N , IN ], Wpos V = [0N×N , IN ]. (9) When the outputs of these two attention heads are summed, then only the element of s1 that corre- sponds to the current position will be larger than 1. From Equation (2) the output at thei-th position of the attention block is: ti = pX j=1 Afst ij xj + pX j=1 Apos ij eN (j) = s1 + eN (i), where x1 = s1 and xj = E:,Yj−1 for j >1. We can extract the value of s1 corresponding to the current position by substracting 1 from the hidden state and apply ReLU: ˆLex = ˆL[IN , −1N ]. Now, we are left with only one non-zero entry and that’s the one corresponding to the next token. We can retain only the non-zero entry if we just sum all the entries of the hidden state with ˆLsum = ˆL[1⊤ N , 0]. The final step is to map this scalar to a V -dimensional vector which has its maximum value at index Yi. This task is equivalent to designing V linear functions, each attaining its maximum at one of 0, 1/V , . . . ,(V −1)/V . To construct this, we use the property of convex functions that their tangent is always under the plot of the function. Therefore, given a convex functionγ(x), we construct the i-th linear function to be simply the tangent of γ at i−1/V . If we take γ(x) = (x − 1/2)2, this results in the following linear layer: Lproj = L \"\u00142(1 − 1) V − 1, . . . ,2(V − 1) V − 1 \u0015T , \u00141 4 − (1 − 1)2 V 2 , . . . ,1 4 − (V − 1)2 V 2 \u0015⊤# . (10) Figure 4 shows the predictors for each individual token id. With just two attention heads and three linear layers, the transformer A[(Wfst Q , Wpos Q ), (Wfst K , Wpos K ), (Wfst V , Wpos V )] # ˆLex # ˆLsum # Lproj # softmax achieves the up- per bound of V N unique outputs by controlling a single virtual token at its input. Note that for this 15Published as a conference paper at ICLR 2024 0/V 2/V 4/V 6/V 8/V V /V Logit for token 1 (largest token when the input is 0/V) Logit for token 8 (largest token when the input is 7/V) input output Figure 4: Illustration of the predictors for each token in the Lproj linear layer for V = 10. The layer is constructed in such a way that the i-th token has the highest confidence when the input is i−1/V . construction, the choice of embedding matrix E ∈ RN×V does not matter. The same transformer architecture can generate onlyV unique outputs if we only control the first token instead. Therefore, it is indeed the case that the embedding space has exponentially more capacity for control than the token space. You can see this transformer implemented and running in practice in Section 2 of this notebook. A.2 C ONDITIONAL GENERATION FOR A SINGLE VIRTUAL TOKEN (nX = nY = 1) This section provides an explicit construction of a transformer with the properties described in The- orem 2. The goal is to construct a transformer that, by varying the choice of the virtual token, can cause the model to act as any map m : [1, . . . , V] → [1, . . . , V]. In other words, by selecting the virtual token, we can fully control how the model will respond to any token the user may provide. First, we need to specify how the map m will be encoded in the virtual token s1. We choose the embedding size de to be V . Now, we can use the same encoding scheme as before, but now each element in s1 corresponds to a different user token, rather than to a position in the generated sequence: s1 = (m(1)/V , . . . ,m(V )/V ). Therefore, the first element of s1 designates the response if the user provides token 1, the second element is the response to the token 2, and so on. Extracting the Yi-th value from s1 and decoding it can be done in a very similar way as for the unconditional case. The only difference is that instead of looking at the user input position, we look at its value. Take E = IV and N = 2. Hence we have the following val head (only differing in the WV matrix from Equation (9)): Wval Q = [02×V , I2], Wval K = [02×V , I2], Wval V = [IV , 0V ×2]. We also need embedding of the first token, so we have a modified version of Equation (8): Wfst Q = [0V , 1, 1], Wfst K = [0V , 1, 0], Wfst V = [IV , 0V ×2]. And hence the output of this attention block at the second position would be: t2 = 2X j=1 Afst ij xj + 2X j=1 Aval ij Wfst V xj = s1 + eV (Y1). Similarly to the unconditional case, only the entry of t2 corresponding to the user token will have a value above 1 and that value would be 1 + m(x1)/V . 16Published as a conference paper at ICLR 2024 We can now extract the one-hot representation of the target token using the same approach as before, just adjusting for the different hidden state size: ˆLex = ˆL[IV , −1V ], ˆLsum = ˆL[1⊤ V , 0], and the same projection had as before (Equation (10)). The final transformer is then: A[(Wfst Q , Wval Q ), (Wfst K , Wval K ), (Wfst V , Wval V )] # ˆLex # ˆLsum # Lproj # softmax . You can see this trans- former implemented and running in practice in Section 3 here. A.3 C ONDITIONAL GENERATION FOR LONGER RESPONSES (nX = 1, nY > 1) We can obtain longer responses via a simple extension. If the response length is N0, then we can encode the map m : [1, . . . , V] → [1, . . . , V]N0 in N0 virtual tokens, each corresponding to one of the target positions: si = (m(1)i/V , . . . ,m(V )i/V ) for i = 1, . . . , N0. For this model we would then have N = 2N0 and de = V . First, we need a head that always looks at the token provided by the user, which will be at position No + 1: Wuser Q = [0V , 1N ], Wuser K = [0(V +No), 1, 0(No−1)], Wuser V = [IV , 0V ×N ]. In order to consume the map at the right location, we need to also look at the embedding of the token No positions before the one we are trying to generate: Wback Q = \u0014 0N0×(N0+V ) IN0 0N0×(N0+V ) 0N0×N0 \u0015 , Wback K = [0N×V , IN ], Wback V = [IV , 0V ×N ]. From here on, the decoding is exactly the same as in the nX = nY = 1 case. The final transformer is then: A[(Wuser Q , Wback Q ), (Wuser K , Wback K ), (Wuser V , Wback V )] # ˆLex # ˆLsum # Lproj # softmax . You can see this transformer implemented and running in practice in Section 4 here. A.4 C ONDITIONAL GENERATION FOR LONGER USER INPUTS (nX > 1, nY = 1) Finally, we consider the case when the user inputX is longer. This is a bit more complicated because we need to search through a domain of sizeV V . We will only consider the case withnX = 2 where we would need two attention layers. A similar approach can be used to construct deeper models for nX > 2. Finally, combining the strategy in the previous section for longer responses with the strategy in this section for longer user inputs allows us to construct transformers that map from arbitrary length user strings to arbitrary length responses. In order to encode a map m : [1, . . . , V]2 → [1, . . . , V] into a single virtual token we would need a more involved construction than before. Similarly to how we discretized each element of the virtual token s1 in V levels before, we are going to now discretize it into V V levels. Each one of these levels would be one of the V V possible maps from the second user token to the response. The first user token would be used to select the corresponding element of s1. Then this scalar will be “unpacked” into a new vector of V elements using the first attention block. Then, the second user token will select an element from this unpacked vector, which will correspond to the target token. We construct the virtual token as follows: s1 = \" VX i=1 m1(i) × V i−1 V V , . . . , VX i=1 mV (i) × V i−1 V V # , where mf (x) = m(f, x) is a map from the second user token to the response when the first token is fixed to be f. An additional change from the previous constructions is that we are going to divide the residual stream into two sections. This is in line with the theory that different parts of the residual stream specialize for different communications needs by different attention heads (Elhage et al., 2021). We will use the first half of the residual stream to extract and “unpack” the correct mapping from second token to target token, while the second half of the residual stream will be used to copy the second token value so that the second attention layer can use it to extract the target. As usual, the embedding 17Published as a conference paper at ICLR 2024 matrix will be the identity matrix: E = IV . Finally, for convenience, we will also use a dummy zero virtual token that we will attend to when we want to not attend to anything. This results in context size N = 4 with the input being \u0012\u0014 0V eN (1) \u0015 , \u0014 s1 eN (2) \u0015 , \u0014 E:,X1 eN (3) \u0015 , \u0014 E:,X2 eN (4) \u0015\u0013 = \u0012\u0014 0V eN (1) \u0015 , \u0014 s1 eN (2) \u0015 , \u0014 eV (X1) eN (3) \u0015 , \u0014 eV (X2) eN (4) \u0015\u0013 . We want the output at the last position to be the target m(X1, X2), that is: arg max u∈1,...,V y4,u = m(X1, X2) for any m, X1, X2. The first attention block will have three attention heads. As before, we want to extract the value of s1 that corresponds to the first token the user provided (X1) and place it in the first half of the residual stream. We want only the third position to do that, while the rest of the positions keep the first half of their residual stream with zeros. Hence we have the following fst head: Wfst Q = \u0014 02×V 1 1 0 1 0 0 1 0 \u0015 , Wfst K = \u0014 02×V 1 0 0 0 0 1 0 0 \u0015 , Wfst V = \u0014 IV 0V ×N 0V ×V 0V ×N \u0015 . The user1 head extracts the value of the first user-provided token (X1) and also places it in the first half of the residual stream: Wuser1 Q = \u0014 02×V 1 1 0 1 0 0 1 0 \u0015 , Wuser1 K = \u0014 02×V 1 0 0 0 0 0 1 0 \u0015 , Wuser1 V = \u0014 IV 0V ×N 0V ×V 0V ×N \u0015 . And the user2 head does the same for the value of the second user-provided token (X2), placing it in the second half of the residual stream: Wuser2 Q = \u0014 02×V 1 1 1 0 0 0 0 1 \u0015 , Wuser2 K = \u0014 02×V 1 0 0 0 0 0 0 1 \u0015 , Wuser2 V = \u00140V ×V 0V ×N 2IV 0V ×N \u0015 , where the factor 2 is there because, as usual, the first linear layer will subtract 1 from everything in order to extract the value selected by the first token. This linear layer looks as usual: ˆLex2 = ˆL[I2V , −12V ]. The result is that the first V elements will be 0 except one which designates which map from second user token to output we should use, and the second V elements have a one hot-encoding of the second user token. Constructing an MLP that unpacks the mapping can become quite involved so we do not provide an explicit form for it. But from the universal approximation theorems and the finiteness of the domain and range, we know that such an MLP should exist. We thus designate by unpack the MLP that decodes the first half of the residual stream to: \u0012mX1 (1) V , . . . ,mX1 (V ) V \u0013 and keeps the second half unchanged. And now, by using two attention heads, the second attention block extracts the value of the above vector at the position designated by the second token, in a fashion not dissimilar to all the previous cases: Wemb Q = [0⊤ V , 1⊤ V ], Wemb K = [1⊤ V , 0⊤ V ], Wemb V = [IV 0V ×V ] , Wuser2’ Q = [0⊤ V , 1⊤ V ], Wuser2’ K = [0⊤ V , 1⊤ V ], Wuser2’ V = [0V ×V IV ] , And finally, with ˆLex = ˆL[IV , −1V ], ˆLsum = ˆL[1⊤ V , 0], and the same projec- tion had as before (Equation (10)), we get the target token. The final transformer is then: A[(Wfst Q , Wuser1 Q , Wuser2 Q ), (Wfst K , Wuser1 K , Wuser2 K ), (Wfst V , Wuser1 V , Wuser2 V )] # ˆLex2 # unpack # A[(Wemb Q , Wuser2’ Q ), (Wemb K , Wuser2’ K ), (Wemb V , Wuser2’ V )] # ˆLex # ˆLsum # Lproj # softmax . You can see this transformer implemented and running in practice in Section 5 here. 18Published as a conference paper at ICLR 2024 T AB LE : Fo u r th Ro u n d Qu a l ify in g : NEW_ ENT RI ES_ TH IS_ RO UND : 2 4 T EXT : Fo u r th ro u n d q u a l ify in g h a d 2 4 n e w e n trie s . T AB LE : Fo u r th Ro u n d Qu a l ify in g : NEW _ ENT RI ES _ TH IS _ RO UND : 2 4 T EXT : Fo u r th ro u n d q u a l ify in g h a d 2 4 n e w e n trie s . T AB LE : Fo u r th Ro u n d Qu a l ify in g : NEW_ ENT RI ES_ TH IS_ RO UND : 2 4 T EXT : Fo u r th ro u n d q u a l ify in g h a d 2 4 n e w e n trie s . T AB LE : Fo u r th Ro u n d Qu a l ify in g : NEW _ ENT RI ES _ TH IS _ RO UND : 2 4 T EXT : Fo u r th ro u n d q u a l ify in g h a d 2 4 n e w e n trie s . T AB LE:Fo u rthRo u n dQu a lify in g:NEW_ ENTRIES_ THIS_ ROUND: 24TEXT:Fo u rthro u n dq u a lify in gh a d 24n e we n trie s. T AB LE : Fo u r th Ro u n d Qu a l ify in g : NEW _ ENT RI ES _ TH IS _ RO UND : 2 4 T EXT : Fo u r th ro u n d q u a l ify in g h a d 2 4 n e w e n trie s . Attention for layer 1, head 12 of LLaMA with a pre�x Attention for layer 1, head 12 of LLaMA with a pre�x normalized over the content Attention for layer 1, head 12 of LLaMA without a pre�x Figure 5: The attention of the twelfth head of the first layer of LLaMA (Touvron et al., 2023). The left plot shows the attention with a prefix of length one. The second plot shows the same attention but normalized such that the attenion over the non-prefix positions sums to 1. The right plot shows the at- tention of the pre-trained model (without prefix). The center and the right plots are the same, illustrat- ing that the presence of the prefix indeed only scales down the attention over the content (non-prefix positions) but does not change its relative distribution, providing empirical validation of Equation (6). The test sequence is TABLE: Fourth Round Qualifying : NEW ENTRIES THIS ROUND : 24 TEXT: Fourth round qualifying had 24 new entries. from the DART table-to-test dataset (Nan et al., 2021). 0 5 1 0 1 5 2 0 2 5 3 0 3 5 4 0 4 5 5 0 5 5 6 0 6 5 7 0 7 5 8 0 8 5 9 0 9 5 1 0 0 1 0 5 1 1 0 1 1 5 1 2 0 1 2 5 024681 01 21 41 61 82 02 22 42 62 83 03 23 43 6 0 5 1 0 1 5 2 0 2 5 3 0 3 5 4 0 4 5 5 0 5 5 6 0 6 5 7 0 7 5 8 0 8 5 9 0 9 5 1 0 0 1 0 5 1 1 0 1 1 5 1 2 0 1 2 5 Actual pre�xed activations Input position Residual stream Residual stream Predicted activations Figure 6: The activations of the twelfth head of the first layer of LLaMA (Touvron et al., 2023). The left plot shows the activations in the presence of the prefix. The right plot shows the activations ti of the pretrained model, scaled by one minus the attention that the prefix would take and then biased in the direction WV s1. The two plots are the same, illustrating that our theory, Equation (7) in particular, also holds for real-world large transformer models. The test sequence is the same as in Figure 5. 19Published as a conference paper at ICLR 2024 Pre�x 1 Pre�x 2 Pre�x 3 Pre�x 4 Pre�x 5 Pre�x 6 Pre�x 7 Pre�x 8 Pre�x 9 Pre�x 10 Pre�x 1 Pre�x 2 Pre�x 3 Pre�x 4 Pre�x 5 Pre�x 6 Pre�x 7 Pre�x 8 Pre�x 9 Pre�x 10 Pre�x 1 Pre�x 2 Pre�x 3 Pre�x 4 Pre�x 5 Pre�x 6 Pre�x 7 Pre�x 8 Pre�x 9 Pre�x 10 Attention of layer 1 0.0 0.2 0.4 0.6 0.8 1.0 Attention of layer 5 0.0 0.2 0.4 0.6 0.8 1.0 Attention of layer 9 0.0 0.2 0.4 0.6 0.8 1.0 Attention of layer 2 Attention of layer 6 0.0 0.2 0.4 0.6 0.8 1.0 Attention of layer 10 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 Attention of layer 3 Attention of layer 7 0.0 0.2 0.4 0.6 0.8 1.0 Attention of layer 11 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 Attention of layer 4 Attention of layer 8 0.0 0.2 0.4 0.6 0.8 1.0 Attention of layer 12 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 Figure 7: The range of attention (1st to 99th percentile) for a single GPT-2 (Radford et al., 2019) prefix trained on the Emotion dataset (Saravia et al., 2018). The prefix is of size 10 ( nS = 10). This is the attention of the last user input token ( nX) because this is the position at which the class prediction is done. For illustration purposes, we have normalized the attention so that the attention over the 10 prefix positions sums to 1. The range of attention over the 10 positions for each layer are shown. B A TTENTION DISTRIBUTION OVER THE PREFIX As discussed in Section 4, longer prefixes define a subspace from which the bias for the attention block is selected. For a prefix of size nS, that means that this subspace is nS-dimensional. Each of prefix position j defines a basis vector WV sj for this subspace, while the attention Apt i,Sj on this position determines how much of this basis component contributes to the bias. In ordered to span the whole subspace and make full use of the capacity of the prefix, Apt i,Sj should vary between 0 and 1 for different inputs. However, we observe that this does not happen in prac- tice. Figure 7 shows the ranges of attention the different prefix positions take for the GPT-2 model (Radford et al., 2019). For layer 1, for example, the attention each prefix positions gets is almost constant hence, the effective subspace is collapsed and there is a single bias vector that’s applied to the attention layer output, regardless of the user input X. Some other layers show slightly higher variation. For example, layer 3 has three prefix positions with large variations. Therefore, the effective bias subspace is 3-dimensional and the user input X governs which bias vector from this subspace will be selected. 20Published as a conference paper at ICLR 2024 linear linear + exp nonlinear ﬁxed MLP ﬁxed MLP ﬁxed feature extraction linear linear + exp ﬁxed feature extraction linear vector linear linear + exp ﬁxed feature extraction scalar linear linear + exp ﬁxed feature extraction ﬁxed MLP ﬁxed MLP Figure 8: Prefix-tuning as a neural network architecture. While linearities and non-linearities are present, the only learnable parameters s(1), s(2), ...have limited interaction with the inputs x1 and x2. The interaction of the prefix parameters with each input is only via the scalar attention, shown here with a light connection. The mixing of information between the inputs happens via residual connections with the pretrained fixed feature extraction and hence is not learnable. The MLP is also fixed and hence only acts as a multivariate activation function. This limited interaction explains why prefix-tuning struggles to learn new tasks even in deeper models. C E XPRESSIVITY OF PREFIX -TUNING ACROSS DEEPER MODELS In Section 6, we considered the effect of the presence of the prefix in the first attention layer on the attention of the second. However, the effects become more complex as one adds more attention attention layers. We also ignored the MLPs between, but in practice they can play an important role. Here, instead, we analyse prefix-tuning as learning a neural network. We argue that while the resulting architecture includes both linear operation and non-linear activations, the structure is unlikely to learn efficiently. For simplicity, we will consider two inputs x1 and x2 and a single prefix s. The output of the attention head, parameterized by s is then: As(x1, x2) = ⟨y1, y2⟩ y1 = exp(x1⊤Hs)WV s + exp(x1⊤Hx1)WV x1 + exp(x1⊤Hx2)WV x2 C1 (11) y2 = exp(x2⊤Hs)WV s + exp(x2⊤Hx1)WV x1 + exp(x2⊤Hx2)WV x2, C2 where we have omitted the T/ √ k factors and have folded the softmax normalization into C1 and C2. The layer inputs, pretrained parameters and the learnable parameters are correspondingly high- lighted. The attention head is clearly a non-linear operation. However, the learnable parameter s participates only in the left term. It interacts with only one of the inputs at a time and only by computing a single scalar value x⊤ i Hs. As we discussed above, the interaction between x1 and x2 is non-trainable and can be thought as a hard-coded feature extraction. Each of the outputs is then passed to a pretrained MLP which can be thought of as an activation function. This would be a multivariate activation function, which while unusual in the contemporary practice has been studied before (Solazzi and Uncini, 2004). Figure 8 illustrates the computation graph of the resulting neural network and shows that the only learnable interaction between the inputs is indirect. Therefore, prefix-tuning can be considered as learning a neural network where the only interaction between the inputs happens via non-learnable residual connections. Nevertheless, the alternating linear and nonlinear operations are reminiscent of the standard neural network architecture and their universal approximation properties (Hassoun, 1995). That begs the question if the prefix-tuning architecture can be a universal approximator and whether it would be a parameter-efficient one. An example of prefix-tuning failing to be a universal approximator. While we leave the formal analysis of the representational capacity of prefix-tuning as future work, we provide an example of pretrained parameters for which the architecture is not a universal approximator. As can be seen in Figure 8, all information must pass through the non-learnable MLPs. Thus, if the MLPs destroy all input information, there is no value for the prefixess(1), s(2), ...that can change that fact. The MLPs 21Published as a conference paper at ICLR 2024 can destroy all information, if, e.g., one of their linear layers has a zero weight matrix. While this is unlikely to be learned with typical pretraining, this demonstrates that if prefix-tuning could be a universal approximator, that would pose specific requirements on the pretrained MLPs and it is not clear whether real-world pretraining would satisfy these requirements. D E XTENDED RESULTS In this appendix we present further experiments in the context of Section 5. We consider different prefix lengths ( nS ∈ {10, 50, 100}) and different model sizes (4, 16 and 32 layers). We consider two extensions, the first one maintains the pretraining step as in Section 5, the other one extends the set of pretraining tasks with 4 additional tasks. Both pretraining and prefix-tuning are done for 100 000 iterations with the prefix-tuning accuracy reported every 10 000 iterations. D.1 P RETRAINING AS IN SECTION 5 The first setting has the same pretraining tasks as in Section 5, namely sorting in ascending ( ↗) or descending (↘) order, and adding one (+1) or two (+2) to each element of the input sequence. We evaluate by prefix-tuning on the same four tasks plus incrementing the ascending sorted sequence (↗+1), double histogram (H), element-wise modulo operation (with respect to the first element of the sequence), and FilterAtLeastNTimes which puts zeros at the positions of elements whose value appears at less than N times in the sequence with N being the first element of the sequence: Pretraining tasks: Sort in ascending order (↗) Sort in descending order (↘) Add 1(+1) Add 2 (+2) Prefix-tuning tasks: Sort in ascending order (↗) Sort in descending order (↘) Add 1(+1) Add 2 (+2) Sort ascending and add 1 (↗+1) Modulo the first element (Modulo) Double Histogram (H) Filter the elements that are at least as large as the first element (FilterAtLeastNTimes) The results are plotted in Figure 9. As expected, the prefix-tuned accuracy on the pretraining tasks is close to 100%. Interestingly, prefix length 50 for the largest (32-layer) architecture appears to be an exception and does not learn the +1, ↗ and ↘. As also observed in Section 5, regardless of the prefix length and the model size, prefix-tuning a model pretrained with these four tasks cannot learn the double histogram task ( H). FilterAtLeast- NTimes also appears to be challenging but the 16-layer and 32-layer experiments reach about 50% accuracy for the longest prefix size nS = 100. This is curious as FilterAtLeastNTimes is related to the double histogram task: it can be considered as thresholding the double histogram output. Fil- terAtLeastNTimes, similarly to double histogram, is therefore not compositional in the pretraining task. It is surprising then that FilterAtLeastNTimes would achieve higher accuracy than double his- togram. This hints that, perhaps, compositionality does not fully explain why prefix-tuning works for some and not other downstream tasks. The results in Figure 9 also hint that bigger is not always better. For example, the 4-layer model prefix-tuned for the Modulo task performs better with a prefix size 50 than the larger prefix size 100. A similar effect can be observed with the 16-layer model prefix-tuned for the ↗+1 task. Larger models are also not necessarily more conducive to successful prefix-tuning: for many of the cases the 32-layer models perform worse when prefix-tuned than the 16-layer models. D.2 E XTENDED PRETRAINING The second setting extends the set of pretraining tasks. On top of the original three pretraining tasks ↗, ↘,+1 (without +2) we also pretrain on element-wise modulo operation (with respect to the first element of the sequence), element-wise less than (less than the first element in the sequence), 22Published as a conference paper at ICLR 2024 Models pretrained with the original 4 tasks (                       ) 20000 40000 60000 80000 100000 0.0 0.5 1.0 20000 40000 60000 80000 100000  20000 40000 60000 80000 100000 20000 40000 60000 80000 100000 0.0 0.5 1.0 Add 1Add 2Double Histogram FilterAtLeast NTimes Sort Descending ModuloSort AscendingSort Ascending  + Add 1 20000 40000 60000 80000 100000  20000 40000 60000 80000 100000 20000 40000 60000 80000 100000 0.0 0.5 1.0 20000 40000 60000 80000 100000  20000 40000 60000 80000 100000 20000 40000 60000 80000 100000 0.0 0.5 1.0 20000 40000 60000 80000 100000  20000 40000 60000 80000 100000 20000 40000 60000 80000 100000 0.0 0.5 1.0 20000 40000 60000 80000 100000  20000 40000 60000 80000 100000 20000 40000 60000 80000 100000 0.0 0.5 1.0 20000 40000 60000 80000 100000  20000 40000 60000 80000 100000 20000 40000 60000 80000 100000 0.0 0.5 1.0 20000 40000 60000 80000 100000  20000 40000 60000 80000 100000 20000 40000 60000 80000 100000 0.0 0.5 1.0 20000 40000 60000 80000 100000  20000 40000 60000 80000 100000 Models with 4 attention layers Models with 16 attention layers Models with 32 attention layers Preﬁx-tuning tasks Preﬁx length: 10 50 100 Preﬁx-tuning training iterations Preﬁx-tuning training iterations Preﬁx-tuning training iterations Figure 9: Extended result with pretraining as in Section 5. The pretraining and the prefix-tuning tasks are described in Appendix D.1. Each prefix was trained for 100 000 iterations and we report accuracy at every 10 000 iterations. Each experiment is performed with 3 random seeds, except the 32 layer case which, due to the computational costs involved, was performed only with one seed. element-wise divisible (by the first element of the sequence), and inverse binary (element-wise nega- tion). For the inverse binary task, the input is restricted to be binary. We evaluate by prefix-tuning on the same seven tasks, as well as 11 additional ones as listed in the following table: Pretraining tasks: Sort in ascending order (↗) Sort in descending order (↘) Add 1 (+1) Modulo the first element (Modulo) Filter the elements that are less than the first element (LessThan) Filter the elements that are divisible by first element (Divisible) Element-wise negation (InverseBinary) Prefix-tuning tasks: Sort in ascending order (↗) Sort in descending order (↘) Add 1 (+1) Add 2 (+2) Add 3 (+3) Modulo the first element (Modulo) Filter the elements that are less than the first element (LessThan) Filter the elements that are not less than the first element (MoreThanEqual) Filter the elements that are divisible by first element (Divisible) Filter the elements that are not divisible by first element (NotDivisible) Element-wise negation (InverseBinary) Double Histogram (H) Filter the elements that are at least as large as the first element (FilterAtLeastNTimes) Sort ascending, followed by add 1 (↗+1) Add 1, followed by LessThan (+1 + LessThan) LessThan, followed by Add 1 (LessThan +1) LessThan, followed by sort ascending (LessThan + ↗) Divisible, followed by Add 1 (Divisible +1) 23Published as a conference paper at ICLR 2024 Models with 4 attention layers Models with 16 attention layers Models with 32 attention layers Preﬁx-tuning tasks 20000 40000 60000 80000 100000 0.0 0.5 1.0 20000 40000 60000 80000 100000  20000 40000 60000 80000 100000 20000 40000 60000 80000 100000 0.0 0.5 1.0 20000 40000 60000 80000 100000  20000 40000 60000 80000 100000 20000 40000 60000 80000 100000 0.0 0.5 1.0 20000 40000 60000 80000 100000  20000 40000 60000 80000 100000 20000 40000 60000 80000 100000 0.0 0.5 1.0 20000 40000 60000 80000 100000  20000 40000 60000 80000 100000 20000 40000 60000 80000 100000 0.0 0.5 1.0 20000 40000 60000 80000 100000  20000 40000 60000 80000 100000 20000 40000 60000 80000 100000 0.0 0.5 1.0 20000 40000 60000 80000 100000  20000 40000 60000 80000 100000 20000 40000 60000 80000 100000 0.0 0.5 1.0 20000 40000 60000 80000 100000  20000 40000 60000 80000 100000 20000 40000 60000 80000 100000 0.0 0.5 1.0 20000 40000 60000 80000 100000  20000 40000 60000 80000 100000 20000 40000 60000 80000 100000 0.0 0.5 1.0 20000 40000 60000 80000 100000  20000 40000 60000 80000 100000 20000 40000 60000 80000 100000 0.0 0.5 1.0 20000 40000 60000 80000 100000  20000 40000 60000 80000 100000 20000 40000 60000 80000 100000 0.0 0.5 1.0 20000 40000 60000 80000 100000  20000 40000 60000 80000 100000 20000 40000 60000 80000 100000 0.0 0.5 1.0 20000 40000 60000 80000 100000  20000 40000 60000 80000 100000 20000 40000 60000 80000 100000 0.0 0.5 1.0 20000 40000 60000 80000 100000  20000 40000 60000 80000 100000 20000 40000 60000 80000 100000 0.0 0.5 1.0 20000 40000 60000 80000 100000  20000 40000 60000 80000 100000 20000 40000 60000 80000 100000 0.0 0.5 1.0 20000 40000 60000 80000 100000  20000 40000 60000 80000 100000 20000 40000 60000 80000 100000 0.0 0.5 1.0 20000 40000 60000 80000 100000  20000 40000 60000 80000 100000 20000 40000 60000 80000 100000 0.0 0.5 1.0 20000 40000 60000 80000 100000  20000 40000 60000 80000 100000 20000 40000 60000 80000 100000 0.0 0.5 1.0 20000 40000 60000 80000 100000  20000 40000 60000 80000 100000 Preﬁx length: 10 50 100 Add 1Add 2Add 3Add 1  + LessThan DivisibleDivisible  + Add 1 Double Histogram FilterAtLeast NTimes InverseBinaryLessThanLessThan  + Add1 LessThan + Sort Ascending ModuloMoreThanEqualNotDivisibleSort AscendingSort Descending Sort Ascending  + Add 1 Preﬁx-tuning training iterations Preﬁx-tuning training iterations Preﬁx-tuning training iterations Models pretrained with an extended set of tasks Figure 10: Extended result with pretraining with additional tasks. The pretraining and the prefix-tuning tasks are described in Appendix D.2. Each prefix was trained for 100 000 iterations and we report accuracy at every 10 000 iterations. Each experiment is performed with 3 random seeds, except the 32 layer case which, due to the computational costs involved, was performed only with one seed. 24Published as a conference paper at ICLR 2024 The results are shown in Figure 10. Even though the model did not see the+2 and +3 tasks, it appears that prefix-tuning can generalize from the +1 task. The +1 + LessThan task is another example of successful prefix-tuning for a task that is compositional in pretraining tasks. Similarly, for Divisible +1, LessThan +1, LessThan + ↗, MoreThanEqual, NotDivisible, Similarly to the case in Appendix D.1, we observe several instances in which the largest, 32-layer, model performs worse, when prefix-tuned, than the smaller models, as well as cases where shorter prefixes perform better than longer ones. E F URTHER EXPERIMENTS WITH MEMORIZATION Our experiments in Section 5 focused on algorithmic tasks such as sorting, incrementing and count- ing. However, learning a natural language also includes a substantial memorization component. For example, learning a new language requires learning its vocabulary. Hence, if a novel task requires memorization of novel concepts, the fine-tuning method should be able to memorize them. To this end, we evaluate and compare the abilities of prefix-tuning and LoRA to learn to memorize a large number of new words and show that, for the same amount of learnable parameters, LoRA can learn to translate to a new language while prefix-tuning cannot. These findings further strengthen our results from Section 5. We use the PHOR-in-One dataset (Costa et al., 2022) that contains 4921 unique translations of English words into German, Spanish and Portuguese. The dataset is preprocessed to remove all accents in order to ensure that fine-tuning does not require characters that the model has not seen during pre-training (e.g., ´ec ¸¨u would become ecu). Character-level tokenization is used, with the additional <TR> token that separates the source and target words and a <PAD> token that we use to ensure all training sequences are of the same length. We then pre-train a 4-layer 4-head transformer to translate the English words to German. As seen in Table 4, this model successfully memorizes 99.3% of the English-German pairs. We then want to fine-tune this model to instead translate the English words to Spanish. Spanish is linguistically closer to English than German, so, in a way, the fine-tuning task is simpler than the pre-training task. We do prefix-tuning on English-Spanish pairs with a prefix of size nS = 66 but it achieves only 0.18% accuracy (about 10 words which are the same in English and Spanish, e.g., instrumental, orbital, solar). However, rank-4 LoRA is able to achieve 94.8% accuracy with half the training iterations. Both fine-tuning methods have similar numbers of learnable parameters (see Table 4). Therefore, this is further evidence that prefix-tuning fails to learn a new task, while LoRA with the same number of learnable parameters can. Note that there is no train-test split for the word pairs. We are evaluating the accuracy on the training set as this experiment is measuring memorization rather than generalization. Table 4: Further experiments on memorization of word pairs form (Costa et al., 2022) in different languages. The model is pre-trained on the English-to-German word pairs to almost perfect accuracy. Prefix-tuning fails to modify it to memorize English-Spanish word pairs while LoRA with the same number of parameters and half the training iterations reaches 94% percent accuracy. 5 random seeds. Pre-training on English to German Prefix-tune (nS=66) on English to Spanish Rank-4 LoRA on English to Spanish Accuracy 99.3 ± 0.1% 0.18 ± 0.01% 94.8 ± 1.1% Learnable parameters 3.19M 67 584 66 780 Training iterations 50 000 100 000 50 000 25",
      "meta_data": {
        "arxiv_id": "2310.19698v2",
        "authors": [
          "Aleksandar Petrov",
          "Philip H. S. Torr",
          "Adel Bibi"
        ],
        "published_date": "2023-10-30T16:19:34Z",
        "pdf_url": "https://arxiv.org/pdf/2310.19698v2.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper provides a theoretical understanding of how context-based fine-tuning methods (prompting, in-context learning, soft prompting, prefix-tuning) influence internal model computation and their expressiveness limitations. It shows that soft prompting and prefix-tuning, despite utilizing a continuous embedding space, are less expressive than full fine-tuning. Specifically, these methods cannot alter the relative attention patterns over content and can only bias attention layer outputs in a fixed direction. This implies they can elicit or combine existing skills but struggle to learn novel tasks requiring new attention patterns, even with the same number of learnable parameters as full fine-tuning.",
        "methodology": "The methodology combines theoretical analysis with empirical validation using constructed transformer models. Theoretically, the paper analyzes the influence of prompts and prefixes on attention patterns (Equation 6) and attention block outputs (Equation 7), showing how they can only scale down attention over content and introduce a constant bias. Constructive proofs (Theorems 1 and 2, detailed in Appendix A) demonstrate the exponential generation capacity of virtual tokens. Empirically, the paper uses small, custom-built transformer models trained on algorithmic tasks (e.g., sorting, arithmetic operations) to compare prefix-tuning with full fine-tuning and LoRA, analyzing their ability to learn new or composite tasks. It also provides empirical validation on real-world models (LLaMA and GPT-2) to support its theoretical findings regarding attention distribution and activations.",
        "experimental_setup": "The experimental setup involves minimal transformer models (1-layer, 1-head; 1-layer, 4-head; 40-layer, 4-head) pretrained on various algorithmic tasks such as sorting (ascending/descending), adding (+1, +2), modulo, filtering, and inverse binary operations using random sequences of 10 digits (0-7). These models were then fine-tuned using prefix-tuning (with prefix sizes 1, 10, 50, 100), full fine-tuning, and LoRA (rank-1 for algorithmic tasks, rank-4 for memorization) on existing, compositional, and novel tasks like 'double histogram' (H) and 'FilterAtLeastNTimes'. Accuracy (exact sequence match) was used as the primary metric. Further experiments included memorization tasks using the PHOR-in-One dataset (English-German/Spanish word pairs). Training typically involved 100,000 iterations for pretraining and prefix-tuning, with 50,000 for LoRA, usually with 3 random seeds (1 for 32-layer models due to computational cost). Real-world LLaMA and GPT-2 models were used to validate attention patterns and activations.",
        "limitations": "The analysis is primarily limited to prefixing with prompts, soft prompts, and prefix-tuning, with theoretical results for suffix-tuning not necessarily extending to suffixing with prompts or soft prompts. The question of whether suffixing is more expressive than prefixing remains open. The paper also leaves the formal analysis of conditions under which context-based fine-tuning methods might be universal approximators, and their parameter inefficiency, as future work. The experiments mostly utilized simple toy problems, and the extent to which the demonstrated limitations apply to large-scale pretrained transformers is a subject for future research. Additionally, the possibility of MLPs destroying input information could limit prefix-tuning's universal approximation capabilities.",
        "future_research_directions": "Future research directions include investigating the expressiveness of suffixing compared to prefixing, conducting formal analyses on the conditions for context-based fine-tuning methods to be universal approximators, and a more rigorous analysis of their parameter inefficiency. Extending the study to large-scale pretrained transformers is also crucial to ascertain the applicability of the observed limitations in real-world scenarios. The paper also suggests using prefix-tuning as a tool for model interpretability, specifically to identify what 'skills' a model possesses based on its success or failure with certain prefix-tuned tasks."
      }
    }
  ]
}