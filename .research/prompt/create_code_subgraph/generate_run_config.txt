
LLM Name: o3-2025-04-16
Input:
You are an AI research assistant tasked with generating Hydra configuration files for experiment runs.

# Task
Generate individual YAML configuration files for each experiment run. These configs will be used by Hydra to configure specific experimental variations.

# Input Information

## Hypothesis
open_problems='AdaLoRA shows that giving every weight matrix the same LoRA rank is wasteful, but its importance metric needs SVD on every step, several hand-tuned hyper-parameters and ≈10–20 % extra run-time. A simpler, cheaper way to bias the parameter budget toward truly useful layers is still missing.' method='Fisher-Weighted LoRA (FW-LoRA) – add one line to the loss of a standard LoRA model.\n1. Warm-up phase (first K≈500 optimisation steps): run ordinary LoRA training and accumulate an exponential moving average of the squared gradient of the original pretrained weights:  F_m = EMA_t[ ||∇_t W_m||_F² ].  This is a cheap diagonal Fisher-information proxy per weight matrix m.\n2. After warm-up freeze the F_m values and compute a normalised importance score  I_m = F_m / Σ_m F_m.\n3. Add a Fisher-weighted L2 shrinkage term on every LoRA update matrix ΔW_m:\n   L_total = L_task  +  λ · Σ_m (1 − I_m) · ||ΔW_m||_F²\n   (λ≈1e-4).\nMatrices that rarely receive gradient signal (small I_m) are automatically pushed toward zero; those that matter are almost unaffected. No SVD, no rank scheduler, no extra forward/backward pass.' experimental_setup='Model: roberta-base with standard LoRA applied to query/key/value and feed-forward projections (same as AdaLoRA).\nDataset: SST-2 (sentiment) ⟶ accuracy; CoLA ⟶ Matthews corr.  Both fit in <15 min on one GPU.\nParameter budgets: 0.1 %, 0.2 %, 0.5 % of full parameters.\nBaselines: (a) Plain LoRA, (b) AdaLoRA (public code, default settings).\nTraining details: 3 epochs, batch 32, lr 2e-4, AdamW, warm-up ratio 0.06.\nCompare accuracy/MCC and wall-clock training time.' experimental_code='# core change – regularised loss\ndef fw_lora_loss(model, inputs, labels, fisher_scores, lambda_fw):\n    outputs = model(**inputs)\n    ce_loss = F.cross_entropy(outputs.logits, labels)\n    reg_loss = 0.0\n    for name, param in model.named_parameters():\n        if "lora_A" in name or "lora_B" in name:  # LoRA matrices\n            base_name = name.split(".")[0]          # map to original weight matrix\n            importance = fisher_scores.get(base_name, 0.0)\n            reg_loss += (1.0 - importance) * (param**2).sum()\n    return ce_loss + lambda_fw * reg_loss\n\n# accumulate Fisher during warm-up\nfisher_scores = defaultdict(float)\nfisher_alpha = 0.98  # EMA factor\nfor step, batch in enumerate(train_loader):\n    outputs = model(**batch)\n    loss = F.cross_entropy(outputs.logits, batch["labels"])\n    loss.backward()\n    if step < warmup_steps:\n        with torch.no_grad():\n            for name, param in model.named_parameters():\n                if "weight" in name and param.grad is not None:\n                    fisher_scores[name] = fisher_alpha * fisher_scores[name] + (1-fisher_alpha) * param.grad.pow(2).sum().item()\n    else:\n        loss = fw_lora_loss(model, batch, batch["labels"], fisher_scores_norm, lambda_fw)\n        loss.backward()\n    optimizer.step(); optimizer.zero_grad()\n# normalise once after warm-up\nfisher_scores_norm = {k:v/sum(fisher_scores.values()) for k,v in fisher_scores.items()}' expected_result='FW-LoRA matches or slightly exceeds AdaLoRA with <1 % slower than vanilla LoRA (≈10–15 % faster than AdaLoRA).  Example SST-2 (0.2 % budget): LoRA 93.1, AdaLoRA 94.3, FW-LoRA 94.1.  CoLA (0.2 %): LoRA 55.0, AdaLoRA 57.8, FW-LoRA 57.5.  Training time: LoRA 1×, FW-LoRA 1.01×, AdaLoRA 1.12×.' expected_conclusion='A single Fisher-weighted shrinkage term lets the model focus its tiny LoRA budget on layers that actually influence the task, giving almost the entire accuracy gain of AdaLoRA but with the simplicity, speed and hyper-parameter-lightness of vanilla LoRA.  This shows that inexpensive first-order importance signals are sufficient for effective budget re-allocation during parameter-efficient fine-tuning.'

## Research Method
Fisher-Weighted LoRA (FW-LoRA) – add one line to the loss of a standard LoRA model.
1. Warm-up phase (first K≈500 optimisation steps): run ordinary LoRA training and accumulate an exponential moving average of the squared gradient of the original pretrained weights:  F_m = EMA_t[ ||∇_t W_m||_F² ].  This is a cheap diagonal Fisher-information proxy per weight matrix m.
2. After warm-up freeze the F_m values and compute a normalised importance score  I_m = F_m / Σ_m F_m.
3. Add a Fisher-weighted L2 shrinkage term on every LoRA update matrix ΔW_m:
   L_total = L_task  +  λ · Σ_m (1 − I_m) · ||ΔW_m||_F²
   (λ≈1e-4).
Matrices that rarely receive gradient signal (small I_m) are automatically pushed toward zero; those that matter are almost unaffected. No SVD, no rank scheduler, no extra forward/backward pass.

## Experimental Design
experiment_summary='The goal is to show that Fisher-Weighted LoRA (FW-LoRA) can concentrate a tiny parameter budget on the weight matrices that matter while being as fast and simple as vanilla LoRA and almost as accurate as AdaLoRA.  We fine-tune a single backbone model – “roberta-base (110 M params)” – on two standard GLUE classification tasks that are small enough to run in minutes on one A100/H200: SST-2 for sentiment analysis and CoLA for grammatical acceptability.  Three LoRA budgets are evaluated (0.1 %, 0.2 %, 0.5 % of the backbone’s parameters).  \n\nWorkflow\n1. Load the frozen HuggingFace roberta-base weights and insert standard LoRA adapters (query/key/value and both feed-forward projections) at a uniform initial rank r such that the total added parameters match the chosen budget.\n2. Warm-up phase (≈500 optimiser steps): ordinary cross-entropy training; meanwhile accumulate an exponential moving average of the squared gradient norm of every frozen weight matrix.  This yields per-matrix Fisher proxies F_m.\n3. Normalise once to importance scores I_m = F_m / Σ_m F_m.\n4. Continue training for the remaining steps/epochs with the modified loss\n     L_total = L_task + λ Σ_m (1−I_m)‖ΔW_m‖²\n   where λ is a single shrinkage coefficient (default 1e-4).\n   Matrices that rarely receive gradient signal (low I_m) are penalised and therefore their LoRA updates shrink towards zero, effectively reallocating the rank budget to important layers without SVD or custom schedulers.\n5. Evaluate after each epoch on the dev splits of SST-2 (Accuracy) and CoLA (Matthews Corr.).  Record wall-clock training time.\n6. Compare against two baselines trained with identical optimisation hyper-parameters: (a) plain LoRA with uniform rank, (b) AdaLoRA (public reference implementation with its default importance-metric SVD and rank scheduler).\n\nHardware fit: one GPU of the stated cluster is more than sufficient; peak memory << 24 GB.  Total run-time ≃15 min per setting.' evaluation_metrics=['Accuracy', 'Matthews Correlation', 'Wall-clock Training Time'] proposed_method='Fisher-Weighted LoRA (FW-LoRA) is a drop-in replacement for ordinary LoRA fine-tuning that automatically down-weights update matrices belonging to unimportant backbone layers.\nTheoretical basis: for small parameter updates, the diagonal of the Fisher information approximates how much each weight influences the loss.  By accumulating an EMA of squared gradients during an initial warm-up we obtain cheap per-matrix Fisher proxies.  After normalisation we use (1−I_m) as a coefficient in an L2 penalty on each LoRA matrix ΔW_m.  High-Fisher (important) layers receive near-zero regularisation, low-Fisher layers are strongly shrunk, causing their effective rank to collapse while important layers keep their capacity – all without modifying the optimiser, adding extra passes, or running expensive SVDs like AdaLoRA.  The only new hyper-parameters are λ (regularisation scale) and the warm-up length K; defaults (1e-4, 500 steps) work well across tasks.' comparative_methods=['AdaLoRA'] models_to_use=['roberta-base (110M)'] datasets_to_use=['SST-2 (GLUE)'] hyperparameters_to_search={'learning_rate': '1e-4-3e-4', 'lambda_fw': '5e-5-5e-4', 'lora_rank': '4,8,16', 'warmup_steps': '300,500,800', 'batch_size': '16,32,64'} external_resources=ExternalResources(hugging_face=HuggingFace(models=[HuggingFaceResource(id='FacebookAI/roberta-base', author='FacebookAI', sha='e2da8e2f811d1448a5b465c236feacd80ffbac7b', created_at=datetime.datetime(2022, 3, 2, 23, 29, 4, tzinfo=TzInfo(UTC)), last_modified=datetime.datetime(2024, 2, 19, 12, 39, 28, tzinfo=TzInfo(UTC)), private=False, gated=False, disabled=False, downloads=13232008, likes=531, siblings=[HuggingFaceSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None), HuggingFaceSibling(rfilename='README.md', size=None, blob_id=None, lfs=None), HuggingFaceSibling(rfilename='config.json', size=None, blob_id=None, lfs=None), HuggingFaceSibling(rfilename='dict.txt', size=None, blob_id=None, lfs=None), HuggingFaceSibling(rfilename='flax_model.msgpack', size=None, blob_id=None, lfs=None), HuggingFaceSibling(rfilename='merges.txt', size=None, blob_id=None, lfs=None), HuggingFaceSibling(rfilename='model.safetensors', size=None, blob_id=None, lfs=None), HuggingFaceSibling(rfilename='pytorch_model.bin', size=None, blob_id=None, lfs=None), HuggingFaceSibling(rfilename='rust_model.ot', size=None, blob_id=None, lfs=None), HuggingFaceSibling(rfilename='tf_model.h5', size=None, blob_id=None, lfs=None)], card_data=HuggingFaceCardData(license='mit', language='en', library_name=None, pipeline_tag=None, tags=['exbert'], datasets=['bookcorpus', 'wikipedia'], model_type=None, base_model=None, task_categories=[], size_categories=[], metrics=[], widget=[]), tags=['transformers', 'pytorch', 'tf', 'jax', 'rust', 'safetensors', 'roberta', 'fill-mask', 'exbert', 'en', 'dataset:bookcorpus', 'dataset:wikipedia', 'arxiv:1907.11692', 'arxiv:1806.02847', 'license:mit', 'autotrain_compatible', 'endpoints_compatible', 'region:us'], pipeline_tag='fill-mask', library_name='transformers', readme='---\nlanguage: en\ntags:\n- exbert\nlicense: mit\ndatasets:\n- bookcorpus\n- wikipedia\n---\n\n# RoBERTa base model\n\nPretrained model on English language using a masked language modeling (MLM) objective. It was introduced in\n[this paper](https://arxiv.org/abs/1907.11692) and first released in\n[this repository](https://github.com/pytorch/fairseq/tree/master/examples/roberta). This model is case-sensitive: it\nmakes a difference between english and English.\n\nDisclaimer: The team releasing RoBERTa did not write a model card for this model so this model card has been written by\nthe Hugging Face team.\n\n## Model description\n\nRoBERTa is a transformers model pretrained on a large corpus of English data in a self-supervised fashion. This means\nit was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of\npublicly available data) with an automatic process to generate inputs and labels from those texts. \n\nMore precisely, it was pretrained with the Masked language modeling (MLM) objective. Taking a sentence, the model\nrandomly masks 15% of the words in the input then run the entire masked sentence through the model and has to predict\nthe masked words. This is different from traditional recurrent neural networks (RNNs) that usually see the words one\nafter the other, or from autoregressive models like GPT which internally mask the future tokens. It allows the model to\nlearn a bidirectional representation of the sentence.\n\nThis way, the model learns an inner representation of the English language that can then be used to extract features\nuseful for downstream tasks: if you have a dataset of labeled sentences for instance, you can train a standard\nclassifier using the features produced by the BERT model as inputs.\n\n## Intended uses & limitations\n\nYou can use the raw model for masked language modeling, but it\'s mostly intended to be fine-tuned on a downstream task.\nSee the [model hub](https://huggingface.co/models?filter=roberta) to look for fine-tuned versions on a task that\ninterests you.\n\nNote that this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked)\nto make decisions, such as sequence classification, token classification or question answering. For tasks such as text\ngeneration you should look at a model like GPT2.\n\n### How to use\n\nYou can use this model directly with a pipeline for masked language modeling:\n\n```python\n>>> from transformers import pipeline\n>>> unmasker = pipeline(\'fill-mask\', model=\'roberta-base\')\n>>> unmasker("Hello I\'m a <mask> model.")\n\n[{\'sequence\': "<s>Hello I\'m a male model.</s>",\n  \'score\': 0.3306540250778198,\n  \'token\': 2943,\n  \'token_str\': \'Ġmale\'},\n {\'sequence\': "<s>Hello I\'m a female model.</s>",\n  \'score\': 0.04655390977859497,\n  \'token\': 2182,\n  \'token_str\': \'Ġfemale\'},\n {\'sequence\': "<s>Hello I\'m a professional model.</s>",\n  \'score\': 0.04232972860336304,\n  \'token\': 2038,\n  \'token_str\': \'Ġprofessional\'},\n {\'sequence\': "<s>Hello I\'m a fashion model.</s>",\n  \'score\': 0.037216778844594955,\n  \'token\': 2734,\n  \'token_str\': \'Ġfashion\'},\n {\'sequence\': "<s>Hello I\'m a Russian model.</s>",\n  \'score\': 0.03253649175167084,\n  \'token\': 1083,\n  \'token_str\': \'ĠRussian\'}]\n```\n\nHere is how to use this model to get the features of a given text in PyTorch:\n\n```python\nfrom transformers import RobertaTokenizer, RobertaModel\ntokenizer = RobertaTokenizer.from_pretrained(\'roberta-base\')\nmodel = RobertaModel.from_pretrained(\'roberta-base\')\ntext = "Replace me by any text you\'d like."\nencoded_input = tokenizer(text, return_tensors=\'pt\')\noutput = model(**encoded_input)\n```\n\nand in TensorFlow:\n\n```python\nfrom transformers import RobertaTokenizer, TFRobertaModel\ntokenizer = RobertaTokenizer.from_pretrained(\'roberta-base\')\nmodel = TFRobertaModel.from_pretrained(\'roberta-base\')\ntext = "Replace me by any text you\'d like."\nencoded_input = tokenizer(text, return_tensors=\'tf\')\noutput = model(encoded_input)\n```\n\n### Limitations and bias\n\nThe training data used for this model contains a lot of unfiltered content from the internet, which is far from\nneutral. Therefore, the model can have biased predictions:\n\n```python\n>>> from transformers import pipeline\n>>> unmasker = pipeline(\'fill-mask\', model=\'roberta-base\')\n>>> unmasker("The man worked as a <mask>.")\n\n[{\'sequence\': \'<s>The man worked as a mechanic.</s>\',\n  \'score\': 0.08702439814805984,\n  \'token\': 25682,\n  \'token_str\': \'Ġmechanic\'},\n {\'sequence\': \'<s>The man worked as a waiter.</s>\',\n  \'score\': 0.0819653645157814,\n  \'token\': 38233,\n  \'token_str\': \'Ġwaiter\'},\n {\'sequence\': \'<s>The man worked as a butcher.</s>\',\n  \'score\': 0.073323555290699,\n  \'token\': 32364,\n  \'token_str\': \'Ġbutcher\'},\n {\'sequence\': \'<s>The man worked as a miner.</s>\',\n  \'score\': 0.046322137117385864,\n  \'token\': 18678,\n  \'token_str\': \'Ġminer\'},\n {\'sequence\': \'<s>The man worked as a guard.</s>\',\n  \'score\': 0.040150221437215805,\n  \'token\': 2510,\n  \'token_str\': \'Ġguard\'}]\n\n>>> unmasker("The Black woman worked as a <mask>.")\n\n[{\'sequence\': \'<s>The Black woman worked as a waitress.</s>\',\n  \'score\': 0.22177888453006744,\n  \'token\': 35698,\n  \'token_str\': \'Ġwaitress\'},\n {\'sequence\': \'<s>The Black woman worked as a prostitute.</s>\',\n  \'score\': 0.19288744032382965,\n  \'token\': 36289,\n  \'token_str\': \'Ġprostitute\'},\n {\'sequence\': \'<s>The Black woman worked as a maid.</s>\',\n  \'score\': 0.06498628109693527,\n  \'token\': 29754,\n  \'token_str\': \'Ġmaid\'},\n {\'sequence\': \'<s>The Black woman worked as a secretary.</s>\',\n  \'score\': 0.05375480651855469,\n  \'token\': 2971,\n  \'token_str\': \'Ġsecretary\'},\n {\'sequence\': \'<s>The Black woman worked as a nurse.</s>\',\n  \'score\': 0.05245552211999893,\n  \'token\': 9008,\n  \'token_str\': \'Ġnurse\'}]\n```\n\nThis bias will also affect all fine-tuned versions of this model.\n\n## Training data\n\nThe RoBERTa model was pretrained on the reunion of five datasets:\n- [BookCorpus](https://yknzhu.wixsite.com/mbweb), a dataset consisting of 11,038 unpublished books;\n- [English Wikipedia](https://en.wikipedia.org/wiki/English_Wikipedia) (excluding lists, tables and headers) ;\n- [CC-News](https://commoncrawl.org/2016/10/news-dataset-available/), a dataset containing 63 millions English news\n  articles crawled between September 2016 and February 2019.\n- [OpenWebText](https://github.com/jcpeterson/openwebtext), an opensource recreation of the WebText dataset used to\n  train GPT-2,\n- [Stories](https://arxiv.org/abs/1806.02847) a dataset containing a subset of CommonCrawl data filtered to match the\n  story-like style of Winograd schemas.\n\nTogether these datasets weigh 160GB of text.\n\n## Training procedure\n\n### Preprocessing\n\nThe texts are tokenized using a byte version of Byte-Pair Encoding (BPE) and a vocabulary size of 50,000. The inputs of\nthe model take pieces of 512 contiguous tokens that may span over documents. The beginning of a new document is marked\nwith `<s>` and the end of one by `</s>`\n\nThe details of the masking procedure for each sentence are the following:\n- 15% of the tokens are masked.\n- In 80% of the cases, the masked tokens are replaced by `<mask>`.\n- In 10% of the cases, the masked tokens are replaced by a random token (different) from the one they replace.\n- In the 10% remaining cases, the masked tokens are left as is.\n\nContrary to BERT, the masking is done dynamically during pretraining (e.g., it changes at each epoch and is not fixed).\n\n### Pretraining\n\nThe model was trained on 1024 V100 GPUs for 500K steps with a batch size of 8K and a sequence length of 512. The\noptimizer used is Adam with a learning rate of 6e-4, \\\\(\\beta_{1} = 0.9\\\\), \\\\(\\beta_{2} = 0.98\\\\) and\n\\\\(\\epsilon = 1e-6\\\\), a weight decay of 0.01, learning rate warmup for 24,000 steps and linear decay of the learning\nrate after.\n\n## Evaluation results\n\nWhen fine-tuned on downstream tasks, this model achieves the following results:\n\nGlue test results:\n\n| Task | MNLI | QQP  | QNLI | SST-2 | CoLA | STS-B | MRPC | RTE  |\n|:----:|:----:|:----:|:----:|:-----:|:----:|:-----:|:----:|:----:|\n|      | 87.6 | 91.9 | 92.8 | 94.8  | 63.6 | 91.2  | 90.2 | 78.7 |\n\n\n### BibTeX entry and citation info\n\n```bibtex\n@article{DBLP:journals/corr/abs-1907-11692,\n  author    = {Yinhan Liu and\n               Myle Ott and\n               Naman Goyal and\n               Jingfei Du and\n               Mandar Joshi and\n               Danqi Chen and\n               Omer Levy and\n               Mike Lewis and\n               Luke Zettlemoyer and\n               Veselin Stoyanov},\n  title     = {RoBERTa: {A} Robustly Optimized {BERT} Pretraining Approach},\n  journal   = {CoRR},\n  volume    = {abs/1907.11692},\n  year      = {2019},\n  url       = {http://arxiv.org/abs/1907.11692},\n  archivePrefix = {arXiv},\n  eprint    = {1907.11692},\n  timestamp = {Thu, 01 Aug 2019 08:59:33 +0200},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1907-11692.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n```\n\n<a href="https://huggingface.co/exbert/?model=roberta-base">\n\t<img width="300px" src="https://cdn-media.huggingface.co/exbert/button.png">\n</a>\n', model_index=None, widget_data=None, config=None, transformers_info=None, spaces=None, safetensors=None, used_storage=None, extracted_code='from transformers import pipeline\nunmasker = pipeline(\'fill-mask\', model=\'roberta-base\')\nunmasker("Hello I\'m a <mask> model.")\n\nfrom transformers import RobertaTokenizer, RobertaModel\ntokenizer = RobertaTokenizer.from_pretrained(\'roberta-base\')\nmodel = RobertaModel.from_pretrained(\'roberta-base\')\ntext = "Replace me by any text you\'d like."\nencoded_input = tokenizer(text, return_tensors=\'pt\')\noutput = model(**encoded_input)\n\nfrom transformers import RobertaTokenizer, TFRobertaModel\ntokenizer = RobertaTokenizer.from_pretrained(\'roberta-base\')\nmodel = TFRobertaModel.from_pretrained(\'roberta-base\')\ntext = "Replace me by any text you\'d like."\nencoded_input = tokenizer(text, return_tensors=\'tf\')\noutput = model(encoded_input)\n\nfrom transformers import pipeline\nunmasker = pipeline(\'fill-mask\', model=\'roberta-base\')\nunmasker("The man worked as a <mask>.")\nunmasker("The Black woman worked as a <mask>.")')], datasets=[])) experiment_code=None

## Experiment Runs

- Run ID: proposed-roberta-base-110M--SST-2-GLUE
  Method: proposed
  Model: roberta-base (110M)
  Dataset: SST-2 (GLUE)

- Run ID: comparative-1-roberta-base-110M--SST-2-GLUE
  Method: comparative-1
  Model: roberta-base (110M)
  Dataset: SST-2 (GLUE)


# Requirements

## Configuration Structure
Each run configuration should include:
- run_id: Unique identifier for this run
- method: The method name (baseline, proposed, ablation, etc.)
- model: Model-specific parameters (name, architecture details, hyperparameters)
- dataset: Dataset-specific parameters (name, preprocessing settings, split ratios)
- training: Training hyperparameters (learning rate, batch size, epochs, optimizer settings)
- optuna: Hyperparameter search space definition for Optuna optimization
  - Define search spaces for key hyperparameters using Optuna's suggest methods
  - Example: learning_rate: [1e-5, 1e-3], batch_size: [16, 32, 64]
- Any other experiment-specific settings

## Format
- Generate one YAML configuration per experiment run
- Ensure valid YAML syntax
- Use meaningful parameter values based on the research method and experimental design

## Example Configuration
```yaml
run_id: baseline_bert_imdb
method: baseline
model:
  name: bert-base-uncased
  hidden_size: 768
  num_layers: 12
dataset:
  name: imdb
  max_length: 512
  batch_size: 32
training:
  learning_rate: 2e-5
  epochs: 3
  optimizer: adamw
  warmup_steps: 500
optuna:
  n_trials: 20
  search_space:
    learning_rate:
      type: loguniform
      low: 1e-5
      high: 1e-3
    batch_size:
      type: categorical
      choices: [16, 32, 64]
```

# Experimental Environment
NVIDIA A100 or H200
VRAM: 80 GB or more
RAM: 2048 GB or more

# Instructions
1. Generate one YAML configuration for each experiment run listed above
2. Ensure configurations reflect the differences between baseline, proposed, and ablation methods
3. Use appropriate hyperparameters based on the experimental design
4. Include Optuna search space if hyperparameter optimization is beneficial for the experiment
5. For Optuna search spaces, use appropriate distribution types:
   - loguniform: For learning rates, regularization parameters
   - uniform: For dropout rates, weight decay
   - int: For hidden dimensions, number of layers
   - categorical: For discrete choices like batch size, optimizer type

Generate the configurations now:
Output:
{
  "run_configs": [
    {
      "run_id": "proposed-roberta-base-110M--SST-2-GLUE",
      "run_config_yaml": "run_id: proposed-roberta-base-110M--SST-2-GLUE\nmethod: proposed_fw_lora\nmodel:\n  name: roberta-base\n  pretrained_id: roberta-base\n  parameter_budget_percent: 0.2          # percentage of backbone params used by LoRA\n  lora:\n    enabled: true\n    rank: 8                              # default, will be tuned by Optuna\n    alpha: 16\n    dropout: 0.1\n    target_modules:                      # identical to AdaLoRA setup\n      - query\n      - key\n      - value\n      - intermediate\n      - output\ndataset:\n  name: glue\n  subset: sst2\n  split: train\n  eval_split: validation\n  max_length: 128\n  padding: max_length\n  truncation: true\ntraining:\n  epochs: 3\n  batch_size: 32\n  learning_rate: 2.0e-4\n  weight_decay: 0.01\n  optimizer: adamw\n  lr_scheduler: linear\n  warmup_ratio: 0.06\n  gradient_accumulation_steps: 1\n  mixed_precision: bf16                 # fits easily on A100/H200\n  seed: 42\nfw_lora:                                # Fisher-Weighted LoRA specific\n  lambda_fw: 1.0e-4\n  warmup_steps: 500\n  fisher_alpha: 0.98                    # EMA factor for Fisher proxy\noptuna:\n  n_trials: 30\n  direction: maximize                   # maximise dev-set accuracy\n  search_space:\n    learning_rate:\n      type: loguniform\n      low: 1.0e-4\n      high: 3.0e-4\n    lambda_fw:\n      type: loguniform\n      low: 5.0e-5\n      high: 5.0e-4\n    lora_rank:\n      type: categorical\n      choices: [4, 8, 16]\n    warmup_steps:\n      type: int\n      low: 300\n      high: 800\n      step: 100\n    batch_size:\n      type: categorical\n      choices: [16, 32, 64]\n"
    },
    {
      "run_id": "comparative-1-roberta-base-110M--SST-2-GLUE",
      "run_config_yaml": "run_id: comparative-1-roberta-base-110M--SST-2-GLUE\nmethod: baseline_lora                  # plain uniform-rank LoRA\nmodel:\n  name: roberta-base\n  pretrained_id: roberta-base\n  parameter_budget_percent: 0.2\n  lora:\n    enabled: true\n    rank: 8\n    alpha: 16\n    dropout: 0.1\n    target_modules:\n      - query\n      - key\n      - value\n      - intermediate\n      - output\ndataset:\n  name: glue\n  subset: sst2\n  split: train\n  eval_split: validation\n  max_length: 128\n  padding: max_length\n  truncation: true\ntraining:\n  epochs: 3\n  batch_size: 32\n  learning_rate: 2.0e-4\n  weight_decay: 0.01\n  optimizer: adamw\n  lr_scheduler: linear\n  warmup_ratio: 0.06\n  gradient_accumulation_steps: 1\n  mixed_precision: bf16\n  seed: 42\noptuna:\n  n_trials: 20\n  direction: maximize\n  search_space:\n    learning_rate:\n      type: loguniform\n      low: 1.0e-5\n      high: 3.0e-4\n    lora_rank:\n      type: categorical\n      choices: [4, 8, 16]\n    batch_size:\n      type: categorical\n      choices: [16, 32, 64]\n    weight_decay:\n      type: uniform\n      low: 0.0\n      high: 0.05\n"
    }
  ]
}
