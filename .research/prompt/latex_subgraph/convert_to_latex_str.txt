
LLM Name: gpt-5-2025-08-07
Input:

You are a LaTeX expert.
Your task is to convert each section of a research paper into plain LaTeX **content only**, without including any section titles or metadata.

Below are the paper sections. For each one, convert only the **content** into LaTeX:

---
Section: title

Fisher-Weighted LoRA: One-Line Importance-Aware Regularisation for Parameter-Efficient Fine-Tuning

---

---
Section: abstract

We introduce Fisher-Weighted LoRA (FW-LoRA), a minimalist modification to low-rank adaptation that automatically reallocates an extremely small adapter budget toward the weight matrices that matter most. During a short warm-up we accumulate an exponential moving average of the squared gradients of each frozen pretrained weight matrix, obtaining a cheap diagonal Fisher-information proxy. After normalising these scores we add one extra term to the objective: a Fisher-weighted L2 penalty that softly drives LoRA updates attached to low-importance matrices toward zero while leaving high-importance ones almost unregularised. FW-LoRA therefore induces importance-aware capacity allocation without singular-value decompositions, rank schedulers, extra forward passes, or architectural changes. We validate the idea by fine-tuning roberta-base on two GLUE classification tasks. On SST-2, FW-LoRA lifts validation accuracy to 93.92 % versus 93.69 % for uniform-rank LoRA at an identical 0.2 % parameter budget, with only a negligible (<1 %) theoretical runtime overhead. Detailed confusion matrices, learning curves, and aggregated metrics confirm that the accuracy gain comes from a better distribution of adapter capacity rather than chance. The results show that a single-line regulariser can capture nearly all of the benefit of much heavier adaptive-rank methods such as AdaLoRA while preserving the practicality and speed of vanilla LoRA.

---

---
Section: introduction

Large language models have become the foundation of contemporary natural-language processing, yet full fine-tuning remains prohibitively expensive when many downstream tasks must be supported. Parameter-efficient fine-tuning (PEFT) mitigates this cost by introducing a small number of trainable parameters while keeping the large pretrained backbone frozen \cite{chen-2023-parameter}. Among the many PEFT strategies, low-rank adaptation (LoRA) is attractive because it injects learnable matrices directly into the weight space and imposes negligible inference overhead. A persistent limitation, however, is that almost all LoRA recipes allocate the same rank to every modified layer, implicitly assuming that each layer contributes equally. Empirical evidence shows that this is rarely the case; mis-allocation is especially harmful when the adapter budget drops below one percent of the backbone size.

Adaptive methods such as AdaLoRA tackle this problem by estimating layer importance and dynamically pruning singular values during training, thereby reallocating rank where it is most needed \cite{zhang-2023-adaptive}. Although effective, these techniques incur additional memory for temporary SVD buffers, extra compute for repeated decompositions, and intricate scheduling logic—obstacles for quick experimentation or resource-constrained deployment.

This work asks a simple question: can we approximate the benefits of adaptive rank allocation with the simplicity and speed of vanilla LoRA? We answer in the affirmative by proposing Fisher-Weighted LoRA (FW-LoRA). The key insight is that the gradients of the frozen backbone, already computed during ordinary training, contain rich information about which weight matrices influence the task loss. By accumulating an exponential moving average of their squared norm for a few hundred optimisation steps we obtain a stable, diagonal Fisher-information proxy per matrix. Normalising these values yields per-matrix importance scores that remain fixed for the rest of training. A single additional term—an L2 penalty weighted by (1 − importance)—then biases the optimiser to devote its limited LoRA capacity to the influential matrices.

Why is this hard? Importance signals must be informative, cheap, and stable early in training. SVD-based metrics satisfy the first criterion but violate the second. Simple heuristics satisfy the second but often fail the first. Our diagonal Fisher proxy achieves a middle ground: it is nothing more than squaring and summing gradients that are already in memory, yet experiments show that it captures enough signal to guide allocation.

How do we verify the claim? We fine-tune the 110 M-parameter roberta-base model on SST-2 (sentiment) and CoLA (grammatical acceptability). Three adapter budgets (0.1 %, 0.2 %, 0.5 %) are considered, but for brevity we report the complete log of the 0.2 % case. FW-LoRA is compared to uniform-rank LoRA under identical optimiser settings; AdaLoRA is discussed qualitatively because matched runs are not yet available. Validation accuracy, confusion matrices, learning curves, and wall-clock time are recorded.

Contributions (summary):
• A one-line Fisher-weighted shrinkage that transforms uniform LoRA into an importance-aware allocator using only first-order signals.
• A practical recipe involving a short warm-up, importance normalisation, and a static penalty—no SVD, no scheduler, no extra passes.
• Empirical evidence on SST-2 showing that FW-LoRA closes most of the gap between plain LoRA and AdaLoRA while retaining LoRA’s simplicity.
• Open-source artefacts (metrics, figures, significance tests) enabling full reproducibility.

The remainder of the paper is organised as follows. Section 2 situates FW-LoRA within existing PEFT literature. Section 3 reviews the necessary background and notation. Section 4 details the method. Section 5 describes the experimental setup. Section 6 reports results and analyses. Section 7 concludes and outlines future work, including broader task coverage, statistical robustness, and integration with robustness-oriented PEFT ideas \cite{yang-2022-robust,petrov-2023-when}.

---

---
Section: related_work

PEFT can be framed as a design space defined by layer grouping, parameter allocation, tunable groups, and adaptation strategy \cite{chen-2023-parameter}. FW-LoRA operates exclusively in the allocation dimension: it keeps the LoRA parameterisation intact yet redistributes the effective capacity by means of a Fisher-weighted penalty.

Uniform-rank LoRA remains the de-facto baseline owing to its ease of use, but its equal allocation disregards inter-layer heterogeneity. AdaLoRA extends LoRA with dynamic singular-value pruning and a rank scheduler that reallocates parameters during training, achieving strong gains at small budgets \cite{zhang-2023-adaptive}. The price is a per-step SVD, additional memory, and hyper-parameters that govern the scheduler. FW-LoRA seeks the same goal using first-order information already present in the training loop, shifting complexity from expensive matrix decompositions to a static regulariser.

Prefix-tuning and prompt-tuning operate in activation space rather than weight space, yielding strong modularity but limited ability to alter internal attention patterns \cite{petrov-2023-when}. Robust variants add task-specific prefixes to defend against adversarial perturbations \cite{yang-2022-robust}. These approaches are complementary to ours: FW-LoRA targets weight-space capacity allocation and can in principle be combined with activation-space methods.

Finally, layer normalisation stabilises gradient statistics in transformers and therefore indirectly affects the Fisher signals exploited by FW-LoRA \cite{ba-2016-layer}. We inherit the pretrained normalisation parameters unchanged.

In summary, FW-LoRA delivers AdaLoRA-like importance awareness at plain-LoRA cost, filling a gap between uniform allocation and heavy adaptive schemes.

---

---
Section: background

Low-rank adaptation augments selected pretrained weight matrices W_m with ΔW_m = A_m B_m, where A_m ∈ ℝ^{d×r}, B_m ∈ ℝ^{r×d}, and r ≪ d. Only A_m and B_m are trained; W_m is frozen. Given a fixed parameter budget B (expressed as a percentage of the backbone) the practitioner must decide how to distribute rank across matrices. Uniform distribution is simple but often suboptimal.

Diagonal Fisher proxies. Let L_task be the downstream loss. During back-propagation the gradient ∇_t W_m is available. The Fisher information matrix of W_m is FIM_m = E; computing it exactly is infeasible, but its diagonal trace can be approximated by the expectation of squared gradients. We therefore maintain an exponential moving average F_m ← α F_m + (1 − α) ||∇_t W_m||_F² with α close to one. After K warm-up steps these values stabilise.

Importance scores. We normalise the frozen F_m to obtain I_m = F_m / Σ_k F_k, ensuring 0 ≤ I_m ≤ 1 and Σ_m I_m = 1. Larger I_m implies that W_m has historically received stronger gradients and is therefore more influential.

Fisher-weighted penalty. For the remainder of training we minimise
L_total = L_task + λ Σ_m (1 − I_m) ||ΔW_m||_F²,
where λ is a global shrinkage coefficient. The term (1 − I_m) scales the penalty: adapters attached to unimportant matrices (small I_m) are heavily regularised; those on important matrices are almost free. Crucially, no SVD, rank pruning, or discrete scheduling is required. The method assumes (i) early gradients reflect long-term importance and (ii) a diagonal proxy suffices for allocation—assumptions validated empirically in Section 6.

The overall algorithm therefore inserts three lightweight operations into a standard LoRA loop and leaves all structural PEFT choices untouched \cite{chen-2023-parameter}.

---

---
Section: method

FW-LoRA proceeds in three sequential phases.

1. Warm-up (steps 1…K). Train with standard LoRA and accumulate F_m = EMA_{t≤K}(||∇_t W_m||_F²) with decay α = 0.98. No extra forward or backward passes are performed; we simply square and sum existing gradients.

2. Importance normalisation. After step K compute I_m = F_m / Σ_k F_k once and freeze these scalars for the rest of training.

3. Fisher-weighted training. Optimise the LoRA parameters by minimising L_total = L_task + λ Σ_m (1 − I_m) ||ΔW_m||_F² with λ ≈ 1 × 10^{−4}. The additional backward term is trivial to implement: multiply each adapter’s weight decay by its factor (1 − I_m).

Hyper-parameters. In all experiments we fix K = 500 steps, α = 0.98, and λ = 1 × 10^{−4}. A small study (§6) shows that performance is insensitive to these values within reasonable ranges.

Complexity. The warm-up adds one scalar update per matrix per step; the Fisher term adds a per-matrix scaling during back-propagation. Both costs are negligible compared with a full SVD required by AdaLoRA \cite{zhang-2023-adaptive}. Memory footprint equals that of plain LoRA plus one float per matrix to store I_m.

---

---
Section: experimental_setup

Backbone and adapters. We start from the HuggingFace roberta-base checkpoint (110 M parameters). LoRA adapters are inserted into query, key, value, intermediate.dense, and output.dense projections. A uniform rank r = 8 and scaling α_lora = 16 yield a 0.2 % parameter budget.

Data and metrics. Two GLUE tasks are considered. For SST-2 we report validation accuracy; for CoLA the primary metric is Matthews correlation. All inputs are tokenised to a maximum length of 128 with truncation.

Optimisation. Both FW-LoRA and the plain-LoRA baseline are trained for three epochs using AdamW (learning rate 2 × 10^{−4}, weight decay 0.01), a linear scheduler with 6 % warm-up ratio, batch size 32, gradient accumulation 1, bf16 precision, and random seed 42.

FW-LoRA specifics. Warm-up steps K = 500, EMA decay α = 0.98, shrinkage λ = 1 × 10^{−4}. Importance scores are frozen thereafter.

Logging. After each epoch we record validation metrics, confusion matrices, training losses, and wall-clock time. All runs are executed on a single A100 GPU; peak memory is well below 24 GB.

Runs reported. (1) FW-LoRA on SST-2 as described above. (2) Uniform-rank LoRA baseline differing only in the absence of the Fisher term and, owing to an implementation quirk, the baseline targets the dense projection rather than intermediate.dense/output.dense. We acknowledge this confounder and discuss its impact in Section 6.

---

---
Section: results

We present all logged results for the SST-2 task. Higher accuracy and lower loss indicate better performance unless noted.

Overall accuracy. FW-LoRA reaches a best validation accuracy of 93.922 % while plain LoRA attains 93.693 %. Training accuracies are 94.445 % and 93.654 %, respectively. Validation losses are comparable (0.1886 vs 0.1854).

Confusion matrices. FW-LoRA: [, ]. Baseline: [, ]. FW-LoRA reduces false positives by seven at the cost of five additional false negatives, yielding the net gain in accuracy.

Runtime. FW-LoRA logs 978.9 s wall-clock versus 586.3 s for the baseline. The theoretical overhead of FW-LoRA is <1 %, so the observed gap likely stems from the different set of adapted layers. Future runs will equalise these targets to isolate true overhead.

Ablation highlights. Varying K ∈ {300, 500, 800} changes accuracy by ≤0.1 pp; sweeping λ between 5 × 10^{−5} and 5 × 10^{−4} finds a flat optimum around 1 × 10^{−4}. Removing the Fisher term after warm-up reverts performance to the baseline, confirming its necessity.

Figures.
Figure 1: Validation confusion matrix, FW-LoRA (higher diagonal is better) (filename: proposed-roberta-base-110M--SST-2-GLUE_confusion_matrix.pdf)
Figure 2: Validation accuracy curve, FW-LoRA (higher is better) (filename: proposed-roberta-base-110M--SST-2-GLUE_eval_acc_learning_curve.pdf)
Figure 3: GLUE accuracy curve, FW-LoRA (higher is better) (filename: proposed-roberta-base-110M--SST-2-GLUE_glue_accuracy_learning_curve.pdf)
Figure 4: Training loss curve, FW-LoRA (lower is better) (filename: proposed-roberta-base-110M--SST-2-GLUE_train_loss_learning_curve.pdf)
Figure 5: Validation confusion matrix, plain LoRA (higher diagonal is better) (filename: comparative-1-roberta-base-110M--SST-2-GLUE_confusion_matrix.pdf)
Figure 6: Validation accuracy curve, plain LoRA (higher is better) (filename: comparative-1-roberta-base-110M--SST-2-GLUE_eval_acc_learning_curve.pdf)
Figure 7: GLUE accuracy curve, plain LoRA (higher is better) (filename: comparative-1-roberta-base-110M--SST-2-GLUE_glue_accuracy_learning_curve.pdf)
Figure 8: Training loss curve, plain LoRA (lower is better) (filename: comparative-1-roberta-base-110M--SST-2-GLUE_train_loss_learning_curve.pdf)
Figure 9: Bar chart of best validation accuracy (higher is better) (filename: comparison_best_eval_acc_bar_chart.pdf)
Figure 10: Bar chart of runtime in seconds (lower is better) (filename: comparison_runtime_sec_bar_chart.pdf)
Figure 11: Metrics artefact (filename: metrics.json)
Figure 12: Aggregated metrics artefact (filename: aggregated_metrics.json)
Figure 13: Significance tests artefact (filename: significance_tests.json)

Discussion. Although the absolute gain (+0.23 pp) is modest, it is achieved with a single line of code and no hyper-parameter tuning beyond defaults. The baseline-to-FW-LoRA improvement mirrors trends reported by AdaLoRA on similar tasks and budgets \cite{zhang-2023-adaptive}, suggesting that first-order Fisher information is a viable surrogate for more elaborate importance metrics.

Limitations. Results are single-seed and limited to one task in depth; CoLA logs are omitted due to space. The runtime analysis is confounded by different target layers. Statistical significance is therefore suggestive rather than definitive. Nevertheless, the present evidence supports the central claim that importance-aware regularisation can be obtained almost for free.

---

---
Section: conclusion

We presented Fisher-Weighted LoRA, an importance-aware extension of low-rank adaptation that requires only a brief gradient accumulation phase and a single Fisher-weighted L2 term. The method captures layer importance via a cheap diagonal Fisher proxy, reallocates adapter capacity implicitly, and improves validation accuracy on SST-2 over uniform-rank LoRA with negligible theoretical overhead. By avoiding the computational burdens of adaptive SVD-based schemes such as AdaLoRA \cite{zhang-2023-adaptive}, FW-LoRA offers a practical default for resource-constrained settings.

Future directions include: (i) systematic comparisons with AdaLoRA under matched layer selections and multiple random seeds; (ii) broader evaluations across GLUE and non-classification tasks; (iii) integration with robustness-oriented PEFT frameworks \cite{yang-2022-robust}; and (iv) exploration of adaptive shrinkage schedules that update importance on-the-fly. We hope that the simplicity of FW-LoRA will lower the barrier to importance-aware PEFT and spur adoption in real-world fine-tuning pipelines.

---


## LaTeX Formatting Rules:
- Use \subsection{...} for any subsections within this section.
    - Subsection titles should be distinct from the section name;
    - Do not use '\subsection{  }', or other slight variations. Use more descriptive and unique titles.
    - Avoid excessive subdivision. If a subsection is brief or overlaps significantly with another, consider merging them for clarity and flow.

- For listing contributions, use the LaTeX \begin{itemize}...\end{itemize} format.
    - Each item should start with a short title in \textbf{...} format.
    - Avoid using -, *, or other Markdown bullet styles.

- When including tables, use the `tabularx` environment with `\textwidth` as the target width.
    - At least one column must use the `X` type to enable automatic width adjustment and line breaking.
    - Include `\hline` at the top, after the header, and at the bottom. Avoid vertical lines unless necessary.
    - To left-align content in `X` columns, define `
ewcolumntype{Y}{>{
aggedrightrraybackslash}X}` using the `array` package.

- When writing pseudocode, use the `algorithm` and `algorithmicx` LaTeX environments.
    - Only include pseudocode in the `Method` section. Pseudocode is not allowed in any other sections.
    - Prefer the `\begin{algorithmic}` environment using **lowercase commands** such as `\State`, `\For`, and `\If`, to ensure compatibility and clean formatting.
    - Pseudocode must represent actual algorithms or procedures with clear logic. Do not use pseudocode to simply rephrase narrative descriptions or repeat what has already been explained in text.
        - Good Example:
        ```latex
        \State Compute transformed tokens: \(	ilde{T} \leftarrow W\,T\)
        \State Update: \(T_{new} \leftarrow 	ilde{T} + \mu\,T_{prev}\)
        ```
- Figures and images are ONLY allowed in the "Results" section.
    - Use LaTeX float option `[H]` to force placement.

- All figures must be inserted using the following LaTeX format, using a `width` that reflects the filename:
    ```latex
    \includegraphics[width=\linewidth]{ images/filename.pdf }
    ```
    The `<appropriate-width>` must be selected based on the filename suffix:
    - If the filename ends with _pair1.pdf or _pair2.pdf, use 0.48\linewidth as the width of each subfigure environment and place the figures side by side using `subcaption` package.
    - Otherwise (default), use 0.7\linewidth

- **Escaping special characters**:
    - LaTeX special characters (`#`, `$`, `%`, `&`, `~`, `_`, `^`, `{`, `}`, `\`) must be escaped with a leading backslash when they appear in plain text (e.g., `data\_set`, `C\&C`).
    - Underscores **must always be escaped** (`\_`) outside math mode, even in filenames (e.g., memory\_profiler), code-style words, itemize lists, or citation contexts.

- Always use ASCII hyphens (`-`) instead of en-dashes (`–`) or em-dashes (`—`) to avoid spacing issues in hyphenated terms.
- Do not include any of these higher-level commands such as \documentclass{...}, \begin{document}, and \end{document}.
    - Additionally, avoid including section-specific commands such as \begin{abstract}, \section{  }, or any other similar environment definitions.
- Do not modify citation placeholders:
    - Citation placeholders appear in the format [citation_key], where citation_key contains underscores, numbers, and text (e.g., [vaswani_2017_attention], [smith_2023_deep]).
    - You must preserve these placeholders EXACTLY as they appear in the input text.
- If no LaTeX conversion is needed, output the content as-is without status messages like [Unchanged].
Output:
{
    "title": "Fisher-Weighted LoRA: One-Line Importance-Aware Regularisation for Parameter-Efficient Fine-Tuning",
    "abstract": "We introduce Fisher-Weighted LoRA (FW-LoRA), a minimalist modification to low-rank adaptation that automatically reallocates an extremely small adapter budget toward the weight matrices that matter most. During a short warm-up we accumulate an exponential moving average of the squared gradients of each frozen pretrained weight matrix, obtaining a cheap diagonal Fisher-information proxy. After normalising these scores we add one extra term to the objective: a Fisher-weighted L2 penalty that softly drives LoRA updates attached to low-importance matrices toward zero while leaving high-importance ones almost unregularised. FW-LoRA therefore induces importance-aware capacity allocation without singular-value decompositions, rank schedulers, extra forward passes, or architectural changes. We validate the idea by fine-tuning roberta-base on two GLUE classification tasks. On SST-2, FW-LoRA lifts validation accuracy to 93.92 \\% versus 93.69 \\% for uniform-rank LoRA at an identical 0.2 \\% parameter budget, with only a negligible ($<1\\%$) theoretical runtime overhead. Detailed confusion matrices, learning curves, and aggregated metrics confirm that the accuracy gain comes from a better distribution of adapter capacity rather than chance. The results show that a single-line regulariser can capture nearly all of the benefit of much heavier adaptive-rank methods such as AdaLoRA while preserving the practicality and speed of vanilla LoRA.",
    "introduction": "Large language models have become the foundation of contemporary natural-language processing, yet full fine-tuning remains prohibitively expensive when many downstream tasks must be supported. Parameter-efficient fine-tuning (PEFT) mitigates this cost by introducing a small number of trainable parameters while keeping the large pretrained backbone frozen \\cite{chen-2023-parameter}. Among the many PEFT strategies, low-rank adaptation (LoRA) is attractive because it injects learnable matrices directly into the weight space and imposes negligible inference overhead. A persistent limitation, however, is that almost all LoRA recipes allocate the same rank to every modified layer, implicitly assuming that each layer contributes equally. Empirical evidence shows that this is rarely the case; mis-allocation is especially harmful when the adapter budget drops below one percent of the backbone size.\n\nAdaptive methods such as AdaLoRA tackle this problem by estimating layer importance and dynamically pruning singular values during training, thereby reallocating rank where it is most needed \\cite{zhang-2023-adaptive}. Although effective, these techniques incur additional memory for temporary SVD buffers, extra compute for repeated decompositions, and intricate scheduling logic-obstacles for quick experimentation or resource-constrained deployment.\n\nThis work asks a simple question: can we approximate the benefits of adaptive rank allocation with the simplicity and speed of vanilla LoRA? We answer in the affirmative by proposing Fisher-Weighted LoRA (FW-LoRA). The key insight is that the gradients of the frozen backbone, already computed during ordinary training, contain rich information about which weight matrices influence the task loss. By accumulating an exponential moving average of their squared norm for a few hundred optimisation steps we obtain a stable, diagonal Fisher-information proxy per matrix. Normalising these values yields per-matrix importance scores that remain fixed for the rest of training. A single additional term-an L2 penalty weighted by $(1 - \\text{importance})$-then biases the optimiser to devote its limited LoRA capacity to the influential matrices.\n\nWhy is this hard? Importance signals must be informative, cheap, and stable early in training. SVD-based metrics satisfy the first criterion but violate the second. Simple heuristics satisfy the second but often fail the first. Our diagonal Fisher proxy achieves a middle ground: it is nothing more than squaring and summing gradients that are already in memory, yet experiments show that it captures enough signal to guide allocation.\n\nHow do we verify the claim? We fine-tune the 110 M-parameter roberta-base model on SST-2 (sentiment) and CoLA (grammatical acceptability). Three adapter budgets (0.1 \\%, 0.2 \\%, 0.5 \\%) are considered, but for brevity we report the complete log of the 0.2 \\% case. FW-LoRA is compared to uniform-rank LoRA under identical optimiser settings; AdaLoRA is discussed qualitatively because matched runs are not yet available. Validation accuracy, confusion matrices, learning curves, and wall-clock time are recorded.\n\n\\subsection{Contributions}\n\\begin{itemize}\n  \\item \\textbf{Fisher-weighted shrinkage:} A one-line Fisher-weighted shrinkage that transforms uniform LoRA into an importance-aware allocator using only first-order signals.\n  \\item \\textbf{Practical recipe:} A practical recipe involving a short warm-up, importance normalisation, and a static penalty-no SVD, no scheduler, no extra passes.\n  \\item \\textbf{Empirical evidence:} Empirical evidence on SST-2 showing that FW-LoRA closes most of the gap between plain LoRA and AdaLoRA while retaining LoRA's simplicity.\n  \\item \\textbf{Open-source artefacts:} Open-source artefacts (metrics, figures, significance tests) enabling full reproducibility.\n\\end{itemize}\n\nThe remainder of the paper is organised as follows. Section 2 situates FW-LoRA within existing PEFT literature. Section 3 reviews the necessary background and notation. Section 4 details the method. Section 5 describes the experimental setup. Section 6 reports results and analyses. Section 7 concludes and outlines future work, including broader task coverage, statistical robustness, and integration with robustness-oriented PEFT ideas \\cite{yang-2022-robust,petrov-2023-when}.",
    "related_work": "PEFT can be framed as a design space defined by layer grouping, parameter allocation, tunable groups, and adaptation strategy \\cite{chen-2023-parameter}. FW-LoRA operates exclusively in the allocation dimension: it keeps the LoRA parameterisation intact yet redistributes the effective capacity by means of a Fisher-weighted penalty.\n\nUniform-rank LoRA remains the de-facto baseline owing to its ease of use, but its equal allocation disregards inter-layer heterogeneity. AdaLoRA extends LoRA with dynamic singular-value pruning and a rank scheduler that reallocates parameters during training, achieving strong gains at small budgets \\cite{zhang-2023-adaptive}. The price is a per-step SVD, additional memory, and hyper-parameters that govern the scheduler. FW-LoRA seeks the same goal using first-order information already present in the training loop, shifting complexity from expensive matrix decompositions to a static regulariser.\n\nPrefix-tuning and prompt-tuning operate in activation space rather than weight space, yielding strong modularity but limited ability to alter internal attention patterns \\cite{petrov-2023-when}. Robust variants add task-specific prefixes to defend against adversarial perturbations \\cite{yang-2022-robust}. These approaches are complementary to ours: FW-LoRA targets weight-space capacity allocation and can in principle be combined with activation-space methods.\n\nFinally, layer normalisation stabilises gradient statistics in transformers and therefore indirectly affects the Fisher signals exploited by FW-LoRA \\cite{ba-2016-layer}. We inherit the pretrained normalisation parameters unchanged.\n\nIn summary, FW-LoRA delivers AdaLoRA-like importance awareness at plain-LoRA cost, filling a gap between uniform allocation and heavy adaptive schemes.",
    "background": "Low-rank adaptation augments selected pretrained weight matrices $W_m$ with $\\Delta W_m = A_m B_m$, where $A_m \\in \\mathbb{R}^{d\\times r}$, $B_m \\in \\mathbb{R}^{r\\times d}$, and $r \\ll d$. Only $A_m$ and $B_m$ are trained; $W_m$ is frozen. Given a fixed parameter budget $B$ (expressed as a percentage of the backbone) the practitioner must decide how to distribute rank across matrices. Uniform distribution is simple but often suboptimal.\n\n\\textit{Diagonal Fisher proxies.} Let $L_{task}$ be the downstream loss. During back-propagation the gradient $\\nabla_t W_m$ is available. The Fisher information matrix of $W_m$ is $FIM_m = \\mathbb{E}$; computing it exactly is infeasible, but its diagonal trace can be approximated by the expectation of squared gradients. We therefore maintain an exponential moving average $F_m \\leftarrow \\alpha F_m + (1 - \\alpha) \\lVert \\nabla_t W_m \\rVert_F^2$ with $\\alpha$ close to one. After $K$ warm-up steps these values stabilise.\n\n\\textit{Importance scores.} We normalise the frozen $F_m$ to obtain $I_m = F_m / \\sum_k F_k$, ensuring $0 \\le I_m \\le 1$ and $\\sum_m I_m = 1$. Larger $I_m$ implies that $W_m$ has historically received stronger gradients and is therefore more influential.\n\n\\textit{Fisher-weighted penalty.} For the remainder of training we minimise\n\\[\nL_{total} = L_{task} + \\lambda \\sum_m (1 - I_m) \\, \\lVert \\Delta W_m \\rVert_F^2,\n\\]\nwhere $\\lambda$ is a global shrinkage coefficient. The term $(1 - I_m)$ scales the penalty: adapters attached to unimportant matrices (small $I_m$) are heavily regularised; those on important matrices are almost free. Crucially, no SVD, rank pruning, or discrete scheduling is required. The method assumes (i) early gradients reflect long-term importance and (ii) a diagonal proxy suffices for allocation-assumptions validated empirically in Section 6.\n\nThe overall algorithm therefore inserts three lightweight operations into a standard LoRA loop and leaves all structural PEFT choices untouched \\cite{chen-2023-parameter}.",
    "method": "FW-LoRA proceeds in three sequential phases.\n\n\\begin{enumerate}\n  \\item Warm-up (steps $1\\ldots K$). Train with standard LoRA and accumulate $F_m = EMA_{t\\le K}(\\lVert \\nabla_t W_m \\rVert_F^2)$ with decay $\\alpha = 0.98$. No extra forward or backward passes are performed; we simply square and sum existing gradients.\n  \\item Importance normalisation. After step $K$ compute $I_m = F_m / \\sum_k F_k$ once and freeze these scalars for the rest of training.\n  \\item Fisher-weighted training. Optimise the LoRA parameters by minimising $L_{total} = L_{task} + \\lambda \\sum_m (1 - I_m) \\lVert \\Delta W_m \\rVert_F^2$ with $\\lambda \\approx 1 \\times 10^{-4}$. The additional backward term is trivial to implement: multiply each adapter's weight decay by its factor $(1 - I_m)$.\n\\end{enumerate}\n\n\\subsection{Hyper-parameters and insensitivity}\nIn all experiments we fix $K = 500$ steps, $\\alpha = 0.98$, and $\\lambda = 1 \\times 10^{-4}$. A small study (\\S6) shows that performance is insensitive to these values within reasonable ranges.\n\n\\subsection{Complexity and memory}\nThe warm-up adds one scalar update per matrix per step; the Fisher term adds a per-matrix scaling during back-propagation. Both costs are negligible compared with a full SVD required by AdaLoRA \\cite{zhang-2023-adaptive}. Memory footprint equals that of plain LoRA plus one float per matrix to store $I_m$.\n\n\\begin{algorithm}[H]\n\\caption{FW-LoRA training procedure}\n\\begin{algorithmic}\n  \\State \\textbf{Inputs:} pretrained weights $\\{W_m\\}$ (frozen), adapter params $\\{A_m,B_m\\}$, decay $\\alpha$, warm-up steps $K$, shrinkage $\\lambda$\n  \\State \\textbf{Init:} $F_m \\leftarrow 0$ for all $m$; attach $\\Delta W_m = A_m B_m$ to selected $W_m$\n  \\For{step $t = 1,2,\\ldots$}\n    \\State Compute task loss $L_{task}$; back-propagate to obtain gradients $\\nabla_t W_m$ and $\\nabla_t A_m, \\nabla_t B_m$\n    \\If{$t \\le K$}\n      \\State $F_m \\leftarrow \\alpha F_m + (1-\\alpha)\\, \\lVert \\nabla_t W_m \\rVert_F^2$ for all $m$\n    \\ElsIf{$t = K+1$}\n      \\State $I_m \\leftarrow F_m / \\sum_k F_k$ for all $m$; freeze $I_m$\n    \\EndIf\n    \\State Compute $L_{reg} \\leftarrow \\lambda \\sum_m (1 - I_m)\\, \\lVert \\Delta W_m \\rVert_F^2$ (use $I_m = 0$ during warm-up)\n    \\State Update adapters using gradients of $L_{task} + L_{reg}$ (backbone $\\{W_m\\}$ remain frozen)\n  \\EndFor\n\\end{algorithmic}\n\\end{algorithm}",
    "experimental_setup": "\\subsection{Backbone and adapters}\nWe start from the HuggingFace roberta-base checkpoint (110 M parameters). LoRA adapters are inserted into query, key, value, intermediate.dense, and output.dense projections. A uniform rank $r = 8$ and scaling $\\alpha_{\\text{lora}} = 16$ yield a 0.2 \\% parameter budget.\n\n\\subsection{Data and metrics}\nTwo GLUE tasks are considered. For SST-2 we report validation accuracy; for CoLA the primary metric is Matthews correlation. All inputs are tokenised to a maximum length of 128 with truncation.\n\n\\subsection{Optimisation}\nBoth FW-LoRA and the plain-LoRA baseline are trained for three epochs using AdamW (learning rate $2 \\times 10^{-4}$, weight decay $0.01$), a linear scheduler with 6 \\% warm-up ratio, batch size 32, gradient accumulation 1, bf16 precision, and random seed 42.\n\n\\subsection{FW-LoRA specifics}\nWarm-up steps $K = 500$, EMA decay $\\alpha = 0.98$, shrinkage $\\lambda = 1 \\times 10^{-4}$. Importance scores are frozen thereafter.\n\n\\subsection{Logging}\nAfter each epoch we record validation metrics, confusion matrices, training losses, and wall-clock time. All runs are executed on a single A100 GPU; peak memory is well below 24 GB.\n\n\\subsection{Runs reported}\n(1) FW-LoRA on SST-2 as described above. (2) Uniform-rank LoRA baseline differing only in the absence of the Fisher term and, owing to an implementation quirk, the baseline targets the dense projection rather than intermediate.dense/output.dense. We acknowledge this confounder and discuss its impact in Section 6.",
    "results": "We present all logged results for the SST-2 task. Higher accuracy and lower loss indicate better performance unless noted.\n\n\\textit{Overall accuracy.} FW-LoRA reaches a best validation accuracy of 93.922 \\% while plain LoRA attains 93.693 \\%. Training accuracies are 94.445 \\% and 93.654 \\%, respectively. Validation losses are comparable (0.1886 vs 0.1854).\n\n\\textit{Confusion matrices.} FW-LoRA: [, ]. Baseline: [, ]. FW-LoRA reduces false positives by seven at the cost of five additional false negatives, yielding the net gain in accuracy.\n\n\\textit{Runtime.} FW-LoRA logs 978.9 s wall-clock versus 586.3 s for the baseline. The theoretical overhead of FW-LoRA is $<1\\%$, so the observed gap likely stems from the different set of adapted layers. Future runs will equalise these targets to isolate true overhead.\n\n\\textit{Ablation highlights.} Varying $K \\in \\{300, 500, 800\\}$ changes accuracy by $\\le 0.1$ pp; sweeping $\\lambda$ between $5 \\times 10^{-5}$ and $5 \\times 10^{-4}$ finds a flat optimum around $1 \\times 10^{-4}$. Removing the Fisher term after warm-up reverts performance to the baseline, confirming its necessity.\n\n\\begin{figure}[H]\n  \\centering\n  \\includegraphics[width=0.7\\linewidth]{ images/proposed-roberta-base-110M--SST-2-GLUE\\_confusion\\_matrix.pdf }\n  \\caption{Validation confusion matrix, FW-LoRA (higher diagonal is better).}\n\\end{figure}\n\n\\begin{figure}[H]\n  \\centering\n  \\includegraphics[width=0.7\\linewidth]{ images/proposed-roberta-base-110M--SST-2-GLUE\\_eval\\_acc\\_learning\\_curve.pdf }\n  \\caption{Validation accuracy curve, FW-LoRA (higher is better).}\n\\end{figure}\n\n\\begin{figure}[H]\n  \\centering\n  \\includegraphics[width=0.7\\linewidth]{ images/proposed-roberta-base-110M--SST-2-GLUE\\_glue\\_accuracy\\_learning\\_curve.pdf }\n  \\caption{GLUE accuracy curve, FW-LoRA (higher is better).}\n\\end{figure}\n\n\\begin{figure}[H]\n  \\centering\n  \\includegraphics[width=0.7\\linewidth]{ images/proposed-roberta-base-110M--SST-2-GLUE\\_train\\_loss\\_learning\\_curve.pdf }\n  \\caption{Training loss curve, FW-LoRA (lower is better).}\n\\end{figure}\n\n\\begin{figure}[H]\n  \\centering\n  \\includegraphics[width=0.7\\linewidth]{ images/comparative-1-roberta-base-110M--SST-2-GLUE\\_confusion\\_matrix.pdf }\n  \\caption{Validation confusion matrix, plain LoRA (higher diagonal is better).}\n\\end{figure}\n\n\\begin{figure}[H]\n  \\centering\n  \\includegraphics[width=0.7\\linewidth]{ images/comparative-1-roberta-base-110M--SST-2-GLUE\\_eval\\_acc\\_learning\\_curve.pdf }\n  \\caption{Validation accuracy curve, plain LoRA (higher is better).}\n\\end{figure}\n\n\\begin{figure}[H]\n  \\centering\n  \\includegraphics[width=0.7\\linewidth]{ images/comparative-1-roberta-base-110M--SST-2-GLUE\\_glue\\_accuracy\\_learning\\_curve.pdf }\n  \\caption{GLUE accuracy curve, plain LoRA (higher is better).}\n\\end{figure}\n\n\\begin{figure}[H]\n  \\centering\n  \\includegraphics[width=0.7\\linewidth]{ images/comparative-1-roberta-base-110M--SST-2-GLUE\\_train\\_loss\\_learning\\_curve.pdf }\n  \\caption{Training loss curve, plain LoRA (lower is better).}\n\\end{figure}\n\n\\begin{figure}[H]\n  \\centering\n  \\includegraphics[width=0.7\\linewidth]{ images/comparison\\_best\\_eval\\_acc\\_bar\\_chart.pdf }\n  \\caption{Bar chart of best validation accuracy (higher is better).}\n\\end{figure}\n\n\\begin{figure}[H]\n  \\centering\n  \\includegraphics[width=0.7\\linewidth]{ images/comparison\\_runtime\\_sec\\_bar\\_chart.pdf }\n  \\caption{Bar chart of runtime in seconds (lower is better).}\n\\end{figure}\n\n\\textit{Discussion.} Although the absolute gain (+0.23 pp) is modest, it is achieved with a single line of code and no hyper-parameter tuning beyond defaults. The baseline-to-FW-LoRA improvement mirrors trends reported by AdaLoRA on similar tasks and budgets \\cite{zhang-2023-adaptive}, suggesting that first-order Fisher information is a viable surrogate for more elaborate importance metrics.\n\n\\textit{Limitations.} Results are single-seed and limited to one task in depth; CoLA logs are omitted due to space. The runtime analysis is confounded by different target layers. Statistical significance is therefore suggestive rather than definitive. Nevertheless, the present evidence supports the central claim that importance-aware regularisation can be obtained almost for free.",
    "conclusion": "We presented Fisher-Weighted LoRA, an importance-aware extension of low-rank adaptation that requires only a brief gradient accumulation phase and a single Fisher-weighted L2 term. The method captures layer importance via a cheap diagonal Fisher proxy, reallocates adapter capacity implicitly, and improves validation accuracy on SST-2 over uniform-rank LoRA with negligible theoretical overhead. By avoiding the computational burdens of adaptive SVD-based schemes such as AdaLoRA \\cite{zhang-2023-adaptive}, FW-LoRA offers a practical default for resource-constrained settings.\n\nFuture directions include: (i) systematic comparisons with AdaLoRA under matched layer selections and multiple random seeds; (ii) broader evaluations across GLUE and non-classification tasks; (iii) integration with robustness-oriented PEFT frameworks \\cite{yang-2022-robust}; and (iv) exploration of adaptive shrinkage schedules that update importance on-the-fly. We hope that the simplicity of FW-LoRA will lower the barrier to importance-aware PEFT and spur adoption in real-world fine-tuning pipelines."
}
