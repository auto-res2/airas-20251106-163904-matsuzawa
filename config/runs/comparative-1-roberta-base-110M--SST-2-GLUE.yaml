run_id: comparative-1-roberta-base-110M--SST-2-GLUE
method: baseline_lora                  # plain uniform-rank LoRA
model:
  name: roberta-base
  pretrained_id: roberta-base
  parameter_budget_percent: 0.2
  lora:
    enabled: true
    rank: 8
    alpha: 16
    dropout: 0.1
    target_modules:
      - query
      - key
      - value
      - dense
dataset:
  name: glue
  subset: sst2
  split: train
  eval_split: validation
  max_length: 128
  padding: max_length
  truncation: true
training:
  epochs: 3
  batch_size: 32
  learning_rate: 2.0e-4
  weight_decay: 0.01
  optimizer: adamw
  lr_scheduler: linear
  warmup_ratio: 0.06
  gradient_accumulation_steps: 1
  mixed_precision: bf16
  seed: 42
optuna:
  n_trials: 20
  direction: maximize
  search_space:
    learning_rate:
      type: loguniform
      low: 1.0e-5
      high: 3.0e-4
    lora_rank:
      type: categorical
      choices: [4, 8, 16]
    batch_size:
      type: categorical
      choices: [16, 32, 64]
    weight_decay:
      type: uniform
      low: 0.0
      high: 0.05
